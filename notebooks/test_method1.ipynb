{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib \n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path):\n",
    "        return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts and len(parts) > 0:\n",
    "                    if parts[0].isdigit():\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id in fire_class_ids:\n",
    "                            return True\n",
    "    except Exception as e:\n",
    "        pass \n",
    "    return False\n",
    "\n",
    "def extract_color_histograms(img_processed, color_space, bins):\n",
    "    histograms = []\n",
    "    if color_space == 'hsv':\n",
    "        if img_processed.dtype == np.float32:\n",
    "            hist_h = cv2.calcHist([img_processed], [0], None, [bins], [0, 180]) \n",
    "            hist_s = cv2.calcHist([img_processed], [1], None, [bins], [0, 1]) \n",
    "            histograms.extend([hist_h.flatten(), hist_s.flatten()])\n",
    "        elif img_processed.dtype == np.uint8:\n",
    "            hist_h = cv2.calcHist([img_processed], [0], None, [bins], [0, 180])\n",
    "            hist_s = cv2.calcHist([img_processed], [1], None, [bins], [0, 256])\n",
    "            histograms.extend([hist_h.flatten(), hist_s.flatten()])\n",
    "    elif color_space == 'ycbcr':\n",
    "        if img_processed.dtype == np.float32:\n",
    "            hist_y = cv2.calcHist([img_processed], [0], None, [bins], [0, 1])\n",
    "            hist_cb = cv2.calcHist([img_processed], [1], None, [bins], [-0.5, 0.5])\n",
    "            hist_cr = cv2.calcHist([img_processed], [2], None, [bins], [-0.5, 0.5])\n",
    "            histograms.extend([hist_y.flatten(), hist_cb.flatten(), hist_cr.flatten()])\n",
    "        elif img_processed.dtype == np.uint8:\n",
    "            hist_y = cv2.calcHist([img_processed], [0], None, [bins], [0, 256])\n",
    "            hist_cb = cv2.calcHist([img_processed], [1], None, [bins], [0, 256])\n",
    "            hist_cr = cv2.calcHist([img_processed], [2], None, [bins], [0, 256])\n",
    "            histograms.extend([hist_y.flatten(), hist_cb.flatten(), hist_cr.flatten()])\n",
    "    if histograms: return np.concatenate(histograms)\n",
    "    else: return np.array([])\n",
    "\n",
    "def extract_lbp_features(img_gray, radius, n_points, method):\n",
    "    if n_points is None:\n",
    "        n_points = 8 * radius\n",
    "    lbp_image = local_binary_pattern(img_gray, n_points, radius, method=method)\n",
    "    if method == 'uniform':\n",
    "        n_bins = n_points + 2 \n",
    "        hist_range = (0, n_points + 2)\n",
    "    else:\n",
    "        n_bins = int(lbp_image.max() + 1)\n",
    "        hist_range = (0, n_bins)\n",
    "        if lbp_image.size > 0: \n",
    "            n_bins = int(lbp_image.max() + 1)\n",
    "        else:\n",
    "            n_bins = 1 \n",
    "        hist_range = (0, n_bins)\n",
    "    lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=n_bins, range=hist_range)\n",
    "\n",
    "    lbp_hist = lbp_hist.astype(np.float32)\n",
    "    if lbp_hist.sum() > 0:\n",
    "        lbp_hist /= lbp_hist.sum() \n",
    "    return lbp_hist.flatten()\n",
    "\n",
    "def extract_hog_features(img_gray, orientations, pixels_per_cell, cells_per_block, block_norm):\n",
    "    if img_gray.dtype == np.uint8:\n",
    "         img_gray = img_gray.astype(np.float32) / 255.0 \n",
    "    min_height = cells_per_block[0] * pixels_per_cell[0]\n",
    "    min_width = cells_per_block[1] * pixels_per_cell[1]\n",
    "    if img_gray.shape[0] < min_height or img_gray.shape[1] < min_width:\n",
    "        return np.array([])\n",
    "\n",
    "    try:\n",
    "        hog_features = hog(img_gray, orientations=orientations,\n",
    "                           pixels_per_cell=pixels_per_cell,\n",
    "                           cells_per_block=cells_per_block,\n",
    "                           block_norm=block_norm,\n",
    "                           visualize=False, feature_vector=True)\n",
    "        return hog_features.flatten()\n",
    "    except ValueError as e:\n",
    "         return np.array([])\n",
    "\n",
    "def combine_features(img_dict, feature_params):\n",
    "    all_features = []\n",
    "    if 'hsv' in img_dict:\n",
    "        \n",
    "        hsv_hist = extract_color_histograms(img_dict['hsv'], 'hsv', bins=feature_params.get('hist_bins', 100))\n",
    "        if hsv_hist.size > 0:\n",
    "            all_features.append(hsv_hist)\n",
    "    if 'ycbcr' in img_dict:\n",
    "         \n",
    "         ycbcr_hist = extract_color_histograms(img_dict['ycbcr'], 'ycbcr', bins=feature_params.get('hist_bins', 100))\n",
    "         if ycbcr_hist.size > 0:\n",
    "              all_features.append(ycbcr_hist)\n",
    "\n",
    "    if 'gray' in img_dict:\n",
    "        img_gray_processed = img_dict['gray']\n",
    "\n",
    "        lbp_features = extract_lbp_features(img_gray_processed,\n",
    "                                            radius=feature_params.get('lbp_radius', 3),\n",
    "                                            n_points=feature_params.get('lbp_n_points', None),\n",
    "                                            method=feature_params.get('lbp_method', 'uniform'))\n",
    "        if lbp_features.size > 0:\n",
    "             all_features.append(lbp_features)\n",
    "\n",
    "        hog_features = extract_hog_features(img_gray_processed,\n",
    "                                           orientations=feature_params.get('hog_orientations', 9),\n",
    "                                           pixels_per_cell=feature_params.get('hog_pixels_per_cell', (8, 8)),\n",
    "                                           cells_per_block=feature_params.get('hog_cells_per_block', (2, 2)),\n",
    "                                           block_norm=feature_params.get('hog_block_norm', 'L2-Hys'))\n",
    "        if hog_features.size > 0:\n",
    "             all_features.append(hog_features)\n",
    "\n",
    "    if all_features:\n",
    "        try:\n",
    "            combined_vector = np.concatenate(all_features)\n",
    "            return combined_vector\n",
    "        except ValueError as e:\n",
    "            return np.array([]) \n",
    "\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def get_config(dataset_choice):\n",
    "    config = {}\n",
    "    if dataset_choice == 'kaggle':\n",
    "        config['data_root'] = \"fire_dataset\" \n",
    "        config['target_img_size'] = (128, 128) \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr'] \n",
    "        config['normalize_pixels'] = 1 \n",
    "        config['fire_class_ids'] = None \n",
    "    elif dataset_choice == 'dfire':\n",
    "        config['dfire_root'] = \"D-Fire\"\n",
    "        config['split_name'] = \"test\" \n",
    "        config['data_root'] = os.path.join(config['dfire_root'], config['split_name'])\n",
    "        config['target_img_size'] = (128, 128) \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr'] \n",
    "        config['normalize_pixels'] = 1 \n",
    "        config['fire_class_ids'] = [0] \n",
    "    else:\n",
    "        raise ValueError(f\"Desteklenmeyen veri seti seçimi: {dataset_choice}\")\n",
    "\n",
    "    print(f\"Kullanılan veri seti: {dataset_choice}\")\n",
    "    print(f\"Veri kök dizini: {config.get('data_root')}\")\n",
    "    print(f\"Hedef görüntü boyutu: {config['target_img_size']}\")\n",
    "    print(f\"Yüklenecek renk uzayları: {config['color_spaces_to_load']}\")\n",
    "    print(f\"Piksel normalizasyonu: {'Evet' if config['normalize_pixels'] else 'Hayır'}\")\n",
    "    if dataset_choice == 'dfire':\n",
    "         print(f\"D-Fire Bölmesi: {config['split_name']}\")\n",
    "         print(f\"D-Fire Yangın Sınıfı ID'leri: {config['fire_class_ids']}\")\n",
    "    return config\n",
    "\n",
    "def get_feature_params():\n",
    "    feature_params = {\n",
    "        'hist_bins': 100, \n",
    "        'lbp_radius': 3, \n",
    "        'lbp_n_points': None, \n",
    "        'lbp_method': 'uniform', \n",
    "        'hog_orientations': 9, \n",
    "        'hog_pixels_per_cell': (8, 8), \n",
    "        'hog_cells_per_block': (2, 2), \n",
    "        'hog_block_norm': 'L2-Hys' \n",
    "    }\n",
    "    print(\"\\nÖzellik çıkarma parametreleri:\", feature_params)\n",
    "    return feature_params\n",
    "\n",
    "def load_and_preprocess_data(config):\n",
    "    data_root = config['data_root']\n",
    "    target_img_size = config['target_img_size']\n",
    "    color_spaces_to_load = config['color_spaces_to_load']\n",
    "    normalize_pixels = config['normalize_pixels']\n",
    "    dataset_choice = config.get('dataset_choice', 'kaggle') \n",
    "\n",
    "    processed_data_dict = None\n",
    "    if dataset_choice == 'kaggle':\n",
    "         if os.path.exists(data_root):\n",
    "            print(f\"Kaggle veri seti yükleniyor: {data_root}\")\n",
    "            processed_data_dict = load_prep_img(\n",
    "                data_root,\n",
    "                target_img_size,\n",
    "                color_spaces=color_spaces_to_load,\n",
    "                normalize=normalize_pixels\n",
    "            )\n",
    "         else:\n",
    "            print(f\"Hata: Kaggle veri seti kök dizini bulunamadı: {data_root}\")\n",
    "\n",
    "    elif dataset_choice == 'dfire':\n",
    "         fire_class_ids = config['fire_class_ids']\n",
    "         if os.path.exists(data_root):\n",
    "            print(f\"D-Fire veri seti yükleniyor: {data_root}\")\n",
    "            processed_data_dict = load_prep_dfire(\n",
    "                data_root,\n",
    "                target_img_size,\n",
    "                fire_class_ids=fire_class_ids,\n",
    "                color_spaces=color_spaces_to_load,\n",
    "                normalize=normalize_pixels\n",
    "            )\n",
    "         else:\n",
    "            print(f\"Hata: D-Fire bölmesi kök dizini bulunamadı: {data_root}\")\n",
    "\n",
    "    if processed_data_dict and 'labels' in processed_data_dict and len(processed_data_dict.get('labels', [])) > 0:\n",
    "        print(f\"\\n{len(processed_data_dict['labels'])} görüntü başarıyla yüklendi ve ön işlendi.\")\n",
    "    else:\n",
    "        print(\"Hata: Veri yüklenemedi veya ön işlenemedi.\")\n",
    "        return None \n",
    "\n",
    "    return processed_data_dict\n",
    "\n",
    "\n",
    "def extract_features(processed_data_dict, feature_params):\n",
    "    features = []\n",
    "    valid_labels = []\n",
    "    if processed_data_dict and 'labels' in processed_data_dict and len(processed_data_dict.get('labels', [])) > 0:\n",
    "        labels_all = processed_data_dict['labels']\n",
    "        num_images_total = len(labels_all)\n",
    "        available_data_keys = [key for key in processed_data_dict.keys() if key != 'labels']\n",
    "        print(f\"Toplam {num_images_total} görüntüden özellikler çıkarılıyor.\")\n",
    "        for i in tqdm(range(num_images_total), desc=f\"Özellikler Çıkarılıyor\"):\n",
    "            img_dict_single = {}\n",
    "            for key in available_data_keys:\n",
    "                 if processed_data_dict[key] is not None and i < len(processed_data_dict[key]):\n",
    "                      img_dict_single[key] = processed_data_dict[key][i]\n",
    "            if img_dict_single:\n",
    "                features_single = combine_features(img_dict_single, feature_params)\n",
    "                if features_single.size > 0:\n",
    "                    features.append(features_single)\n",
    "                    valid_labels.append(labels_all[i])\n",
    "                else:pass \n",
    "            else:pass \n",
    "\n",
    "        if features:\n",
    "            features_array = np.vstack(features).astype(np.float32)\n",
    "            labels_array = np.array(valid_labels, dtype=np.int32)\n",
    "            print(f\"\\nBaşarıyla özellik çıkarılan resim sayısı: {features_array.shape[0]}\")\n",
    "            print(f\"Özellik vektörleri boyutu: {features_array.shape[1]}\")\n",
    "            print(f\"Feature Array Shape: {features_array.shape}\")\n",
    "            print(f\"Label Array Shape: {labels_array.shape}\")\n",
    "        else:\n",
    "            print(\"\\nHiçbir görüntüden özellik çıkarılamadı!\")\n",
    "            features_array = np.array([])\n",
    "            labels_array = np.array([])\n",
    "        return features_array, labels_array\n",
    "    else:\n",
    "        print(\"Hata: Özellik çıkarmak için ön işlenmiş veri yok veya boş.\")\n",
    "        return np.array([]), np.array([]) \n",
    "\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0:\n",
    "        print(\"Özellik dizisi boş, veri bölünemedi.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(f\"\\nVeri bölünüyor: Eğitim oranı ({1-test_size:.0%}), Test oranı ({test_size:.0%})\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array \n",
    "    )\n",
    "    print(f\"Eğitim özellikleri boyutu: {X_train.shape}\")\n",
    "    print(f\"Test özellikleri boyutu: {X_test.shape}\")\n",
    "    print(f\"Eğitim etiketleri boyutu: {y_train.shape}\")\n",
    "    print(f\"Test etiketleri boyutu: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features_transform(X_data, scaler):\n",
    "    if X_data is None or X_data.shape[0] == 0:\n",
    "         print(\"scale edilemedi! data bos\")\n",
    "         return None\n",
    "    try:\n",
    "         X_scaled = scaler.transform(X_data)\n",
    "         return X_scaled\n",
    "    except Exception as e:\n",
    "         print(f\"Veri scale edilirken hata oluştu: {e}\")\n",
    "         return None\n",
    "\n",
    "def apply_pca_transform(X_data, pca):\n",
    "    if X_data is None or X_data.shape[0] == 0:\n",
    "         return None\n",
    "    try:\n",
    "         X_pca = pca.transform(X_data)\n",
    "         return X_pca\n",
    "    except Exception as e:\n",
    "         print(f\"Veri PCA ile dönüştürülürken hata oluştu: {e}\")\n",
    "         return None\n",
    "\n",
    "def load_prep_img(data_root, target_size,\n",
    "    color_spaces=['bgr', 'hsv', 'ycbcr'], normalize=1):\n",
    "    processed_data = {cs: [] for cs in color_spaces + ['gray']}\n",
    "    labels = []\n",
    "    class_dirs = {'fire_images': 1, 'non_fire_images': 0} \n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped = 0\n",
    "    print(f\"Görüntüler yükleniyor ve ön işleniyor: {data_root}\")\n",
    "    for class_name, label in class_dirs.items():\n",
    "        class_dir_path = os.path.join(data_root, class_name)\n",
    "        if not os.path.isdir(class_dir_path):\n",
    "             print(f\"Uyarı: Sınıf dizini bulunamadı: {class_dir_path}\")\n",
    "             continue\n",
    "        all_files = os.listdir(class_dir_path)\n",
    "        image_files = [f for f in all_files if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "\n",
    "        if not image_files:\n",
    "             print(f\"Uyarı: {class_dir_path} dizininde görüntü dosyası bulunamadı.\")\n",
    "             continue\n",
    "\n",
    "        for filename in tqdm(image_files, desc=f\"İşleniyor {class_name}\"):\n",
    "            file_path = os.path.join(class_dir_path, filename)\n",
    "            img_bgr = cv2.imread(file_path)\n",
    "            if img_bgr is None:\n",
    "                total_images_skipped += 1\n",
    "                continue\n",
    "            img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            if normalize: img_resized = img_resized.astype(np.float32) / 255.0\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "            processed_data['gray'].append(img_gray)\n",
    "            if 'bgr' in color_spaces:\n",
    "                processed_data['bgr'].append(img_resized)\n",
    "            if 'hsv' in color_spaces:\n",
    "                if normalize: \n",
    "                    img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "                else: \n",
    "                    img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "                processed_data['hsv'].append(img_hsv)\n",
    "            if 'ycbcr' in color_spaces:\n",
    "                if normalize: \n",
    "                    img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "                else: \n",
    "                    img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "                processed_data['ycbcr'].append(img_ycbcr)\n",
    "\n",
    "            labels.append(label)\n",
    "            total_images_processed += 1\n",
    "\n",
    "    print(f\"\\nToplam işlenen görüntü: {total_images_processed}, atlanan: {total_images_skipped}\")\n",
    "\n",
    "    output_dtype = np.float32 if normalize else np.uint8\n",
    "    numpy_data = {}\n",
    "    for cs, data_list in processed_data.items():\n",
    "        if data_list:\n",
    "            try:\n",
    "                numpy_data[cs] = np.array(data_list, dtype=output_dtype)\n",
    "            except ValueError as e:\n",
    "                print(f\"Hata: {cs} verisi için numpy dizisi oluşturulurken şekil hatası: {e}\")\n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype) \n",
    "        else:\n",
    "            if cs == 'gray':\n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype).reshape(0, target_size[1], target_size[0])\n",
    "            else:\n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype).reshape(0, target_size[1], target_size[0], 3)\n",
    "\n",
    "    numpy_data['labels'] = np.array(labels, dtype=np.int32)\n",
    "    return numpy_data\n",
    "\n",
    "\n",
    "def load_prep_dfire(split_root_path, target_size, fire_class_ids,\n",
    "    color_spaces=['bgr', 'hsv', 'ycbcr'], normalize=1):\n",
    "    processed_data = {cs: [] for cs in color_spaces + ['gray']}\n",
    "    labels = []\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    annotation_extension = '.txt'\n",
    "    images_dir = os.path.join(split_root_path, 'images')\n",
    "    labels_dir = os.path.join(split_root_path, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir):\n",
    "        print(f\"Hata: Resim dizini bulunamadı: {images_dir}\")\n",
    "        return None\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Hata: Etiket dizini bulunamadı: {labels_dir}\")\n",
    "        \n",
    "        print(\"Uyarı: Etiket dizini bulunamadı, tüm görüntüler yangınsız (0) olarak etiketlenecektir.\")\n",
    "        labels_dir = None \n",
    "\n",
    "    all_image_files = []\n",
    "    if os.path.isdir(images_dir):\n",
    "        all_image_files = [f for f in os.listdir(images_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "\n",
    "    if not all_image_files:\n",
    "         print(f\"Uyarı: {images_dir} dizininde görüntü dosyası bulunamadı.\")\n",
    "         return None\n",
    "\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped_read = 0\n",
    "    total_images_skipped_annotation = 0 \n",
    "    images_with_fire = 0\n",
    "    images_without_fire = 0\n",
    "\n",
    "    print(f\"{os.path.basename(split_root_path)} bölünmesindeki {len(all_image_files)} görüntü işleniyor...\")\n",
    "    for filename in tqdm(all_image_files, desc=f\"İşleniyor {os.path.basename(split_root_path)}\"):\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        image_name_without_ext = os.path.splitext(filename)[0]\n",
    "        annotation_path = os.path.join(labels_dir, image_name_without_ext + annotation_extension) if labels_dir else None\n",
    "\n",
    "        has_fire = False\n",
    "        if annotation_path and os.path.exists(annotation_path):\n",
    "            try:\n",
    "                with open(annotation_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts and len(parts) > 0:\n",
    "                            try:\n",
    "                                class_id = int(parts[0])\n",
    "                                if class_id in fire_class_ids:\n",
    "                                    has_fire = True\n",
    "                                    break \n",
    "                            except ValueError: pass \n",
    "            except Exception as e:\n",
    "                total_images_skipped_annotation += 1\n",
    "                has_fire = False \n",
    "        elif annotation_path and not os.path.exists(annotation_path):\n",
    "            total_images_skipped_annotation += 1\n",
    "            has_fire = False \n",
    "\n",
    "        label = 1 if has_fire else 0\n",
    "\n",
    "        img_bgr = cv2.imread(image_path)\n",
    "        if img_bgr is None:\n",
    "            total_images_skipped_read += 1\n",
    "            continue\n",
    "\n",
    "        img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if normalize:\n",
    "             img_resized = img_resized.astype(np.float32) / 255.0\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "        processed_data['gray'].append(img_gray)\n",
    "\n",
    "        if 'bgr' in color_spaces:\n",
    "             processed_data['bgr'].append(img_resized) \n",
    "        if 'hsv' in color_spaces:\n",
    "            if normalize: \n",
    "                img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "            else: \n",
    "                img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "            processed_data['hsv'].append(img_hsv)\n",
    "        if 'ycbcr' in color_spaces:\n",
    "            \n",
    "            if normalize: \n",
    "                img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "            else: \n",
    "                img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "            processed_data['ycbcr'].append(img_ycbcr)\n",
    "\n",
    "        labels.append(label)\n",
    "        total_images_processed += 1\n",
    "        if has_fire:\n",
    "            images_with_fire += 1\n",
    "        else:\n",
    "            images_without_fire += 1\n",
    "\n",
    "    print(f\"\\nToplam işlenen görüntü: {total_images_processed}\")\n",
    "    print(f\"Görüntü okuma hatası nedeniyle atlanan: {total_images_skipped_read}\")\n",
    "    print(f\"Etiket işleme hatası/yokluğu nedeniyle atlanan: {total_images_skipped_annotation}\")\n",
    "    print(f\"Yangın var (etiket 1): {images_with_fire}, Yangın yok (etiket 0): {images_without_fire}\")\n",
    "\n",
    "    output_dtype = np.float32 if normalize else np.uint8\n",
    "    numpy_data = {}\n",
    "    for cs, data_list in processed_data.items():\n",
    "        if data_list:\n",
    "             try:\n",
    "                numpy_data[cs] = np.array(data_list, dtype=output_dtype)\n",
    "             except ValueError as e:\n",
    "                print(f\"Hata: {cs} verisi için numpy dizisi oluşturulurken şekil hatası: {e}\")\n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype) \n",
    "        else:\n",
    "             if cs == 'gray':\n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype).reshape(0, target_size[1], target_size[0])\n",
    "             else: \n",
    "                numpy_data[cs] = np.array([], dtype=output_dtype).reshape(0, target_size[1], target_size[0], 3)\n",
    "\n",
    "    numpy_data['labels'] = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    if numpy_data['labels'].shape[0] != total_images_processed:\n",
    "        print(f\"Uyarı: Etiket sayısı işlenen görüntü sayısıyla eşleşmiyor ({numpy_data['labels'].shape[0]} vs {total_images_processed}).\")\n",
    "\n",
    "    return numpy_data\n",
    "\n",
    "\n",
    "def load_and_extract_features_memory_safe(config, feature_params):\n",
    "    dataset_choice = config.get('dataset_choice', 'dfire')\n",
    "    data_root = config.get('data_root')\n",
    "    target_size = config.get('target_img_size')\n",
    "    color_spaces_to_load = config.get('color_spaces_to_load', ['bgr', 'hsv', 'ycbcr'])\n",
    "    normalize_pixels = config.get('normalize_pixels', 1)\n",
    "    fire_class_ids = config.get('fire_class_ids', [])\n",
    "\n",
    "    if not data_root or not target_size:\n",
    "        print(\"Config 'data_root' or 'target_img_size' is missing.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    image_label_pairs = []\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    annotation_extension = '.txt'\n",
    "\n",
    "    images_dir = os.path.join(data_root, 'images')\n",
    "    labels_dir = os.path.join(data_root, 'labels')\n",
    "    if not os.path.isdir(images_dir):\n",
    "        print(f\"Images directory not found: {images_dir}\")\n",
    "        return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Labels directory not found: {labels_dir}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    all_image_files = [f for f in os.listdir(images_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "    if not all_image_files:\n",
    "         print(f\"No image files found in {images_dir}\")\n",
    "         return np.array([]), np.array([])\n",
    "\n",
    "    print(f\"Scanning {len(all_image_files)} images in {images_dir}...\")\n",
    "    for filename in tqdm(all_image_files, desc=\"Determining Labels\"):\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        image_name_without_ext = os.path.splitext(filename)[0]\n",
    "        annotation_path = os.path.join(labels_dir, image_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        image_label_pairs.append((image_path, label))\n",
    "\n",
    "    if not image_label_pairs:\n",
    "        print(\"No image/label pairs found.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped_reading = 0\n",
    "    total_images_skipped_feature = 0\n",
    "\n",
    "    print(f\"Processing {len(image_label_pairs)} images and extracting features...\")\n",
    "    for image_path, label in tqdm(image_label_pairs, desc=\"dfire memsafe feat exc.\"):\n",
    "        img_bgr = cv2.imread(image_path)\n",
    "        if img_bgr is None:\n",
    "            total_images_skipped_reading += 1\n",
    "            continue\n",
    "        img_resized = img_bgr\n",
    "        if target_size:\n",
    "            img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        if normalize_pixels:\n",
    "            img_resized = img_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "        img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "        img_dict_single = {'gray': img_gray}\n",
    "        if 'bgr' in color_spaces_to_load:\n",
    "            img_dict_single['bgr'] = img_resized\n",
    "        if 'hsv' in color_spaces_to_load:\n",
    "            if normalize_pixels:\n",
    "                img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "            else:\n",
    "                img_hsv = cv2.cvtColor(img_resized, cv2.COLOR_BGR2HSV)\n",
    "            img_dict_single['hsv'] = img_hsv\n",
    "\n",
    "        if 'ycbcr' in color_spaces_to_load:\n",
    "             \n",
    "             if normalize_pixels:\n",
    "                img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "             else:\n",
    "                 \n",
    "                 img_ycbcr = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YCrCb)\n",
    "             img_dict_single['ycbcr'] = img_ycbcr\n",
    "\n",
    "        features_single = combine_features(img_dict_single, feature_params)\n",
    "\n",
    "        if features_single.size > 0:\n",
    "            all_features_list.append(features_single)\n",
    "            all_labels_list.append(label)\n",
    "            total_images_processed += 1\n",
    "        else:\n",
    "            total_images_skipped_feature += 1\n",
    "\n",
    "    print(f\"Total images processed for features: {total_images_processed}\")\n",
    "    if total_images_skipped_reading > 0:\n",
    "        print(f\"Total images skipped due to read error: {total_images_skipped_reading}\")\n",
    "    if total_images_skipped_feature > 0:\n",
    "        print(f\"Total images skipped due to feature extraction error/empty features: {total_images_skipped_feature}\")\n",
    "\n",
    "\n",
    "    if not all_features_list:\n",
    "        print(\"No features extracted from any image.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    try:\n",
    "        features_array = np.vstack(all_features_list).astype(np.float32)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error stacking features: {e}. Ensure all feature vectors have the same size.\")\n",
    "        return np.array([]), np.array([])\n",
    "    labels_array = np.array(all_labels_list, dtype=np.int32)\n",
    "    print(f\"Successfully extracted features for {features_array.shape[0]} images.\")\n",
    "    return features_array, labels_array\n",
    "\n",
    "base_dir = '..'\n",
    "MODELS_DIR = os.path.join(base_dir, 'models')\n",
    "\n",
    "TRANSFORMERS_DIR = MODELS_DIR\n",
    "\n",
    "SCALER_FILENAME = 'scaler.pkl' \n",
    "PCA_FILENAME = 'pca_model.pkl' \n",
    "SCALER_PATH = os.path.join(TRANSFORMERS_DIR, SCALER_FILENAME)\n",
    "PCA_PATH = os.path.join(TRANSFORMERS_DIR, PCA_FILENAME)\n",
    "\n",
    "print(\"--- Test Verisi Yükleme ve Ön İşleme ---\")\n",
    "config_test_dfire = get_config('dfire')\n",
    "feature_params = get_feature_params()\n",
    "processed_test_data_dict = load_and_preprocess_data(config_test_dfire)\n",
    "X_test_raw_features = np.array([])\n",
    "y_test = np.array([])\n",
    "\n",
    "if processed_test_data_dict:\n",
    "    X_test_raw_features, y_test = extract_features(processed_test_data_dict, feature_params)\n",
    "    if X_test_raw_features.shape[0] == 0:\n",
    "        print(\"\\nÖzellik çıkarma başarısız oldu veya test seti boş.\")\n",
    "else:\n",
    "    print(\"\\nTest verisi yükleme veya ön işleme başarısız oldu.\")\n",
    "\n",
    "X_test_processed = X_test_raw_features \n",
    "\n",
    "loaded_scaler = None\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    try:\n",
    "        print(f\"\\nScaler yükleniyor: {SCALER_PATH}\")\n",
    "        loaded_scaler = joblib.load(SCALER_PATH)\n",
    "        print(\"Test özellikleri ölçeklendiriliyor...\")\n",
    "        X_test_processed = scale_features_transform(X_test_raw_features, loaded_scaler)\n",
    "        if X_test_processed is None:\n",
    "            print(\"Ölçeklendirme sonrası veri boş.\")\n",
    "            exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Hata: Scaler yüklenirken veya uygulanırken hata oluştu: {e}\")\n",
    "        loaded_scaler = None \n",
    "        X_test_processed = X_test_raw_features \n",
    "else:\n",
    "    print(f\"\\nScaler dosyası bulunamadı: {SCALER_PATH}. Özellikler ölçeklendirilmeyecek.\")\n",
    "\n",
    "loaded_pca = None\n",
    "if os.path.exists(PCA_PATH):\n",
    "    try:\n",
    "        print(f\"\\nPCA modeli yükleniyor: {PCA_PATH}\")\n",
    "        loaded_pca = joblib.load(PCA_PATH)\n",
    "        print(\"Özelliklere PCA uygulanıyor...\")\n",
    "        X_test_processed = apply_pca_transform(X_test_processed, loaded_pca) \n",
    "        if X_test_processed is None:\n",
    "             print(\"PCA sonrası veri boş.\")\n",
    "             exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Hata: PCA modeli yüklenirken veya uygulanırken hata oluştu: {e}\")\n",
    "        loaded_pca = None \n",
    "        \n",
    "else:\n",
    "    print(f\"\\nPCA dosyası bulunamadı: {PCA_PATH}. Boyut indirgeme uygulanmayacak.\")\n",
    "\n",
    "X_test_final = X_test_processed\n",
    "if X_test_final.shape[0] == 0 or y_test.shape[0] == 0 or X_test_final.shape[0] != y_test.shape[0]:\n",
    "    print(\"\\nTest için hazırlanan özellik veya etiket verisinde sorun var. Testler başlatılamıyor.\")\n",
    "    print(f\"X_test_final shape: {X_test_final.shape}, y_test shape: {y_test.shape}\")\n",
    "    exit()\n",
    "print(\"\\n--- Modeller Test Ediliyor ---\")\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    exit()\n",
    "model_files = [f for f in os.listdir(MODELS_DIR) if f.endswith('.pkl')]\n",
    "if not model_files:\n",
    "    print(f\"'{MODELS_DIR}' dizininde test edilecek .pkl dosyası bulunamadı.\")\n",
    "else:\n",
    "    print(f\"'{MODELS_DIR}' dizininde bulunan modeller: {len(model_files)} adet\")\n",
    "    for model_file in model_files:\n",
    "        print(f\"\\n--- Model: {model_file} ---\")\n",
    "        model_path = os.path.join(MODELS_DIR, model_file)\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model = joblib.load(model_path)\n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"Model başarıyla yüklendi ({load_time:.4f} s).\")\n",
    "            print(\"Test veri seti üzerinde tahminler yapılıyor...\")\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(X_test_final)\n",
    "            predict_time = time.time() - start_time\n",
    "            print(f\"Tahminler tamamlandı ({predict_time:.4f} s).\")\n",
    "            print(\"\\nSınıflandırma Raporu:\")\n",
    "            print(classification_report(y_test, predictions))\n",
    "        except Exception as e:\n",
    "            exit()\n",
    "print(\"\\n--- Tüm Model Testleri Tamamlandı ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
