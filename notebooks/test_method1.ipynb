{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6717352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Testing on D-Fire Test Set ---\n",
      "Using dataset: dfire (test split)\n",
      "Data root: ..\\data_subsets\\D-Fire\\test\n",
      "Target image size: (128, 128)\n",
      "Color spaces loaded: ['bgr', 'hsv', 'ycbcr']\n",
      "Normalize pixels: True\n",
      "\n",
      "D-Fire Fire Class IDs considered fire: [0, 1]\n",
      "\n",
      "\n",
      "Using Test Configuration:\n",
      "  Data Root: ..\\data_subsets\\D-Fire\\test\n",
      "  Split: test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Feature extraction complete.\n",
      "Total images initially found: 1635\n",
      "Images skipped (read error): 0\n",
      "Images skipped (feature error): 0\n",
      "Images successfully processed for features: 1635\n",
      "Raw features array shape: (1635, 8526)\n",
      "Labels array shape: (1635,)\n",
      "\n",
      "Loaded initial scaler from: ..\\models\\scaler_initial.pkl\n",
      "\n",
      "Searching for saved models in: ..\\models\n",
      "Found 10 potential saved models.\n",
      "\n",
      "Processing saved model file: kagglelightgbm_best_model_Scaled_Corr50%.pkl\n",
      "   Identified Model Type: LightGBM\n",
      "   Identified Feature Set: Scaled_Corr50%\n",
      "   Model expects 4263 features.\n",
      "   Loaded feature selection transformer from: ..\\models\\selector_Scaled_Corr50%.pkl\n",
      "Features after scaling: (1635, 8526)\n",
      "Features after selection: (1635, 4263)\n",
      "\n",
      "--- Evaluating LightGBM on D-Fire Test Set (Scaled_Corr50%) ---\n",
      "Prediction duration: 3.1215 seconds\n",
      "Accuracy: 0.4985\n",
      "Precision: 0.4993\n",
      "Recall (Sensitivity): 0.9071\n",
      "F1 Score: 0.6441\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr50%):\n",
      "[[ 73 744]\n",
      " [ 76 742]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: kagglelightgbm_best_model_Scaled_Corr75%.pkl\n",
      "   Identified Model Type: LightGBM\n",
      "   Identified Feature Set: Scaled_Corr75%\n",
      "   Model expects 6394 features.\n",
      "   Loaded feature selection transformer from: ..\\models\\selector_Scaled_Corr75%.pkl\n",
      "Features after scaling: (1635, 8526)\n",
      "Features after selection: (1635, 6394)\n",
      "\n",
      "--- Evaluating LightGBM on D-Fire Test Set (Scaled_Corr75%) ---\n",
      "Prediction duration: 0.0450 seconds\n",
      "Accuracy: 0.4887\n",
      "Precision: 0.4940\n",
      "Recall (Sensitivity): 0.9022\n",
      "F1 Score: 0.6384\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr75%):\n",
      "[[ 61 756]\n",
      " [ 80 738]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: kagglemlp_best_model_Scaled_Corr50%.pkl\n",
      "   Error loading or processing model kagglemlp_best_model_Scaled_Corr50%.pkl: <class 'numpy.random._mt19937.MT19937'> is not a known BitGenerator module.. Skipping evaluation.\n",
      "\n",
      "Processing saved model file: kagglesvm_best_model_Scaled_All.pkl\n",
      "   Identified Model Type: SVM\n",
      "   Identified Feature Set: Scaled_All\n",
      "   Model expects 8526 features.\n",
      "Features after scaling: (1635, 8526)\n",
      "No feature selection applied.\n",
      "\n",
      "--- Evaluating SVM on D-Fire Test Set (Scaled_All) ---\n",
      "Prediction duration: 2.5650 seconds\n",
      "Accuracy: 0.5498\n",
      "Precision: 0.5299\n",
      "Recall (Sensitivity): 0.8875\n",
      "F1 Score: 0.6636\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_All):\n",
      "[[173 644]\n",
      " [ 92 726]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: Kaggle_lightgbm_best_model_Scaled_Corr50%.pkl\n",
      "   Identified Model Type: LightGBM\n",
      "   Identified Feature Set: Scaled_Corr50%\n",
      "   Model expects 4263 features.\n",
      "   Loaded feature selection transformer from: ..\\models\\selector_Scaled_Corr50%.pkl\n",
      "Features after scaling: (1635, 8526)\n",
      "Features after selection: (1635, 4263)\n",
      "\n",
      "--- Evaluating LightGBM on D-Fire Test Set (Scaled_Corr50%) ---\n",
      "Prediction duration: 0.0230 seconds\n",
      "Accuracy: 0.4985\n",
      "Precision: 0.4993\n",
      "Recall (Sensitivity): 0.9071\n",
      "F1 Score: 0.6441\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr50%):\n",
      "[[ 73 744]\n",
      " [ 76 742]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: Kaggle_svm_best_model_Scaled_All.pkl\n",
      "   Identified Model Type: SVM\n",
      "   Identified Feature Set: Scaled_All\n",
      "   Model expects 8526 features.\n",
      "Features after scaling: (1635, 8526)\n",
      "No feature selection applied.\n",
      "\n",
      "--- Evaluating SVM on D-Fire Test Set (Scaled_All) ---\n",
      "Prediction duration: 2.5621 seconds\n",
      "Accuracy: 0.5498\n",
      "Precision: 0.5299\n",
      "Recall (Sensitivity): 0.8875\n",
      "F1 Score: 0.6636\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_All):\n",
      "[[173 644]\n",
      " [ 92 726]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: lightgbm_best_model_Scaled_Corr75%.pkl\n",
      "   Identified Model Type: LightGBM\n",
      "   Identified Feature Set: Scaled_Corr75%\n",
      "   Model expects 6394 features.\n",
      "   Loaded feature selection transformer from: ..\\models\\selector_Scaled_Corr75%.pkl\n",
      "Features after scaling: (1635, 8526)\n",
      "Features after selection: (1635, 6394)\n",
      "\n",
      "--- Evaluating LightGBM on D-Fire Test Set (Scaled_Corr75%) ---\n",
      "Prediction duration: 0.0480 seconds\n",
      "Accuracy: 0.6318\n",
      "Precision: 0.6314\n",
      "Recall (Sensitivity): 0.6345\n",
      "F1 Score: 0.6329\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr75%):\n",
      "[[514 303]\n",
      " [299 519]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing saved model file: mlp_best_model_Scaled_RFE75%.pkl\n",
      "   Error loading or processing model mlp_best_model_Scaled_RFE75%.pkl: <class 'numpy.random._mt19937.MT19937'> is not a known BitGenerator module.. Skipping evaluation.\n",
      "\n",
      "Processing saved model file: svm_best_model_Scaled_All.pkl\n",
      "   Identified Model Type: SVM\n",
      "   Identified Feature Set: Scaled_All\n",
      "   Model expects 8626 features.\n",
      "Features after scaling: (1635, 8526)\n",
      "No feature selection applied.\n",
      "   Skipping evaluation for SVM on Scaled_All due to dimension mismatch.\n",
      "   Processed test features have 8526 features, but model expects 8626.\n",
      "\n",
      "Processing saved model file: svm_best_model_Scaled_RFE75%.pkl\n",
      "   Identified Model Type: SVM\n",
      "   Identified Feature Set: Scaled_RFE75%\n",
      "   Model expects 6394 features.\n",
      "   Loaded feature selection transformer from: ..\\models\\selector_Scaled_RFE75%.pkl\n",
      "Features after scaling: (1635, 8526)\n",
      "Features after selection: (1635, 6394)\n",
      "\n",
      "--- Evaluating SVM on D-Fire Test Set (Scaled_RFE75%) ---\n",
      "Prediction duration: 8.5304 seconds\n",
      "Accuracy: 0.8220\n",
      "Precision: 0.8306\n",
      "Recall (Sensitivity): 0.8093\n",
      "F1 Score: 0.8198\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE75%):\n",
      "[[682 135]\n",
      " [156 662]]\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Model Testing Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts and len(parts) > 0:\n",
    "                    if parts[0].isdigit():\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id in fire_class_ids:\n",
    "                            return True\n",
    "    except Exception as e: pass\n",
    "    return False\n",
    "\n",
    "def extract_color_histograms(img_processed, color_space, bins):\n",
    "    histograms = []\n",
    "    ranges = {\n",
    "        'hsv': {'float': ([0, 1], [0, 1], [0, 1]), 'uint8': ([0, 180], [0, 256], [0, 256])},\n",
    "        'ycbcr': {'float': ([0, 1], [-0.5, 0.5], [-0.5, 0.5]), 'uint8': ([0, 256], [0, 256], [0, 256])}\n",
    "    }\n",
    "    channel_indices = {'hsv': [0, 1], 'ycbcr': [1, 2]}\n",
    "    dtype_key = 'float' if img_processed.dtype in [np.float32, np.float64] else 'uint8'\n",
    "    if color_space in ranges and color_space in channel_indices:\n",
    "        for i in channel_indices[color_space]:\n",
    "            if img_processed.dtype != np.float32 and img_processed.dtype != np.uint8:\n",
    "                 img_processed = img_processed.astype(np.float32 if normalize_pixels else np.uint8)\n",
    "                 dtype_key = 'float' if img_processed.dtype in [np.float32, np.float64] else 'uint8'\n",
    "            current_range = ranges[color_space][dtype_key][i]\n",
    "            try:\n",
    "                hist = cv2.calcHist([img_processed], [i], None, [bins], current_range)\n",
    "                histograms.append(hist.flatten())\n",
    "            except Exception as e: pass\n",
    "    if histograms:\n",
    "        return np.concatenate(histograms)\n",
    "    else:\n",
    "        return np.array([]) \n",
    "\n",
    "def extract_lbp_features(img_gray, radius, n_points, method):\n",
    "    if img_gray is None or img_gray.size == 0:\n",
    "         return np.array([])\n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)\n",
    "    if n_points is None:\n",
    "        n_points = 8 * radius\n",
    "\n",
    "    try:\n",
    "        lbp_image = local_binary_pattern(img_gray, n_points, radius, method=method)\n",
    "        if method == 'uniform' or method == 'nri_uniform':\n",
    "            n_bins = int(n_points + 2) \n",
    "            hist_range = (0, n_bins)\n",
    "        elif method == 'ror':\n",
    "            n_bins = int(n_points / radius + 2) \n",
    "            hist_range = (0, n_bins)\n",
    "        else: \n",
    "            n_bins = int(2**n_points)\n",
    "            hist_range = (0, n_bins)\n",
    "\n",
    "        lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=n_bins, range=hist_range)\n",
    "        lbp_hist = lbp_hist.astype(np.float32) \n",
    "        if lbp_hist.sum() > 0:\n",
    "            lbp_hist /= lbp_hist.sum()\n",
    "        return lbp_hist.flatten()\n",
    "    except Exception as e: return np.array([]) \n",
    "\n",
    "def extract_hog_features(img_gray, orientations, pixels_per_cell, cells_per_block, block_norm):\n",
    "    if img_gray is None or img_gray.size == 0: return np.array([])\n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)\n",
    "    img_h, img_w = img_gray.shape\n",
    "    cell_h, cell_w = pixels_per_cell\n",
    "    block_h, block_w = cells_per_block\n",
    "    min_img_h = cell_h * block_h\n",
    "    min_img_w = cell_w * block_w\n",
    "    if img_h < min_img_h or img_w < min_img_w:\n",
    "        return np.array([]) \n",
    "\n",
    "    try:\n",
    "        hog_features = hog(img_gray, orientations=orientations,\n",
    "                           pixels_per_cell=pixels_per_cell,\n",
    "                           cells_per_block=cells_per_block,\n",
    "                           block_norm=block_norm,\n",
    "                           visualize=False, feature_vector=True)\n",
    "        return hog_features.flatten().astype(np.float32) \n",
    "    except Exception as e:\n",
    "        return np.array([]) \n",
    "\n",
    "\n",
    "def combine_features(img_dict, feature_params):\n",
    "    all_features = []\n",
    "    if 'hsv' in img_dict and img_dict['hsv'] is not None:\n",
    "        hsv_hist = extract_color_histograms(img_dict['hsv'], 'hsv', bins=feature_params.get('hist_bins', 100))\n",
    "        if hsv_hist.size > 0:\n",
    "            all_features.append(hsv_hist)\n",
    "    if 'ycbcr' in img_dict and img_dict['ycbcr'] is not None:\n",
    "        ycbcr_hist = extract_color_histograms(img_dict['ycbcr'], 'ycbcr', bins=feature_params.get('hist_bins', 100))\n",
    "        if ycbcr_hist.size > 0:\n",
    "            all_features.append(ycbcr_hist)\n",
    "    \n",
    "    if 'gray' in img_dict and img_dict['gray'] is not None:\n",
    "        img_gray_processed = img_dict['gray'] \n",
    "        lbp_features = extract_lbp_features(img_gray_processed,\n",
    "                                            radius=feature_params.get('lbp_radius', 3),\n",
    "                                            n_points=feature_params.get('lbp_n_points', None),\n",
    "                                            method=feature_params.get('lbp_method', 'uniform'))\n",
    "        if lbp_features.size > 0:\n",
    "            all_features.append(lbp_features)\n",
    "\n",
    "        \n",
    "        hog_features = extract_hog_features(img_gray_processed,\n",
    "                                           orientations=feature_params.get('hog_orientations', 9),\n",
    "                                           pixels_per_cell=feature_params.get('hog_pixels_per_cell', (8, 8)),\n",
    "                                           cells_per_block=feature_params.get('hog_cells_per_block', (2, 2)),\n",
    "                                           block_norm=feature_params.get('hog_block_norm', 'L2-Hys'))\n",
    "        if hog_features.size > 0:\n",
    "            all_features.append(hog_features)\n",
    "\n",
    "    if all_features:\n",
    "        combined_vector = np.concatenate(all_features)\n",
    "        return combined_vector.astype(np.float32) \n",
    "    else:\n",
    "        return np.array([]) \n",
    "\n",
    "\n",
    "def get_config(dataset_choice):\n",
    "    config = {}\n",
    "    if dataset_choice == 'kaggle':\n",
    "        config['dataset_choice'] = 'kaggle'\n",
    "        config['data_root'] = os.path.join('..', 'data_subsets', 'fire_dataset')\n",
    "        config['target_img_size'] = (128, 128)\n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']\n",
    "        config['normalize_pixels'] = 1\n",
    "        config['fire_class_ids'] = None \n",
    "    elif dataset_choice == 'dfire':\n",
    "        config['dataset_choice'] = 'dfire'\n",
    "        config['dfire_root'] = os.path.join('..', 'data_subsets', 'D-Fire')\n",
    "        config['split_name'] = \"test\"\n",
    "        config['data_root'] = os.path.join(config['dfire_root'], config['split_name'])\n",
    "        config['target_img_size'] = (128, 128)\n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']\n",
    "        config['normalize_pixels'] = 1\n",
    "        config['fire_class_ids'] = [0, 1] \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset choice: {dataset_choice}. Choose 'kaggle' or 'dfire'.\")\n",
    "\n",
    "    print(f\"Using dataset: {config['dataset_choice']} ({config.get('split_name', 'N/A')} split)\")\n",
    "    print(f\"Data root: {config.get('data_root')}\")\n",
    "    print(f\"Target image size: {config['target_img_size']}\")\n",
    "    print(f\"Color spaces loaded: {config['color_spaces_to_load']}\")\n",
    "    print(f\"Normalize pixels: {bool(config['normalize_pixels'])}\\n\")\n",
    "    if dataset_choice == 'dfire':\n",
    "         print(f\"D-Fire Fire Class IDs considered fire: {config['fire_class_ids']}\\n\")\n",
    "    return config\n",
    "\n",
    "def get_feature_params():\n",
    "    feature_params = {\n",
    "        'hist_bins': 100,\n",
    "        'lbp_radius': 3,\n",
    "        'lbp_n_points': None, \n",
    "        'lbp_method': 'uniform',\n",
    "        'hog_orientations': 9,\n",
    "        'hog_pixels_per_cell': (8, 8),\n",
    "        'hog_cells_per_block': (2, 2),\n",
    "        'hog_block_norm': 'L2-Hys'\n",
    "    }\n",
    "    return feature_params\n",
    "\n",
    "def load_test_data_and_extract_raw_features(config, feature_params):\n",
    "    dataset_choice = config.get('dataset_choice')\n",
    "    data_root = config.get('data_root')\n",
    "    target_size = config.get('target_img_size')\n",
    "    color_spaces_to_load = config.get('color_spaces_to_load')\n",
    "    normalize_pixels = config.get('normalize_pixels')\n",
    "    fire_class_ids = config.get('fire_class_ids')\n",
    "\n",
    "    if not data_root or not target_size:\n",
    "        print(\"Error: Data root or target size not specified in config.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    image_label_pairs = []\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    if dataset_choice == 'dfire':\n",
    "        annotation_extension = '.txt'\n",
    "        images_dir = os.path.join(data_root, 'images')\n",
    "        labels_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "        if not os.path.isdir(images_dir) or not os.path.isdir(labels_dir):\n",
    "            print(f\"Error: Images or Labels directory not found in {data_root}\")\n",
    "            return np.array([]), np.array([])\n",
    "        all_image_files = [f for f in os.listdir(images_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "        if not all_image_files:\n",
    "            print(f\"No image files found in {images_dir}\")\n",
    "            return np.array([]), np.array([])\n",
    "        for filename in tqdm(all_image_files, desc=\"Determining Labels\", leave=False):\n",
    "            image_name_without_ext = os.path.splitext(filename)[0]\n",
    "            annotation_path = os.path.join(labels_dir, image_name_without_ext + annotation_extension)\n",
    "            label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "            image_label_pairs.append((os.path.join(images_dir, filename), label))\n",
    "\n",
    "    elif dataset_choice == 'kaggle':\n",
    "        fire_dir = os.path.join(data_root, 'fire_images')\n",
    "        non_fire_dir = os.path.join(data_root, 'non_fire_images')\n",
    "        if not os.path.isdir(fire_dir) or not os.path.isdir(non_fire_dir):\n",
    "            print(f\"Error: 'fire_images' or 'non_fire_images' directory not found in {data_root}\")\n",
    "            return np.array([]), np.array([])\n",
    "        print(\"\\nDetermining Test Set Labels (Kaggle structure)...\")\n",
    "        fire_image_files = [os.path.join(fire_dir, f) for f in os.listdir(fire_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "        for img_path in tqdm(fire_image_files, desc=\"Processing Fire Images\", leave=False):\n",
    "            image_label_pairs.append((img_path, 1))\n",
    "        non_fire_image_files = [os.path.join(non_fire_dir, f) for f in os.listdir(non_fire_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "        for img_path in tqdm(non_fire_image_files, desc=\"Processing Non-Fire Images\", leave=False):\n",
    "            image_label_pairs.append((img_path, 0))\n",
    "        print(\"Test Set Label determination complete.\")\n",
    "    if not image_label_pairs:\n",
    "        print(\"No test images with labels found.\")\n",
    "        return np.array([]), np.array([])\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped_reading = 0\n",
    "    total_images_skipped_feature = 0\n",
    "    for image_path, label in tqdm(image_label_pairs, desc=\"Extracting Raw Features\", leave=False):\n",
    "        img_bgr = cv2.imread(image_path)\n",
    "        if img_bgr is None:\n",
    "            total_images_skipped_reading += 1\n",
    "            continue\n",
    "        try:\n",
    "            img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_dict_single = {} \n",
    "            img_processed_bgr = None\n",
    "            if normalize_pixels:\n",
    "                img_processed_bgr = img_resized.astype(np.float32) / 255.0\n",
    "                img_resized_uint8 = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2HSV).astype(np.float32) / np.array([180, 255, 255], dtype=np.float32) \n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2YCrCb).astype(np.float32) / 255.0 \n",
    "            else: \n",
    "                img_processed_bgr = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2GRAY)\n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2HSV)\n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "            img_dict_single['gray'] = img_gray\n",
    "            if 'bgr' in color_spaces_to_load:\n",
    "                img_dict_single['bgr'] = img_processed_bgr\n",
    "            features_single = combine_features(img_dict_single, feature_params)\n",
    "            if features_single.size > 0:\n",
    "                all_features_list.append(features_single)\n",
    "                all_labels_list.append(label)\n",
    "                total_images_processed += 1\n",
    "            else:\n",
    "                total_images_skipped_feature += 1 \n",
    "        except Exception as e:\n",
    "            total_images_skipped_feature += 1 \n",
    "    print(\"Raw Feature extraction complete.\")\n",
    "    print(f\"Total images initially found: {len(image_label_pairs)}\")\n",
    "    print(f\"Images skipped (read error): {total_images_skipped_reading}\")\n",
    "    print(f\"Images skipped (feature error): {total_images_skipped_feature}\")\n",
    "    print(f\"Images successfully processed for features: {total_images_processed}\")\n",
    "    if not all_features_list:\n",
    "        print(\"No raw features extracted from any test image.\")\n",
    "        return np.array([]), np.array([])\n",
    "    features_array = np.array(all_features_list, dtype=np.float32) \n",
    "    labels_array = np.array(all_labels_list, dtype=np.int32)\n",
    "    print(f\"Raw features array shape: {features_array.shape}\")\n",
    "    print(f\"Labels array shape: {labels_array.shape}\")\n",
    "    return features_array, labels_array\n",
    "\n",
    "def preprocess_features_for_model(X_raw, scaler, selector=None):\n",
    "    if X_raw is None or X_raw.shape[0] == 0:\n",
    "        print(\"Preprocessing skipped: raw features are empty.\")\n",
    "        return np.array([])\n",
    "    if scaler is None:\n",
    "        print(\"Error: Scaler not loaded. Cannot preprocess features.\")\n",
    "        return np.array([])\n",
    "    X_scaled = scaler.transform(X_raw)\n",
    "    print(f\"Features after scaling: {X_scaled.shape}\")\n",
    "    if selector:\n",
    "        X_selected = selector.transform(X_scaled)\n",
    "        print(f\"Features after selection: {X_selected.shape}\")\n",
    "        return X_selected\n",
    "    else:\n",
    "        print(\"No feature selection applied.\")\n",
    "        return X_scaled\n",
    "\n",
    "def evaluate_model_on_test(model, X_test, y_test, model_name, feature_set_name):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0:\n",
    "        print(f\"{model_name} evaluation skipped on {feature_set_name}: model not loaded or test data is empty.\")\n",
    "        return\n",
    "    print(f\"\\n--- Evaluating {model_name} on D-Fire Test Set ({feature_set_name}) ---\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction duration: {end_time - start_time:.4f} seconds\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "SCALER_FILENAME = 'scaler_initial.pkl'\n",
    "SCALER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, SCALER_FILENAME)\n",
    "\n",
    "print(\"--- Starting Model Testing on D-Fire Test Set ---\")\n",
    "try:\n",
    "    test_config = get_config('dfire')\n",
    "    test_config['split_name'] = 'test'\n",
    "    test_config['data_root'] = os.path.join(test_config['dfire_root'], test_config['split_name'])\n",
    "    print(\"\\nUsing Test Configuration:\")\n",
    "    print(f\"  Data Root: {test_config['data_root']}\")\n",
    "    print(f\"  Split: {test_config['split_name']}\\n\")\n",
    "    feature_params = get_feature_params()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Configuration Error: {e}\")\n",
    "    sys.exit(1)\n",
    "X_test_raw, y_test = load_test_data_and_extract_raw_features(test_config, feature_params)\n",
    "expected_raw_feature_count = X_test_raw.shape[1]\n",
    "if X_test_raw.shape[0] == 0:\n",
    "    print(\"\\nNo test data loaded or features extracted. Exiting.\")\n",
    "    sys.exit(1) \n",
    "\n",
    "initial_scaler = None\n",
    "if os.path.exists(SCALER_SAVE_PATH):\n",
    "    try:\n",
    "        initial_scaler = joblib.load(SCALER_SAVE_PATH)\n",
    "        print(f\"\\nLoaded initial scaler from: {SCALER_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading initial scaler {SCALER_SAVE_PATH}: {e}\")\n",
    "else:\n",
    "    print(f\"\\nError: Initial scaler not found at {SCALER_SAVE_PATH}. Cannot preprocess features. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"\\nSearching for saved models in:\", MODEL_SAVE_DIR)\n",
    "saved_models_files = [f for f in os.listdir(MODEL_SAVE_DIR) if f.endswith('.pkl') and '_best_model_' in f]\n",
    "if not saved_models_files:\n",
    "    print(\"No saved models found in\", MODEL_SAVE_DIR)\n",
    "else:\n",
    "    print(f\"Found {len(saved_models_files)} potential saved models.\")\n",
    "    for model_filename in saved_models_files:\n",
    "        print(f\"\\nProcessing saved model file: {model_filename}\")\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "        model = None\n",
    "        feature_selector = None\n",
    "        selector_path = None\n",
    "        model_name = \"Unknown\"\n",
    "        feature_set_name = \"Unknown\"\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            parts = model_filename.replace('.pkl', '').split('_best_model_')\n",
    "            if len(parts) == 2:\n",
    "                prefix = parts[0] \n",
    "                feature_set_name = parts[1] \n",
    "                prefix_lower = prefix.lower()\n",
    "                if 'svm' in prefix_lower:\n",
    "                    model_name = 'SVM'\n",
    "                elif 'lightgbm' in prefix_lower:\n",
    "                    model_name = 'LightGBM'\n",
    "                elif 'mlp' in prefix_lower:\n",
    "                    model_name = 'MLP'\n",
    "                print(f\"   Identified Model Type: {model_name}\")\n",
    "                print(f\"   Identified Feature Set: {feature_set_name}\")\n",
    "                if hasattr(model, 'n_features_in_'):\n",
    "                    expected_model_features = model.n_features_in_\n",
    "                    print(f\"   Model expects {expected_model_features} features.\")\n",
    "                    if feature_set_name != 'Scaled_All':\n",
    "                        selector_filename = f'selector_{feature_set_name}.pkl'\n",
    "                        selector_path = os.path.join(MODEL_SAVE_DIR, selector_filename)\n",
    "                        if os.path.exists(selector_path):\n",
    "                            try:\n",
    "                                feature_selector = joblib.load(selector_path)\n",
    "                                print(f\"   Loaded feature selection transformer from: {selector_path}\")             \n",
    "                            except Exception as e:\n",
    "                                print(f\"   Error loading selector {selector_path}: {e}. Cannot test this model properly.\")\n",
    "                                feature_selector = None      \n",
    "                                model = None\n",
    "                        else:\n",
    "                            print(f\"   Warning: Selector file not found at {selector_path} for feature set {feature_set_name}. Cannot test this model properly.\")\n",
    "                            model = None\n",
    "                            \n",
    "                    if model is not None: \n",
    "                        X_test_processed = preprocess_features_for_model(X_test_raw, initial_scaler, feature_selector)\n",
    "                        if X_test_processed.shape[0] > 0 and X_test_processed.shape[1] == expected_model_features:     \n",
    "                            evaluate_model_on_test(model, X_test_processed, y_test, model_name, feature_set_name)\n",
    "                        elif X_test_processed.shape[0] == 0:\n",
    "                            print(f\"   Skipping evaluation for {model_name} on {feature_set_name}: Processed test features are empty.\")\n",
    "                        else:     \n",
    "                            print(f\"   Skipping evaluation for {model_name} on {feature_set_name} due to dimension mismatch.\")\n",
    "                            print(f\"   Processed test features have {X_test_processed.shape[1]} features, but model expects {expected_model_features}.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"   Warning: Model {model_filename} does not have 'n_features_in_' attribute. Cannot verify dimension match.\")\n",
    "                    print(\"   Attempting preprocessing and evaluation anyway.\")\n",
    "                    X_test_processed = preprocess_features_for_model(X_test_raw, initial_scaler, feature_selector)\n",
    "                    if X_test_processed.shape[0] > 0:\n",
    "                        evaluate_model_on_test(model, X_test_processed, y_test, model_name, feature_set_name)\n",
    "                    else:\n",
    "                        print(f\"   Skipping evaluation for {model_name} on {feature_set_name}: Processed test features are empty.\")\n",
    "            else:\n",
    "                print(f\"   Warning: Filename format not recognized: {model_filename}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error loading or processing model {model_filename}: {e}. Skipping evaluation.\")\n",
    "            continue\n",
    "            \n",
    "print(\"\\n--- Model Testing Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
