{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c9ffa7-e578-49b0-96f2-94621d071a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern, hog \n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None \n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:] \n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    class_dirs = {'fire_images': 1, 'non_fire_images': 0}\n",
    "\n",
    "    for class_name, label in class_dirs.items():\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_path): continue\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None: continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_resized = cv2.resize(img, target_size)\n",
    "                img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "                all_images.append(img_normalized)\n",
    "                all_labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = k_features\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif k_features == 'all': return X_train, X_test, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train, X_test, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train, X_test, None\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"original feature shape: {X_train.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train, X_test, None\n",
    "    else: return X_train, X_test, None\n",
    "    \n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train, X_test, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "    search_cv.fit(X_train, y_train, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:        \n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "            \n",
    "    if flatten_layer is None: raise ValueError()\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f692960-779f-4b24-98e3-943390560905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- extracting cnn features: ---\n",
      "training features shape: (748, 65536)\n",
      "testing features shape: (250, 65536)\n",
      "training labels shape: (748,)\n",
      "testing labels shape: (250,)\n",
      "\n",
      "--- scaling cnn features: ---\n",
      "\n",
      "--- selection & pca: ---\n",
      "\n",
      "corr selection: 75%...\n",
      "original feature shape: (748, 65536)\n",
      "selected feature shape: (748, 49152)\n",
      "\n",
      "corr selection: 50%...\n",
      "original feature shape: (748, 65536)\n",
      "selected feature shape: (748, 32768)\n",
      "\n",
      "rfe selection with 75% (step=0.1)...\n",
      "original feature shape: (748, 65536)\n",
      "selected feature shape: (748, 49152)\n",
      "\n",
      "rfe selection with 50% (step=0.1)...\n",
      "original feature shape: (748, 65536)\n",
      "selected feature shape: (748, 32768)\n",
      "\n",
      "pca with n_components=0.95...\n",
      "original feature shape: (748, 65536)\n",
      "PCA transformed feature shape: (748, 489)\n",
      "variance ratio with 489 components: 0.9501\n",
      "\n",
      "pca with n_components=500...\n",
      "original feature shape: (748, 65536)\n",
      "PCA transformed feature shape: (748, 500)\n",
      "variance ratio with 500 components: 0.9527\n",
      "\n",
      "--- feat sets for tuning: ---\n",
      "- Scaled_All_CNN: 65536 features\n",
      "- Scaled_Corr75%_CNN: 49152 features\n",
      "- Scaled_Corr50%_CNN: 32768 features\n",
      "- Scaled_RFE75%_CNN: 49152 features\n",
      "- Scaled_RFE50%_CNN: 32768 features\n",
      "- Scaled_PCA_95%_CNN: 489 features\n",
      "- Scaled_PCA_500_CNN: 500 features\n"
     ]
    }
   ],
   "source": [
    "data_directory = os.path.join('..', 'data_subsets', 'fire_dataset')\n",
    "target_image_width = 128\n",
    "target_image_height = 128 #? i think i dont have time for image w/h opt.\n",
    "\n",
    "X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "if X_images.shape[0] == 0: exit()\n",
    "\n",
    "print(\"\\n--- extracting cnn features: ---\")\n",
    "cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "print(\"\\n--- scaling cnn features: ---\")\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "feature_sets = {}\n",
    "feature_transformers = {}\n",
    "if X_train_scaled is not None:\n",
    "    feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "    feature_transformers['Scaled_All_CNN'] = scaler\n",
    "else: exit()\n",
    "\n",
    "print(\"\\n--- selection & pca: ---\")\n",
    "original_feature_count = X_train_scaled.shape[1]\n",
    "corr_feature_percentages = ['75%', '50%']\n",
    "for percentage_str in corr_feature_percentages:\n",
    "    print(f\"\\ncorr selection: {percentage_str}...\")\n",
    "    try:\n",
    "        X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "            X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "        )\n",
    "        if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "            feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "            feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "        else: continue\n",
    "    except Exception as e: pass\n",
    "\n",
    "rfe_feature_percentages = ['75%', '50%']\n",
    "rfe_step_val = 0.1\n",
    "rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "for percentage_str in rfe_feature_percentages:\n",
    "    print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "    try:\n",
    "        X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "            X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "        )\n",
    "        if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "            feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "            feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "        else: continue\n",
    "    except Exception as e: pass\n",
    "\n",
    "pca_components = [0.95, 500]\n",
    "for n_comp in pca_components:\n",
    "    print(f\"\\npca with n_components={n_comp}...\")\n",
    "    try:\n",
    "        X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "        if X_train_pca is not None and (isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count or isinstance(n_comp, float)):\n",
    "            fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "            fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "            feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "            feature_transformers[fs_name] = pca_transformer\n",
    "        else: continue\n",
    "    except Exception as e: pass\n",
    "\n",
    "print(\"\\n--- feat sets for tuning: ---\")\n",
    "for name, (X_train_fs, _) in feature_sets.items():\n",
    "    print(f\"- {name}: {X_train_fs.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c48618f-33b0-41cd-ad88-ddb75741eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- model training and randomsearchcv: ---\n",
      "\n",
      "\n",
      "=== train&tune LightGBM (Hybrid) ===\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1256.39 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9627694334532643\n",
      "best CV f1 for LightGBM on Scaled_All_CNN: 0.9628\n",
      "\\LightGBM on the test set using Scaled_All_CNN.\n",
      "duration: 0.1135 seconds\n",
      "accuracy: 0.9360\n",
      "precision: 0.9482\n",
      "recall: 0.9683\n",
      "f1 score: 0.9581\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_All_CNN):\n",
      "[[ 51  10]\n",
      " [  6 183]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_Corr50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 591.59 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9584632404683391\n",
      "best CV f1 for LightGBM on Scaled_Corr50%_CNN: 0.9585\n",
      "\\LightGBM on the test set using Scaled_Corr50%_CNN.\n",
      "duration: 0.2106 seconds\n",
      "accuracy: 0.9480\n",
      "precision: 0.9536\n",
      "recall: 0.9788\n",
      "f1 score: 0.9661\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_Corr50%_CNN):\n",
      "[[ 52   9]\n",
      " [  4 185]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_Corr75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1000.83 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9593377420031612\n",
      "best CV f1 for LightGBM on Scaled_Corr75%_CNN: 0.9593\n",
      "\\LightGBM on the test set using Scaled_Corr75%_CNN.\n",
      "duration: 0.3019 seconds\n",
      "accuracy: 0.9320\n",
      "precision: 0.9526\n",
      "recall: 0.9577\n",
      "f1 score: 0.9551\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_Corr75%_CNN):\n",
      "[[ 52   9]\n",
      " [  8 181]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_PCA_500_CNN (500 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 11.28 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9369185575059582\n",
      "best CV f1 for LightGBM on Scaled_PCA_500_CNN: 0.9369\n",
      "\\LightGBM on the test set using Scaled_PCA_500_CNN.\n",
      "duration: 0.0025 seconds\n",
      "accuracy: 0.9480\n",
      "precision: 0.9632\n",
      "recall: 0.9683\n",
      "f1 score: 0.9657\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_PCA_500_CNN):\n",
      "[[ 54   7]\n",
      " [  6 183]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_PCA_95%_CNN (489 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 10.37 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 50, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9373727908604202\n",
      "best CV f1 for LightGBM on Scaled_PCA_95%_CNN: 0.9374\n",
      "\\LightGBM on the test set using Scaled_PCA_95%_CNN.\n",
      "duration: 0.0021 seconds\n",
      "accuracy: 0.9440\n",
      "precision: 0.9581\n",
      "recall: 0.9683\n",
      "f1 score: 0.9632\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_PCA_95%_CNN):\n",
      "[[ 53   8]\n",
      " [  6 183]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_RFE50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 511.04 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.960204562277101\n",
      "best CV f1 for LightGBM on Scaled_RFE50%_CNN: 0.9602\n",
      "\\LightGBM on the test set using Scaled_RFE50%_CNN.\n",
      "duration: 0.1773 seconds\n",
      "accuracy: 0.9280\n",
      "precision: 0.9430\n",
      "recall: 0.9630\n",
      "f1 score: 0.9529\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_RFE50%_CNN):\n",
      "[[ 50  11]\n",
      " [  7 182]]\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_RFE75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 835.11 seconds\n",
      "\n",
      "best params:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "best CV score:\n",
      "0.9602697853350595\n",
      "best CV f1 for LightGBM on Scaled_RFE75%_CNN: 0.9603\n",
      "\\LightGBM on the test set using Scaled_RFE75%_CNN.\n",
      "duration: 0.2990 seconds\n",
      "accuracy: 0.9480\n",
      "precision: 0.9536\n",
      "recall: 0.9788\n",
      "f1 score: 0.9661\n",
      "\n",
      "confusion matrix (LightGBM on Scaled_RFE75%_CNN):\n",
      "[[ 52   9]\n",
      " [  4 185]]\n",
      "\n",
      "\n",
      "=== train&tune SVM (Hybrid) ===\n",
      "\n",
      "--- tune SVM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 437.26 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9664609632617888\n",
      "best CV f1 for SVM on Scaled_All_CNN: 0.9665\n",
      "\\SVM on the test set using Scaled_All_CNN.\n",
      "duration: 3.9094 seconds\n",
      "accuracy: 0.9400\n",
      "precision: 0.9728\n",
      "recall: 0.9471\n",
      "f1 score: 0.9598\n",
      "\n",
      "confusion matrix (SVM on Scaled_All_CNN):\n",
      "[[ 56   5]\n",
      " [ 10 179]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 315.11 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9692352648300142\n",
      "best CV f1 for SVM on Scaled_Corr50%_CNN: 0.9692\n",
      "\\SVM on the test set using Scaled_Corr50%_CNN.\n",
      "duration: 3.2796 seconds\n",
      "accuracy: 0.9440\n",
      "precision: 0.9730\n",
      "recall: 0.9524\n",
      "f1 score: 0.9626\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr50%_CNN):\n",
      "[[ 56   5]\n",
      " [  9 180]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 370.61 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9665777950074211\n",
      "best CV f1 for SVM on Scaled_Corr75%_CNN: 0.9666\n",
      "\\SVM on the test set using Scaled_Corr75%_CNN.\n",
      "duration: 3.4135 seconds\n",
      "accuracy: 0.9400\n",
      "precision: 0.9728\n",
      "recall: 0.9471\n",
      "f1 score: 0.9598\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr75%_CNN):\n",
      "[[ 56   5]\n",
      " [ 10 179]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_500_CNN (500 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 0.78 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9600852429044334\n",
      "best CV f1 for SVM on Scaled_PCA_500_CNN: 0.9601\n",
      "\\SVM on the test set using Scaled_PCA_500_CNN.\n",
      "duration: 0.0182 seconds\n",
      "accuracy: 0.9080\n",
      "precision: 0.9368\n",
      "recall: 0.9418\n",
      "f1 score: 0.9393\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_500_CNN):\n",
      "[[ 49  12]\n",
      " [ 11 178]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_95%_CNN (489 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 0.87 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9627775130268637\n",
      "best CV f1 for SVM on Scaled_PCA_95%_CNN: 0.9628\n",
      "\\SVM on the test set using Scaled_PCA_95%_CNN.\n",
      "duration: 0.0233 seconds\n",
      "accuracy: 0.9080\n",
      "precision: 0.9368\n",
      "recall: 0.9418\n",
      "f1 score: 0.9393\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_95%_CNN):\n",
      "[[ 49  12]\n",
      " [ 11 178]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 333.10 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9841765580924516\n",
      "best CV f1 for SVM on Scaled_RFE50%_CNN: 0.9842\n",
      "\\SVM on the test set using Scaled_RFE50%_CNN.\n",
      "duration: 2.9984 seconds\n",
      "accuracy: 0.9400\n",
      "precision: 0.9728\n",
      "recall: 0.9471\n",
      "f1 score: 0.9598\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE50%_CNN):\n",
      "[[ 56   5]\n",
      " [ 10 179]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 386.98 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.9734950888032059\n",
      "best CV f1 for SVM on Scaled_RFE75%_CNN: 0.9735\n",
      "\\SVM on the test set using Scaled_RFE75%_CNN.\n",
      "duration: 3.2907 seconds\n",
      "accuracy: 0.9400\n",
      "precision: 0.9728\n",
      "recall: 0.9471\n",
      "f1 score: 0.9598\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE75%_CNN):\n",
      "[[ 56   5]\n",
      " [ 10 179]]\n",
      "\n",
      "\n",
      "=== train&tune Custom_MLP (Hybrid) ===\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "WARNING:tensorflow:5 out of the last 41 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023417215260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000233CF362CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RandomSearch duration: 1517.05 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9702339039830562\n",
      "best CV f1 for Custom_MLP on Scaled_All_CNN: 0.9702\n",
      "\\Custom_MLP on the test set using Scaled_All_CNN.\n",
      "duration: 0.3804 seconds\n",
      "accuracy: 0.9040\n",
      "precision: 0.9508\n",
      "recall: 0.9206\n",
      "f1 score: 0.9355\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_All_CNN):\n",
      "[[ 52   9]\n",
      " [ 15 174]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_Corr50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 868.30 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.6, 'model__activation': 'relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9727302340732468\n",
      "best CV f1 for Custom_MLP on Scaled_Corr50%_CNN: 0.9727\n",
      "\\Custom_MLP on the test set using Scaled_Corr50%_CNN.\n",
      "duration: 0.2579 seconds\n",
      "accuracy: 0.9440\n",
      "precision: 0.9630\n",
      "recall: 0.9630\n",
      "f1 score: 0.9630\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_Corr50%_CNN):\n",
      "[[ 54   7]\n",
      " [  7 182]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_Corr75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1344.00 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9690510342943313\n",
      "best CV f1 for Custom_MLP on Scaled_Corr75%_CNN: 0.9691\n",
      "\\Custom_MLP on the test set using Scaled_Corr75%_CNN.\n",
      "duration: 0.3925 seconds\n",
      "accuracy: 0.9360\n",
      "precision: 0.9727\n",
      "recall: 0.9418\n",
      "f1 score: 0.9570\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_Corr75%_CNN):\n",
      "[[ 56   5]\n",
      " [ 11 178]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_PCA_500_CNN (500 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 148.77 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9582243879038467\n",
      "best CV f1 for Custom_MLP on Scaled_PCA_500_CNN: 0.9582\n",
      "\\Custom_MLP on the test set using Scaled_PCA_500_CNN.\n",
      "duration: 0.1803 seconds\n",
      "accuracy: 0.9280\n",
      "precision: 0.9672\n",
      "recall: 0.9365\n",
      "f1 score: 0.9516\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_PCA_500_CNN):\n",
      "[[ 55   6]\n",
      " [ 12 177]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_PCA_95%_CNN (489 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 153.15 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9586597639862996\n",
      "best CV f1 for Custom_MLP on Scaled_PCA_95%_CNN: 0.9587\n",
      "\\Custom_MLP on the test set using Scaled_PCA_95%_CNN.\n",
      "duration: 0.1861 seconds\n",
      "accuracy: 0.9080\n",
      "precision: 0.9511\n",
      "recall: 0.9259\n",
      "f1 score: 0.9383\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_PCA_95%_CNN):\n",
      "[[ 52   9]\n",
      " [ 14 175]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_RFE50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 969.54 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 128, 'model__dropout_rate': 0.2, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9858952690056926\n",
      "best CV f1 for Custom_MLP on Scaled_RFE50%_CNN: 0.9859\n",
      "\\Custom_MLP on the test set using Scaled_RFE50%_CNN.\n",
      "duration: 0.3048 seconds\n",
      "accuracy: 0.9200\n",
      "precision: 0.9669\n",
      "recall: 0.9259\n",
      "f1 score: 0.9459\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_RFE50%_CNN):\n",
      "[[ 55   6]\n",
      " [ 14 175]]\n",
      "\n",
      "--- tune Custom_MLP on fs: Scaled_RFE75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1500.18 seconds\n",
      "\n",
      "best params:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.6, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "best CV score:\n",
      "0.9760269369796345\n",
      "best CV f1 for Custom_MLP on Scaled_RFE75%_CNN: 0.9760\n",
      "\\Custom_MLP on the test set using Scaled_RFE75%_CNN.\n",
      "duration: 0.5321 seconds\n",
      "accuracy: 0.9080\n",
      "precision: 0.9611\n",
      "recall: 0.9153\n",
      "f1 score: 0.9377\n",
      "\n",
      "confusion matrix (Custom_MLP on Scaled_RFE75%_CNN):\n",
      "[[ 54   7]\n",
      " [ 16 173]]\n",
      "\n",
      "\n",
      "=== results summary for all models ===\n",
      "\n",
      "best cv f1 scores:\n",
      "-------------------------------------------------\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All_CNN: 0.9628\n",
      "  - Scaled_Corr50%_CNN: 0.9585\n",
      "  - Scaled_Corr75%_CNN: 0.9593\n",
      "  - Scaled_PCA_500_CNN: 0.9369\n",
      "  - Scaled_PCA_95%_CNN: 0.9374\n",
      "  - Scaled_RFE50%_CNN: 0.9602\n",
      "  - Scaled_RFE75%_CNN: 0.9603\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All_CNN: 0.9665\n",
      "  - Scaled_Corr50%_CNN: 0.9692\n",
      "  - Scaled_Corr75%_CNN: 0.9666\n",
      "  - Scaled_PCA_500_CNN: 0.9601\n",
      "  - Scaled_PCA_95%_CNN: 0.9628\n",
      "  - Scaled_RFE50%_CNN: 0.9842\n",
      "  - Scaled_RFE75%_CNN: 0.9735\n",
      "\n",
      "Custom_MLP:\n",
      "  - Scaled_All_CNN: 0.9702\n",
      "  - Scaled_Corr50%_CNN: 0.9727\n",
      "  - Scaled_Corr75%_CNN: 0.9691\n",
      "  - Scaled_PCA_500_CNN: 0.9582\n",
      "  - Scaled_PCA_95%_CNN: 0.9587\n",
      "  - Scaled_RFE50%_CNN: 0.9859\n",
      "  - Scaled_RFE75%_CNN: 0.9760\n",
      "\n",
      "test results - f1:\n",
      "----------------------------\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All_CNN: 0.9581\n",
      "  - Scaled_Corr50%_CNN: 0.9661\n",
      "  - Scaled_Corr75%_CNN: 0.9551\n",
      "  - Scaled_PCA_500_CNN: 0.9657\n",
      "  - Scaled_PCA_95%_CNN: 0.9632\n",
      "  - Scaled_RFE50%_CNN: 0.9529\n",
      "  - Scaled_RFE75%_CNN: 0.9661\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All_CNN: 0.9598\n",
      "  - Scaled_Corr50%_CNN: 0.9626\n",
      "  - Scaled_Corr75%_CNN: 0.9598\n",
      "  - Scaled_PCA_500_CNN: 0.9393\n",
      "  - Scaled_PCA_95%_CNN: 0.9393\n",
      "  - Scaled_RFE50%_CNN: 0.9598\n",
      "  - Scaled_RFE75%_CNN: 0.9598\n",
      "\n",
      "Custom_MLP:\n",
      "  - Scaled_All_CNN: 0.9355\n",
      "  - Scaled_Corr50%_CNN: 0.9630\n",
      "  - Scaled_Corr75%_CNN: 0.9570\n",
      "  - Scaled_PCA_500_CNN: 0.9516\n",
      "  - Scaled_PCA_95%_CNN: 0.9383\n",
      "  - Scaled_RFE50%_CNN: 0.9459\n",
      "  - Scaled_RFE75%_CNN: 0.9377\n",
      "\n",
      "=== best combo based on f1's ===\n",
      "best model: LightGBM\n",
      "best fs: Scaled_Corr50%_CNN (32768 features)\n",
      "best cvf1 sc: 0.9585\n",
      "test f1: 0.9661\n",
      "test acc: 0.9480\n",
      "test prec: 0.9536\n",
      "test rec: 0.9788\n",
      "params: {'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "conf.m.:\n",
      "[[ 52   9]\n",
      " [  4 185]]\n",
      "\n",
      "--- Saving Best Model Per Algorithm (Based on Test F1) ---\n",
      "   Saved global StandardScaler: ..\\models\\hybrid_kaggle_m1_global_scaler.pkl\n",
      "\n",
      "Processing LightGBM...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_lightgbm_best_model_Scaled_Corr50%_CNN.pkl\n",
      "   Saved feature selection transformer: ..\\models\\hybrid_kaggle_m1_selector_Scaled_Corr50%_CNN.pkl\n",
      "\n",
      "Processing SVM...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_svm_best_model_Scaled_Corr50%_CNN.pkl\n",
      "   Saved feature selection transformer: ..\\models\\hybrid_kaggle_m1_selector_Scaled_Corr50%_CNN.pkl\n",
      "\n",
      "Processing Custom_MLP...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_custom_mlp_best_model_Scaled_Corr50%_CNN.keras\n",
      "   Saved feature selection transformer: ..\\models\\hybrid_kaggle_m1_selector_Scaled_Corr50%_CNN.pkl\n",
      "\n",
      "--- all done!!! ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- model training and randomsearchcv: ---\")\n",
    "import lightgbm as lgb\n",
    "models_to_tune = {\n",
    "     'LightGBM': {\n",
    "        'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1, n_jobs=4),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 80, 120], \n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [8, 15],\n",
    "            'num_leaves': [20, 40, 60],\n",
    "            'subsample': [0.8, 0.9],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "            'min_split_gain': [0.1],\n",
    "            'min_child_samples': [5]\n",
    "    }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'estimator': SVC(random_state=42),\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10, 50],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'Custom_MLP': {\n",
    "        'estimator': SklearnKerasClassifier(\n",
    "            model=create_custom_mlp,\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "        ),\n",
    "        'param_grid': {\n",
    "            'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "            'model__hidden_layer_2_neurons': [0, 64, 128],\n",
    "            'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "            'model__activation': ['relu', 'leaky_relu'],\n",
    "            'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scoring_metric = 'f1'\n",
    "all_results = {}\n",
    "best_overall_test_score = -np.inf\n",
    "best_overall_combination = None\n",
    "best_overall_trained_model = None\n",
    "best_overall_X_test = None\n",
    "best_overall_transformer = None\n",
    "\n",
    "for model_name, model_config in models_to_tune.items():\n",
    "    all_results[model_name] = {}\n",
    "    estimator = model_config['estimator']\n",
    "    param_distributions = model_config['param_grid']\n",
    "    n_iter_search = model_config.get('n_iter', 8)\n",
    "    print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\")\n",
    "    for fs_name in sorted(feature_sets.keys()):\n",
    "        X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "        print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "        if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "        tuned_search = tune_model_hyperparameters(\n",
    "            estimator,\n",
    "            X_train_fs,\n",
    "            y_train,\n",
    "            param_grid=param_distributions,\n",
    "            cv_strategy=cv_strategy,\n",
    "            scoring=scoring_metric,\n",
    "            search_method='RandomSearch',\n",
    "            n_iter=n_iter_search,\n",
    "            validation_split_keras=0.2\n",
    "        )\n",
    "\n",
    "        if tuned_search:\n",
    "            best_model_for_combination = tuned_search.best_estimator_\n",
    "            best_cv_score = tuned_search.best_score_\n",
    "            best_params = tuned_search.best_params_\n",
    "            print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "            test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "            all_results[model_name][fs_name] = {\n",
    "                'best_cv_score': best_cv_score,\n",
    "                'best_params': best_params,\n",
    "                'test_metrics': test_metrics,\n",
    "                'trained_model': best_model_for_combination,\n",
    "                'transformer': feature_transformers.get(fs_name)\n",
    "            }\n",
    "            if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                best_overall_test_score = test_metrics['f1_score']\n",
    "                best_overall_combination = (model_name, fs_name)\n",
    "                best_overall_trained_model = best_model_for_combination\n",
    "                best_overall_X_test = X_test_fs\n",
    "                best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "print(\"\\n\\n=== results summary for all models ===\")\n",
    "if not all_results: pass\n",
    "else:\n",
    "    print(\"\\nbest cv f1 scores:\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                result = fs_results[fs_name]\n",
    "                cv_score = result.get('best_cv_score', float('nan'))\n",
    "                print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "        else: pass\n",
    "\n",
    "    print(\"\\ntest results - f1:\")\n",
    "    print(\"----------------------------\")\n",
    "    best_f1_per_model = {}\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            best_test_f1_for_model = -np.inf\n",
    "            best_fs_name_for_model = None\n",
    "\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                result = fs_results[fs_name]\n",
    "                test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                    best_test_f1_for_model = test_f1\n",
    "                    best_fs_name_for_model = fs_name\n",
    "            if best_fs_name_for_model:\n",
    "                best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "        else: continue\n",
    "\n",
    "    print(\"\\n=== best combo based on f1's ===\")\n",
    "    if best_overall_combination:\n",
    "        model_name, fs_name = best_overall_combination\n",
    "        best_result = all_results[model_name][fs_name]\n",
    "        test_metrics = best_result['test_metrics']\n",
    "\n",
    "        print(f\"best model: {model_name}\")\n",
    "        actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "        print(f\"best fs: {fs_name} ({actual_feature_count} features)\")\n",
    "        print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"test f1: {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"test acc: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"test prec: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"test rec: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"params: {best_result['best_params']}\\n\")\n",
    "        print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "    else: pass\n",
    "\n",
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "print(\"\\n--- Saving Best Model Per Algorithm (Based on Test F1) ---\")\n",
    "if 'scaler' in locals() and scaler is not None:\n",
    "    try:\n",
    "        joblib.dump(scaler, os.path.join(MODEL_SAVE_DIR, 'hybrid_kaggle_m1_global_scaler.pkl'))\n",
    "        print(f\"   Saved global StandardScaler: {os.path.join(MODEL_SAVE_DIR, 'hybrid_kaggle_m1_global_scaler.pkl')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving global StandardScaler: {e}\")\n",
    "else:\n",
    "    print(\"   Global StandardScaler not found or is None, skipping save.\")\n",
    "if 'best_f1_per_model' not in locals() or not best_result:\n",
    "     print(\"Could not determine best feature set per model. ??\")\n",
    "else:\n",
    "    for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        if model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "            best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "            model_to_save = best_combination_results.get('trained_model')\n",
    "            transformer_to_save = best_combination_results.get('transformer')\n",
    "\n",
    "            if model_to_save:\n",
    "                is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                model_filename = f'hybrid_kaggle_m1_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                try:\n",
    "                    if is_keras_model:\n",
    "                        model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                    else:\n",
    "                        joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                    print(f\"   Saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Error saving {model_name} model to {MODEL_SAVE_PATH_ALG}: {e}\")\n",
    "            else:\n",
    "                print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "            if transformer_to_save and best_fs_name_for_model != 'Scaled_All':\n",
    "                 transformer_filename = f'hybrid_kaggle_m1_selector_{best_fs_name_for_model}.pkl'\n",
    "                 TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                 try:\n",
    "                     joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                     print(f\"   Saved feature selection transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                 except Exception as e:\n",
    "                    print(f\"   Error saving transformer for {best_fs_name_for_model} to {TRANSFORMER_SAVE_PATH}: {e}\")\n",
    "            elif best_fs_name_for_model != 'Scaled_All':\n",
    "                print(f\"   Warning: Feature selection transformer not found for {best_fs_name_for_model}.\")\n",
    "        else:\n",
    "            print(f\"No valid results found for the best feature set '{best_fs_name_for_model}' for model {model_name}.\")\n",
    "\n",
    "print(\"\\n--- all done!!! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
