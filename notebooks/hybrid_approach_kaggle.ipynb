{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern, hog \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                if class_id in fire_class_ids: return True\n",
    "    except (ValueError, IOError): pass\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir): return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir): return np.array([]), np.array([])\n",
    "\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    annotation_extension = '.txt'\n",
    "    fire_class_ids = [0, 1] \n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files: return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"dfire prep\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0 \n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e: pass \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None \n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:] \n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = k_features\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif k_features == 'all': return X_train, X_test, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train, X_test, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train, X_test, None\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"original feature shape: {X_train.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train, X_test, None\n",
    "    else: return X_train, X_test, None\n",
    "    \n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train, X_test, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "    search_cv.fit(X_train, y_train, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:        \n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "            \n",
    "    if flatten_layer is None: raise ValueError()\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = os.path.join('..', 'data_subsets', 'D-Fire', 'train')\n",
    "    target_image_width = 128\n",
    "    target_image_height = 128 #? i think i dont have time for image w/h opt.\n",
    "\n",
    "    X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "    if X_images.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- extracting cnn features: ---\")\n",
    "    cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "    features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- scaling cnn features: ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "    if X_train_scaled is not None:\n",
    "        feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "        feature_transformers['Scaled_All_CNN'] = scaler\n",
    "    else: exit()\n",
    "\n",
    "    print(\"\\n--- selection & pca: ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\ncorr selection: {percentage_str}...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']\n",
    "    rfe_step_val = 0.1\n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "        print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "        try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "    \n",
    "    pca_components = [0.95, 500]\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\pca with n_components={n_comp}...\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            if X_train_pca is not None and (isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count or isinstance(n_comp, float)):\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "        \n",
    "    print(\"\\n--- feat sets for tuning: ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "\n",
    "    print(\"\\n--- model training and randomsearchcv: ---\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': SVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [-1, 10, 20],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],\n",
    "                'min_child_samples': [5]\n",
    "            }\n",
    "        },\n",
    "        'Custom_MLP': {\n",
    "            'estimator': SklearnKerasClassifier( \n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0, \n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128], \n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'\n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 8) \n",
    "        print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\")\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch',\n",
    "                n_iter=n_iter_search,\n",
    "                validation_split_keras=0.2 \n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)\n",
    "                }\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "    print(\"\\n\\n=== results summary for all models ===\")\n",
    "    if not all_results: pass\n",
    "    else:\n",
    "        print(\"\\nbest cv f1 scores:\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    cv_score = result.get('best_cv_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "            else: pass\n",
    "\n",
    "        print(\"\\ntest results - f1:\")\n",
    "        print(\"----------------------------\")\n",
    "        best_f1_per_model = {}\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                best_test_f1_for_model = -np.inf\n",
    "                best_fs_name_for_model = None\n",
    "\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                    if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                        best_test_f1_for_model = test_f1\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                if best_fs_name_for_model:\n",
    "                    best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "            else: continue\n",
    "\n",
    "        print(\"\\n=== best combo based on f1's ===\")\n",
    "        if best_overall_combination:\n",
    "            model_name, fs_name = best_overall_combination\n",
    "            best_result = all_results[model_name][fs_name]\n",
    "            test_metrics = best_result['test_metrics']\n",
    "\n",
    "            print(f\"best model: {model_name}\")\n",
    "            actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "            print(f\"best fs: {fs_name} ({actual_feature_count} features)\")\n",
    "            print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\")\n",
    "            print(f\"test f1: {test_metrics['f1_score']:.4f}\")\n",
    "            print(f\"test acc: {test_metrics['accuracy']:.4f}\")\n",
    "            print(f\"test prec: {test_metrics['precision']:.4f}\")\n",
    "            print(f\"test rec: {test_metrics['recall']:.4f}\")\n",
    "            print(f\"params: {best_result['best_params']}\\n\")\n",
    "            print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "        else: pass\n",
    "    \n",
    "    MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    print(\"\\n--- saving best hybrids ---\")\n",
    "    if 'best_f1_per_model' not in locals() or not best_f1_per_model: pass    \n",
    "    else:\n",
    "        for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "            print(f\"\\nprocessing {model_name}...\")\n",
    "            if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "                best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "                model_to_save = best_combination_results.get('trained_model')\n",
    "                transformer_to_save = best_combination_results.get('transformer')\n",
    "                if model_to_save:\n",
    "                    is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                    file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                    model_filename = f'Dfire_hybrid_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                    MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                    try:\n",
    "                        if is_keras_model: model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                        else: joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                        print(f\"   saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                    except Exception as e: pass\n",
    "                else:\n",
    "                    print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "\n",
    "                if transformer_to_save and 'All_CNN' not in best_fs_name_for_model: \n",
    "                     transformer_filename = f'Dfire_hybrid_transformer_{best_fs_name_for_model}.pkl'\n",
    "                     TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                     try:\n",
    "                         joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                         print(f\"   saved feature s/r transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                     except Exception as e: pass\n",
    "            \n",
    "    print(\"\\n--- all done!! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
