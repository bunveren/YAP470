{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e200af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing images from ..\\data_subsets\\fire_dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fire_images: 100%|██████████| 755/755 [00:07<00:00, 98.94it/s] \n",
      "Processing non_fire_images: 100%|██████████| 244/244 [00:03<00:00, 61.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 998 images.\n",
      "\n",
      "--- Initializing and training the CNN classification model ---\n",
      "Starting CNN training...\n",
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - accuracy: 0.6386 - loss: 0.7505 - val_accuracy: 0.7950 - val_loss: 0.4714\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.8686 - loss: 0.3426 - val_accuracy: 0.7750 - val_loss: 0.4494\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 184ms/step - accuracy: 0.9239 - loss: 0.2313 - val_accuracy: 0.8000 - val_loss: 0.4132\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.9211 - loss: 0.2422 - val_accuracy: 0.8150 - val_loss: 0.3801\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 188ms/step - accuracy: 0.9371 - loss: 0.1905 - val_accuracy: 0.8150 - val_loss: 0.3678\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 202ms/step - accuracy: 0.9415 - loss: 0.1828 - val_accuracy: 0.8200 - val_loss: 0.3494\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9522 - loss: 0.1402 - val_accuracy: 0.8200 - val_loss: 0.3445\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9597 - loss: 0.1227 - val_accuracy: 0.8700 - val_loss: 0.3023\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9733 - loss: 0.1165 - val_accuracy: 0.8650 - val_loss: 0.2920\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9688 - loss: 0.1105 - val_accuracy: 0.8850 - val_loss: 0.2838\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9777 - loss: 0.0954 - val_accuracy: 0.9100 - val_loss: 0.2544\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9579 - loss: 0.1302 - val_accuracy: 0.9200 - val_loss: 0.2117\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9845 - loss: 0.0832 - val_accuracy: 0.9400 - val_loss: 0.2005\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9796 - loss: 0.0890 - val_accuracy: 0.9400 - val_loss: 0.1869\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9880 - loss: 0.0721 - val_accuracy: 0.9350 - val_loss: 0.1813\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9889 - loss: 0.0772 - val_accuracy: 0.9450 - val_loss: 0.1696\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9955 - loss: 0.0650 - val_accuracy: 0.9400 - val_loss: 0.1631\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9876 - loss: 0.0624 - val_accuracy: 0.9350 - val_loss: 0.1809\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9827 - loss: 0.0839 - val_accuracy: 0.9400 - val_loss: 0.1710\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9898 - loss: 0.0777 - val_accuracy: 0.9400 - val_loss: 0.1791\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9842 - loss: 0.0775 - val_accuracy: 0.9400 - val_loss: 0.1940\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9917 - loss: 0.0519 - val_accuracy: 0.9300 - val_loss: 0.2146\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9839 - loss: 0.0699 - val_accuracy: 0.9250 - val_loss: 0.2161\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9843 - loss: 0.0884 - val_accuracy: 0.9300 - val_loss: 0.2136\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9797 - loss: 0.0716 - val_accuracy: 0.9200 - val_loss: 0.2308\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9994 - loss: 0.0433 - val_accuracy: 0.9350 - val_loss: 0.2036\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9801 - loss: 0.0754 - val_accuracy: 0.9350 - val_loss: 0.2012\n",
      "CNN training complete.\n",
      "\n",
      "--- Creating CNN Feature Extractor from the trained CNN model ---\n",
      "\n",
      "--- Extracting CNN features for hybrid model training ---\n",
      "Training features shape: (748, 65536)\n",
      "Testing features shape: (250, 65536)\n",
      "Training labels shape: (748,)\n",
      "Testing labels shape: (250,)\n",
      "\n",
      "--- Scaling CNN features for hybrid models ---\n",
      "Features scaled.\n",
      "\n",
      "--- Performing feature selection & PCA ---\n",
      "\n",
      "Performing correlation selection: 75%...\n",
      "Original feature shape: (748, 65536)\n",
      "Selected feature shape (Correlation 75%): (748, 49152)\n",
      "\n",
      "Performing correlation selection: 50%...\n",
      "Original feature shape: (748, 65536)\n",
      "Selected feature shape (Correlation 50%): (748, 32768)\n",
      "\n",
      "Performing RFE selection with 75% (step=0.1)...\n",
      "Original feature shape: (748, 65536)\n",
      "Selected feature shape (RFE 75%): (748, 49152)\n",
      "\n",
      "Performing RFE selection with 50% (step=0.1)...\n",
      "Original feature shape: (748, 65536)\n",
      "Selected feature shape (RFE 50%): (748, 32768)\n",
      "\n",
      "Performing PCA with n_components=0.95...\n",
      "Original feature shape: (748, 65536)\n",
      "PCA transformed feature shape: (748, 450)\n",
      "Variance ratio with 450 components: 0.9502\n",
      "\n",
      "Performing PCA with n_components=500...\n",
      "Original feature shape: (748, 65536)\n",
      "PCA transformed feature shape: (748, 500)\n",
      "Variance ratio with 500 components: 0.9627\n",
      "\n",
      "--- Available Feature Sets for Tuning ---\n",
      "- Scaled_All_CNN: 65536 features\n",
      "- Scaled_Corr75%_CNN: 49152 features\n",
      "- Scaled_Corr50%_CNN: 32768 features\n",
      "- Scaled_RFE75%_CNN: 49152 features\n",
      "- Scaled_RFE50%_CNN: 32768 features\n",
      "- Scaled_PCA_95%_CNN: 450 features\n",
      "- Scaled_PCA_500_CNN: 500 features\n",
      "\n",
      "--- Model Training and RandomizedSearchCV for Hybrid Models ---\n",
      "\n",
      "\n",
      "=== Training & Tuning LightGBM (Hybrid Model) ===\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_All_CNN (65536 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1435.12 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.951698533872543\n",
      "Best CV f1 for LightGBM on Scaled_All_CNN: 0.9517\n",
      "Evaluating LightGBM on the test set using Scaled_All_CNN...\n",
      "Prediction duration: 0.0995 seconds\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.9579\n",
      "Recall: 0.9630\n",
      "F1 Score: 0.9604\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_All_CNN):\n",
      "[[ 53   8]\n",
      " [  7 182]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 666.22 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 50, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9498288602948891\n",
      "Best CV f1 for LightGBM on Scaled_Corr50%_CNN: 0.9498\n",
      "Evaluating LightGBM on the test set using Scaled_Corr50%_CNN...\n",
      "Prediction duration: 0.1482 seconds\n",
      "Accuracy: 0.9120\n",
      "Precision: 0.9326\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.9424\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr50%_CNN):\n",
      "[[ 48  13]\n",
      " [  9 180]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1056.72 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.948807109906164\n",
      "Best CV f1 for LightGBM on Scaled_Corr75%_CNN: 0.9488\n",
      "Evaluating LightGBM on the test set using Scaled_Corr75%_CNN...\n",
      "Prediction duration: 0.4630 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9519\n",
      "Recall: 0.9418\n",
      "F1 Score: 0.9468\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr75%_CNN):\n",
      "[[ 52   9]\n",
      " [ 11 178]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_PCA_500_CNN (500 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 13.32 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 50, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9469934965820115\n",
      "Best CV f1 for LightGBM on Scaled_PCA_500_CNN: 0.9470\n",
      "Evaluating LightGBM on the test set using Scaled_PCA_500_CNN...\n",
      "Prediction duration: 0.0031 seconds\n",
      "Accuracy: 0.9240\n",
      "Precision: 0.9620\n",
      "Recall: 0.9365\n",
      "F1 Score: 0.9491\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_PCA_500_CNN):\n",
      "[[ 54   7]\n",
      " [ 12 177]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_PCA_95%_CNN (450 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 7.89 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 20, 'n_estimators': 50, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9449017877212201\n",
      "Best CV f1 for LightGBM on Scaled_PCA_95%_CNN: 0.9449\n",
      "Evaluating LightGBM on the test set using Scaled_PCA_95%_CNN...\n",
      "Prediction duration: 0.0045 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9617\n",
      "Recall: 0.9312\n",
      "F1 Score: 0.9462\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_PCA_95%_CNN):\n",
      "[[ 54   7]\n",
      " [ 13 176]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 655.94 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9486706026826012\n",
      "Best CV f1 for LightGBM on Scaled_RFE50%_CNN: 0.9487\n",
      "Evaluating LightGBM on the test set using Scaled_RFE50%_CNN...\n",
      "Prediction duration: 0.1660 seconds\n",
      "Accuracy: 0.9280\n",
      "Precision: 0.9572\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9521\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE50%_CNN):\n",
      "[[ 53   8]\n",
      " [ 10 179]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1083.53 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9470425427436275\n",
      "Best CV f1 for LightGBM on Scaled_RFE75%_CNN: 0.9470\n",
      "Evaluating LightGBM on the test set using Scaled_RFE75%_CNN...\n",
      "Prediction duration: 0.2999 seconds\n",
      "Accuracy: 0.9280\n",
      "Precision: 0.9524\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.9524\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE75%_CNN):\n",
      "[[ 52   9]\n",
      " [  9 180]]\n",
      "\n",
      "\n",
      "=== Training & Tuning SVM (Hybrid Model) ===\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_All_CNN (65536 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1035.66 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.965206134995985\n",
      "Best CV f1 for SVM on Scaled_All_CNN: 0.9652\n",
      "Evaluating SVM on the test set using Scaled_All_CNN...\n",
      "Prediction duration: 8.4005 seconds\n",
      "Accuracy: 0.9360\n",
      "Precision: 0.9676\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9572\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_All_CNN):\n",
      "[[ 55   6]\n",
      " [ 10 179]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 784.20 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
      "\n",
      "Best CV score:\n",
      "0.971237861298154\n",
      "Best CV f1 for SVM on Scaled_Corr50%_CNN: 0.9712\n",
      "Evaluating SVM on the test set using Scaled_Corr50%_CNN...\n",
      "Prediction duration: 17.7214 seconds\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.9579\n",
      "Recall: 0.9630\n",
      "F1 Score: 0.9604\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr50%_CNN):\n",
      "[[ 53   8]\n",
      " [  7 182]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1025.42 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9677565732215657\n",
      "Best CV f1 for SVM on Scaled_Corr75%_CNN: 0.9678\n",
      "Evaluating SVM on the test set using Scaled_Corr75%_CNN...\n",
      "Prediction duration: 7.1963 seconds\n",
      "Accuracy: 0.9360\n",
      "Precision: 0.9676\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9572\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr75%_CNN):\n",
      "[[ 55   6]\n",
      " [ 10 179]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_PCA_500_CNN (500 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 2.48 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 1}\n",
      "\n",
      "Best CV score:\n",
      "0.9653722506023416\n",
      "Best CV f1 for SVM on Scaled_PCA_500_CNN: 0.9654\n",
      "Evaluating SVM on the test set using Scaled_PCA_500_CNN...\n",
      "Prediction duration: 0.0202 seconds\n",
      "Accuracy: 0.9320\n",
      "Precision: 0.9624\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9547\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_PCA_500_CNN):\n",
      "[[ 54   7]\n",
      " [ 10 179]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_PCA_95%_CNN (450 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 2.32 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9651947395548525\n",
      "Best CV f1 for SVM on Scaled_PCA_95%_CNN: 0.9652\n",
      "Evaluating SVM on the test set using Scaled_PCA_95%_CNN...\n",
      "Prediction duration: 0.0153 seconds\n",
      "Accuracy: 0.9320\n",
      "Precision: 0.9725\n",
      "Recall: 0.9365\n",
      "F1 Score: 0.9542\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_PCA_95%_CNN):\n",
      "[[ 56   5]\n",
      " [ 12 177]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 866.50 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.981530086516963\n",
      "Best CV f1 for SVM on Scaled_RFE50%_CNN: 0.9815\n",
      "Evaluating SVM on the test set using Scaled_RFE50%_CNN...\n",
      "Prediction duration: 2.0822 seconds\n",
      "Accuracy: 0.9400\n",
      "Precision: 0.9677\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.9600\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE50%_CNN):\n",
      "[[ 55   6]\n",
      " [  9 180]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1036.76 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9720439690256396\n",
      "Best CV f1 for SVM on Scaled_RFE75%_CNN: 0.9720\n",
      "Evaluating SVM on the test set using Scaled_RFE75%_CNN...\n",
      "Prediction duration: 7.5167 seconds\n",
      "Accuracy: 0.9360\n",
      "Precision: 0.9676\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9572\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE75%_CNN):\n",
      "[[ 55   6]\n",
      " [ 10 179]]\n",
      "\n",
      "\n",
      "=== Training & Tuning Custom_MLP (Hybrid Model) ===\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_All_CNN (65536 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "WARNING:tensorflow:5 out of the last 41 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000199694A7EC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001991EB92660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RandomSearch duration: 1318.75 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9708336086457429\n",
      "Best CV f1 for Custom_MLP on Scaled_All_CNN: 0.9708\n",
      "Evaluating Custom_MLP on the test set using Scaled_All_CNN...\n",
      "Prediction duration: 0.2546 seconds\n",
      "Accuracy: 0.9360\n",
      "Precision: 0.9727\n",
      "Recall: 0.9418\n",
      "F1 Score: 0.9570\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_All_CNN):\n",
      "[[ 56   5]\n",
      " [ 11 178]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_Corr50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 553.94 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9744521683420655\n",
      "Best CV f1 for Custom_MLP on Scaled_Corr50%_CNN: 0.9745\n",
      "Evaluating Custom_MLP on the test set using Scaled_Corr50%_CNN...\n",
      "Prediction duration: 0.1820 seconds\n",
      "Accuracy: 0.9360\n",
      "Precision: 0.9626\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.9574\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_Corr50%_CNN):\n",
      "[[ 54   7]\n",
      " [  9 180]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_Corr75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 941.36 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9745769523114053\n",
      "Best CV f1 for Custom_MLP on Scaled_Corr75%_CNN: 0.9746\n",
      "Evaluating Custom_MLP on the test set using Scaled_Corr75%_CNN...\n",
      "Prediction duration: 0.2363 seconds\n",
      "Accuracy: 0.9320\n",
      "Precision: 0.9574\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.9549\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_Corr75%_CNN):\n",
      "[[ 53   8]\n",
      " [  9 180]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_PCA_500_CNN (500 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 117.06 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9691450328643282\n",
      "Best CV f1 for Custom_MLP on Scaled_PCA_500_CNN: 0.9691\n",
      "Evaluating Custom_MLP on the test set using Scaled_PCA_500_CNN...\n",
      "Prediction duration: 0.1436 seconds\n",
      "Accuracy: 0.9240\n",
      "Precision: 0.9722\n",
      "Recall: 0.9259\n",
      "F1 Score: 0.9485\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_PCA_500_CNN):\n",
      "[[ 56   5]\n",
      " [ 14 175]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_PCA_95%_CNN (450 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 113.52 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.6, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9660081631117269\n",
      "Best CV f1 for Custom_MLP on Scaled_PCA_95%_CNN: 0.9660\n",
      "Evaluating Custom_MLP on the test set using Scaled_PCA_95%_CNN...\n",
      "Prediction duration: 0.1548 seconds\n",
      "Accuracy: 0.9240\n",
      "Precision: 0.9722\n",
      "Recall: 0.9259\n",
      "F1 Score: 0.9485\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_PCA_95%_CNN):\n",
      "[[ 56   5]\n",
      " [ 14 175]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_RFE50%_CNN (32768 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 918.40 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9841592155828739\n",
      "Best CV f1 for Custom_MLP on Scaled_RFE50%_CNN: 0.9842\n",
      "Evaluating Custom_MLP on the test set using Scaled_RFE50%_CNN...\n",
      "Prediction duration: 0.2832 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9617\n",
      "Recall: 0.9312\n",
      "F1 Score: 0.9462\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_RFE50%_CNN):\n",
      "[[ 54   7]\n",
      " [ 13 176]]\n",
      "\n",
      "--- Tuning Custom_MLP on Feature Set: Scaled_RFE75%_CNN (49152 features) ---\n",
      "Starting RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 1026.69 seconds\n",
      "\n",
      "Best parameters:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.6, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9771396625405245\n",
      "Best CV f1 for Custom_MLP on Scaled_RFE75%_CNN: 0.9771\n",
      "Evaluating Custom_MLP on the test set using Scaled_RFE75%_CNN...\n",
      "Prediction duration: 0.3518 seconds\n",
      "Accuracy: 0.9320\n",
      "Precision: 0.9624\n",
      "Recall: 0.9471\n",
      "F1 Score: 0.9547\n",
      "\n",
      "Confusion Matrix (Custom_MLP on Scaled_RFE75%_CNN):\n",
      "[[ 54   7]\n",
      " [ 10 179]]\n",
      "\n",
      "\n",
      "=== Results Summary for All Hybrid Models ===\n",
      "\n",
      "Best CV F1 Scores:\n",
      "-------------------------------------------------\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All_CNN: 0.9517\n",
      "  - Scaled_Corr50%_CNN: 0.9498\n",
      "  - Scaled_Corr75%_CNN: 0.9488\n",
      "  - Scaled_PCA_500_CNN: 0.9470\n",
      "  - Scaled_PCA_95%_CNN: 0.9449\n",
      "  - Scaled_RFE50%_CNN: 0.9487\n",
      "  - Scaled_RFE75%_CNN: 0.9470\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All_CNN: 0.9652\n",
      "  - Scaled_Corr50%_CNN: 0.9712\n",
      "  - Scaled_Corr75%_CNN: 0.9678\n",
      "  - Scaled_PCA_500_CNN: 0.9654\n",
      "  - Scaled_PCA_95%_CNN: 0.9652\n",
      "  - Scaled_RFE50%_CNN: 0.9815\n",
      "  - Scaled_RFE75%_CNN: 0.9720\n",
      "\n",
      "Custom_MLP:\n",
      "  - Scaled_All_CNN: 0.9708\n",
      "  - Scaled_Corr50%_CNN: 0.9745\n",
      "  - Scaled_Corr75%_CNN: 0.9746\n",
      "  - Scaled_PCA_500_CNN: 0.9691\n",
      "  - Scaled_PCA_95%_CNN: 0.9660\n",
      "  - Scaled_RFE50%_CNN: 0.9842\n",
      "  - Scaled_RFE75%_CNN: 0.9771\n",
      "\n",
      "Test Results - F1 Score:\n",
      "----------------------------\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All_CNN: 0.9604\n",
      "  - Scaled_Corr50%_CNN: 0.9424\n",
      "  - Scaled_Corr75%_CNN: 0.9468\n",
      "  - Scaled_PCA_500_CNN: 0.9491\n",
      "  - Scaled_PCA_95%_CNN: 0.9462\n",
      "  - Scaled_RFE50%_CNN: 0.9521\n",
      "  - Scaled_RFE75%_CNN: 0.9524\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All_CNN: 0.9572\n",
      "  - Scaled_Corr50%_CNN: 0.9604\n",
      "  - Scaled_Corr75%_CNN: 0.9572\n",
      "  - Scaled_PCA_500_CNN: 0.9547\n",
      "  - Scaled_PCA_95%_CNN: 0.9542\n",
      "  - Scaled_RFE50%_CNN: 0.9600\n",
      "  - Scaled_RFE75%_CNN: 0.9572\n",
      "\n",
      "Custom_MLP:\n",
      "  - Scaled_All_CNN: 0.9570\n",
      "  - Scaled_Corr50%_CNN: 0.9574\n",
      "  - Scaled_Corr75%_CNN: 0.9549\n",
      "  - Scaled_PCA_500_CNN: 0.9485\n",
      "  - Scaled_PCA_95%_CNN: 0.9485\n",
      "  - Scaled_RFE50%_CNN: 0.9462\n",
      "  - Scaled_RFE75%_CNN: 0.9547\n",
      "\n",
      "=== Best Overall Combination Based on Test F1 ===\n",
      "Best Model: LightGBM\n",
      "Best Feature Set: Scaled_All_CNN (65536 features)\n",
      "Best CV F1 Score: 0.9517\n",
      "Test F1 Score: 0.9604\n",
      "Test Accuracy: 0.9400\n",
      "Test Precision: 0.9579\n",
      "Test Recall: 0.9630\n",
      "Best Parameters: {'subsample': 0.8, 'num_leaves': 60, 'n_estimators': 120, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 53   8]\n",
      " [  7 182]]\n",
      "\n",
      "--- Saving Best Model Per Algorithm (Based on Test F1) ---\n",
      "   Saved global StandardScaler: ..\\models\\hybrid_kaggle_m1_global_scaler.pkl\n",
      "   Saved trained CNN Feature Extractor: ..\\models\\hybrid_kaggle_m1_cnn_feature_extractor.keras\n",
      "\n",
      "Processing LightGBM...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_lightgbm_best_model_Scaled_All_CNN.pkl\n",
      "\n",
      "Processing SVM...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_svm_best_model_Scaled_Corr50%_CNN.pkl\n",
      "   Saved feature selection transformer: ..\\models\\hybrid_kaggle_m1_selector_Scaled_Corr50%_CNN.pkl\n",
      "\n",
      "Processing Custom_MLP...\n",
      "   Saved model: ..\\models\\hybrid_kaggle_m1_custom_mlp_best_model_Scaled_Corr50%_CNN.keras\n",
      "   Saved feature selection transformer: ..\\models\\hybrid_kaggle_m1_selector_Scaled_Corr50%_CNN.pkl\n",
      "\n",
      "--- All Hybrid Model Training and Saving Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern, hog \n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((16, (3, 3)),),\n",
    "    dense_layers=(128,), # CNN'in sonundaki dense katmanları (sınıflandırma öncesi)\n",
    "    dropout_rate=0.3,\n",
    "    activation='leaky_relu',\n",
    "    num_classes=1, # İkili sınıflandırma için 1\n",
    "    output_activation='sigmoid', # İkili sınıflandırma için sigmoid\n",
    "    learning_rate=0.00002,\n",
    "    meta=None \n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout -> Flatten -> Dense -> Output\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:] \n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Sınıflandırma katmanı\n",
    "    model.add(Dense(num_classes, activation=output_activation))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    class_dirs = {'fire_images': 1, 'non_fire_images': 0}\n",
    "\n",
    "    print(f\"Loading and preprocessing images from {data_dir}...\")\n",
    "    for class_name, label in class_dirs.items():\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            print(f\"Warning: Class directory not found: {class_path}. Skipping.\")\n",
    "            continue\n",
    "        for img_name in tqdm(os.listdir(class_path), desc=f\"Processing {class_name}\"):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                continue\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None: continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_resized = cv2.resize(img, target_size)\n",
    "                img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "                all_images.append(img_normalized)\n",
    "                all_labels.append(label)\n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing {img_path}: {e}\") # Çok fazla çıktı verebilir\n",
    "                continue\n",
    "    print(f\"Loaded {len(all_images)} images.\")\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0:\n",
    "        print(\"No features to split.\")\n",
    "        return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Testing features shape: {X_test.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "        print(\"No data for scaling.\")\n",
    "        return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Features scaled.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "        print(\"No data for correlation selection.\")\n",
    "        return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "    \n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for k_features: {k_features}\")\n",
    "            return X_train, X_test, None\n",
    "    elif k_features == 'all':\n",
    "        return X_train, X_test, None # No selection\n",
    "    elif isinstance(k_features, int) and k_features > 0:\n",
    "        k_features_int = min(k_features, n_total_features)\n",
    "    else:\n",
    "        print(f\"Invalid k_features value: {k_features}\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features:\n",
    "        print(f\"KBest: Number of features to select ({k_features_int}) is out of valid range. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "        \n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"Original feature shape: {X_train.shape}\")\n",
    "    print(f\"Selected feature shape (Correlation {k_features}): {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "        print(\"No data for RFE selection.\")\n",
    "        return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for n_features_to_select: {n_features_to_select}\")\n",
    "            return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': # RFE'de 'auto' özel bir durum, burada desteklenmiyor\n",
    "        print(\"RFE: 'auto' n_features_to_select not supported for explicit percentage/count logic.\")\n",
    "        return X_train, X_test, None\n",
    "    else:\n",
    "        print(f\"Invalid n_features_to_select value: {n_features_to_select}\")\n",
    "        return X_train, X_test, None\n",
    "    \n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features:\n",
    "        print(f\"RFE: Number of features to select ({n_features_int}) is out of valid range. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "        print(f\"Original feature shape: {X_train.shape}\")\n",
    "        print(f\"Selected feature shape (RFE {n_features_to_select}): {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE selection: {e}\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0:\n",
    "        print(\"No data for hyperparameter tuning.\")\n",
    "        return None\n",
    "    print(f\"Starting {search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "        # KerasClassifier'da verbose RandomSearchCV tarafından kontrol edildiği için burada ayarlamaya gerek yok.\n",
    "        # callbacks de zaten estimator tanımında verilmiş.\n",
    "        \n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=1, # Keras modeli için n_jobs > 1 sorun çıkarabilir\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else:\n",
    "        print(f\"Search method '{search_method}' not supported.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        search_cv.fit(X_train, y_train, **fit_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RandomizedSearchCV fit: {e}\")\n",
    "        return None\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nBest parameters:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nBest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0:\n",
    "        print(f\"{model_name} evaluation skipped on {feature_set_name}: model not trained or test data is empty.\")\n",
    "        return {}\n",
    "    print(f\"Evaluating {model_name} on the test set using {feature_set_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # KerasClassifier ve TF.keras.Model için özel predict handling\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        # KerasClassifier'ın predict metodu doğrudan tahminleri döndürür\n",
    "        y_pred = model.predict(X_test)\n",
    "        # binary_only=True etiketi nedeniyle çıktı 0 veya 1 olacaktır, ancak yine de emin olmak için kontrol edelim\n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1: # multi-class\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        elif len(y_pred.shape) > 1 and y_pred.shape[1] == 1: # binary, often floats\n",
    "             y_pred = (y_pred > 0.5).astype(int)\n",
    "        else: # 1D array of floats for binary classification\n",
    "            y_pred = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "    elif isinstance(model, tf.keras.Model):\n",
    "        # Doğrudan Keras modeli ise predict_proba gibi davranır\n",
    "        y_pred_proba = model.predict(X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    else: # Scikit-learn modelleri\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction duration: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "        print(\"No data for PCA reduction.\")\n",
    "        return None, None, None\n",
    "    try:        \n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"Original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"Variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA: {e}\")\n",
    "        return X_train, X_test, None # PCA başarısız olursa orijinal veriyi döndür\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "            \n",
    "    if flatten_layer is None:\n",
    "        raise ValueError(\"Flatten layer not found in the CNN model architecture. Cannot create feature extractor.\")\n",
    "    \n",
    "    # Flatten katmanının çıktısını veren bir model oluştur\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "# --- Ana Çalıştırma Kısmı ---\n",
    "data_directory = os.path.join('..', 'data_subsets', 'fire_dataset')\n",
    "target_image_width = 128\n",
    "target_image_height = 128\n",
    "\n",
    "# 1. Görüntüleri Yükle ve Ön İşle\n",
    "X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "if X_images.shape[0] == 0:\n",
    "    print(\"No images loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 2. CNN Modelini Oluştur ve Eğit (Sınıflandırma Modeli Olarak)\n",
    "print(\"\\n--- Initializing and training the CNN classification model ---\")\n",
    "full_cnn_model = create_custom_cnn(\n",
    "    input_shape=X_images.shape[1:],\n",
    "    dense_layers=(128,), # CNN'in sonundaki Dense katmanları (sınıflandırma öncesi)\n",
    "    num_classes=1, # İkili sınıflandırma\n",
    "    output_activation='sigmoid', # İkili sınıflandırma\n",
    ")\n",
    "\n",
    "# CNN Eğitim verisi ve Doğrulama verisini ayır\n",
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(\n",
    "    X_images, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "print(\"Starting CNN training...\")\n",
    "cnn_history = full_cnn_model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    epochs=50, # Yeterli bir epoch sayısı seçin, EarlyStopping ile duracaktır\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_cnn, y_val_cnn),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"CNN training complete.\")\n",
    "\n",
    "# 3. Eğitilmiş CNN Modelinden Özellik Çıkarıcıyı Türet\n",
    "print(\"\\n--- Creating CNN Feature Extractor from the trained CNN model ---\")\n",
    "cnn_feature_extractor = create_cnn_feature_extractor(full_cnn_model)\n",
    "\n",
    "# 4. Eğitilmiş CNN Özellik Çıkarıcı ile Tüm Görüntülerden Özellikleri Elde Et\n",
    "print(\"\\n--- Extracting CNN features for hybrid model training ---\")\n",
    "features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "\n",
    "# 5. Hibrit Modeller için Veriyi Böl\n",
    "# Bu X_train_orig ve X_test_orig, CNN'den gelen özelliklerdir.\n",
    "X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "if X_train_orig is None or X_train_orig.shape[0] == 0:\n",
    "    print(\"No features for hybrid model training. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 6. CNN Özelliklerini Ölçekle\n",
    "print(\"\\n--- Scaling CNN features for hybrid models ---\")\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "feature_sets = {}\n",
    "feature_transformers = {}\n",
    "\n",
    "if X_train_scaled is not None:\n",
    "    feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "    feature_transformers['Scaled_All_CNN'] = scaler\n",
    "else:\n",
    "    print(\"Scaled features are None. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 7. Özellik Seçimi ve Boyut Azaltma (Correlation, RFE, PCA)\n",
    "print(\"\\n--- Performing feature selection & PCA ---\")\n",
    "original_feature_count = X_train_scaled.shape[1]\n",
    "\n",
    "# Korelasyon tabanlı özellik seçimi\n",
    "corr_feature_percentages = ['75%', '50%']\n",
    "for percentage_str in corr_feature_percentages:\n",
    "    print(f\"\\nPerforming correlation selection: {percentage_str}...\")\n",
    "    try:\n",
    "        X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "            X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "        )\n",
    "        if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "            feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "            feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "    except Exception as e:\n",
    "        print(f\"Error during correlation selection for {percentage_str}: {e}\")\n",
    "\n",
    "# RFE özellik seçimi\n",
    "rfe_feature_percentages = ['75%', '50%']\n",
    "rfe_step_val = 0.1\n",
    "rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "for percentage_str in rfe_feature_percentages:\n",
    "    print(f\"\\nPerforming RFE selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "    try:\n",
    "        X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "            X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "        )\n",
    "        if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "            feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "            feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE selection for {percentage_str}: {e}\")\n",
    "\n",
    "# PCA boyut azaltma\n",
    "pca_components = [0.95, 500] # 0.95 varyansın korunması, 500 bileşen\n",
    "for n_comp in pca_components:\n",
    "    print(f\"\\nPerforming PCA with n_components={n_comp}...\")\n",
    "    try:\n",
    "        X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "        if X_train_pca is not None and (isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count or isinstance(n_comp, float)):\n",
    "            fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "            fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "            feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "            feature_transformers[fs_name] = pca_transformer\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA for n_components={n_comp}: {e}\")\n",
    "\n",
    "print(\"\\n--- Available Feature Sets for Tuning ---\")\n",
    "for name, (X_train_fs, _) in feature_sets.items():\n",
    "    print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "\n",
    "# 8. Hibrit Modelleri Eğit ve Hiperparametre Ayarı Yap\n",
    "print(\"\\n--- Model Training and RandomizedSearchCV for Hybrid Models ---\")\n",
    "models_to_tune = {\n",
    "     'LightGBM': {\n",
    "        'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1, n_jobs=4),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [50, 80, 120], \n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [8, 15],\n",
    "            'num_leaves': [20, 40, 60],\n",
    "            'subsample': [0.8, 0.9],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "            'min_split_gain': [0.1],\n",
    "            'min_child_samples': [5]\n",
    "    }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'estimator': SVC(random_state=42, probability=True), # proba True for predict_proba in test\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1, 10, 50],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'Custom_MLP': {\n",
    "        'estimator': SklearnKerasClassifier(\n",
    "            model=create_custom_mlp,\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=0, # KerasClassifier verbose'u RandomSearchCV tarafından kontrol edilir\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "        ),\n",
    "        'param_grid': {\n",
    "            'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "            'model__hidden_layer_2_neurons': [0, 64, 128],\n",
    "            'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "            'model__activation': ['relu', 'leaky_relu'],\n",
    "            'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scoring_metric = 'f1'\n",
    "all_results = {}\n",
    "best_overall_test_score = -np.inf\n",
    "best_overall_combination = None\n",
    "best_overall_trained_model = None\n",
    "best_overall_X_test = None\n",
    "best_overall_transformer = None\n",
    "\n",
    "for model_name, model_config in models_to_tune.items():\n",
    "    all_results[model_name] = {}\n",
    "    estimator = model_config['estimator']\n",
    "    param_distributions = model_config['param_grid']\n",
    "    n_iter_search = model_config.get('n_iter', 8) # Varsayılan iterasyon sayısı\n",
    "    print(f\"\\n\\n=== Training & Tuning {model_name} (Hybrid Model) ===\")\n",
    "    for fs_name in sorted(feature_sets.keys()):\n",
    "        X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "        print(f\"\\n--- Tuning {model_name} on Feature Set: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "        if X_train_fs is None or X_train_fs.shape[0] == 0:\n",
    "            print(f\"Skipping {fs_name} due to empty training data.\")\n",
    "            continue\n",
    "\n",
    "        tuned_search = tune_model_hyperparameters(\n",
    "            estimator,\n",
    "            X_train_fs,\n",
    "            y_train,\n",
    "            param_grid=param_distributions,\n",
    "            cv_strategy=cv_strategy,\n",
    "            scoring=scoring_metric,\n",
    "            search_method='RandomSearch',\n",
    "            n_iter=n_iter_search,\n",
    "            validation_split_keras=0.2\n",
    "        )\n",
    "\n",
    "        if tuned_search:\n",
    "            best_model_for_combination = tuned_search.best_estimator_\n",
    "            best_cv_score = tuned_search.best_score_\n",
    "            best_params = tuned_search.best_params_\n",
    "            print(f\"Best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "            test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "            all_results[model_name][fs_name] = {\n",
    "                'best_cv_score': best_cv_score,\n",
    "                'best_params': best_params,\n",
    "                'test_metrics': test_metrics,\n",
    "                'trained_model': best_model_for_combination,\n",
    "                'transformer': feature_transformers.get(fs_name)\n",
    "            }\n",
    "            if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                best_overall_test_score = test_metrics['f1_score']\n",
    "                best_overall_combination = (model_name, fs_name)\n",
    "                best_overall_trained_model = best_model_for_combination\n",
    "                best_overall_X_test = X_test_fs # Bu bilgi test notebook'unda kullanılmaz\n",
    "                best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "print(\"\\n\\n=== Results Summary for All Hybrid Models ===\")\n",
    "if not all_results:\n",
    "    print(\"No results available for hybrid models.\")\n",
    "else:\n",
    "    print(\"\\nBest CV F1 Scores:\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                result = fs_results[fs_name]\n",
    "                cv_score = result.get('best_cv_score', float('nan'))\n",
    "                print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "        else:\n",
    "            print(\"  No results for this model.\")\n",
    "\n",
    "    print(\"\\nTest Results - F1 Score:\")\n",
    "    print(\"----------------------------\")\n",
    "    best_f1_per_model = {}\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            best_test_f1_for_model = -np.inf\n",
    "            best_fs_name_for_model = None\n",
    "\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                result = fs_results[fs_name]\n",
    "                test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                    best_test_f1_for_model = test_f1\n",
    "                    best_fs_name_for_model = fs_name\n",
    "            if best_fs_name_for_model:\n",
    "                best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "        else:\n",
    "            print(\"  No results for this model.\")\n",
    "\n",
    "    print(\"\\n=== Best Overall Combination Based on Test F1 ===\")\n",
    "    if best_overall_combination:\n",
    "        model_name, fs_name = best_overall_combination\n",
    "        best_result = all_results[model_name][fs_name]\n",
    "        test_metrics = best_result['test_metrics']\n",
    "\n",
    "        print(f\"Best Model: {model_name}\")\n",
    "        actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "        print(f\"Best Feature Set: {fs_name} ({actual_feature_count} features)\")\n",
    "        print(f\"Best CV F1 Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_params']}\\n\")\n",
    "        print(f\"Confusion Matrix:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "    else:\n",
    "        print(\"No best overall combination found.\")\n",
    "\n",
    "# 9. Modelleri ve Dönüştürücüleri Kaydet\n",
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "print(\"\\n--- Saving Best Model Per Algorithm (Based on Test F1) ---\")\n",
    "\n",
    "# Global StandardScaler'ı kaydet\n",
    "if 'scaler' in locals() and scaler is not None:\n",
    "    try:\n",
    "        joblib.dump(scaler, os.path.join(MODEL_SAVE_DIR, 'hybrid_kaggle_m1_global_scaler.pkl'))\n",
    "        print(f\"   Saved global StandardScaler: {os.path.join(MODEL_SAVE_DIR, 'hybrid_kaggle_m1_global_scaler.pkl')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving global StandardScaler: {e}\")\n",
    "else:\n",
    "    print(\"   Global StandardScaler not found or is None, skipping save.\")\n",
    "\n",
    "# Eğitilmiş CNN Özellik Çıkarıcıyı Kaydetme\n",
    "if 'cnn_feature_extractor' in locals() and cnn_feature_extractor is not None:\n",
    "    try:\n",
    "        # Önemli: Bu, eğitilmiş CNN'den türetilen özellik çıkarıcıdır.\n",
    "        cnn_feature_extractor_filename = 'hybrid_kaggle_m1_cnn_feature_extractor.keras' \n",
    "        CNN_FEATURE_EXTRACTOR_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, cnn_feature_extractor_filename)\n",
    "        \n",
    "        cnn_feature_extractor.save(CNN_FEATURE_EXTRACTOR_SAVE_PATH)\n",
    "        print(f\"   Saved trained CNN Feature Extractor: {CNN_FEATURE_EXTRACTOR_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error saving trained CNN Feature Extractor: {e}\")\n",
    "else:\n",
    "    print(\"   Trained CNN Feature Extractor object not found or is None. Skipping save.\")\n",
    "\n",
    "if 'best_f1_per_model' not in locals() or not best_f1_per_model: # Corrected variable name\n",
    "     print(\"Could not determine best feature set per model. Skipping model/transformer saves.\")\n",
    "else:\n",
    "    for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        if model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "            best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "            model_to_save = best_combination_results.get('trained_model')\n",
    "            transformer_to_save = best_combination_results.get('transformer')\n",
    "\n",
    "            if model_to_save:\n",
    "                is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                model_filename = f'hybrid_kaggle_m1_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                try:\n",
    "                    if is_keras_model:\n",
    "                        # Scikeras KerasClassifier objesi içindeki asıl Keras modelini kaydet\n",
    "                        model_to_save.model_.save(MODEL_SAVE_PATH_ALG) \n",
    "                    else:\n",
    "                        joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                    print(f\"   Saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Error saving {model_name} model to {MODEL_SAVE_PATH_ALG}: {e}\")\n",
    "            else:\n",
    "                print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "            \n",
    "            # Transformer'ı kaydet (Scaled_All_CNN için zaten scaler kaydedildi, bu yüzden kontrol ekliyoruz)\n",
    "            if transformer_to_save and best_fs_name_for_model != 'Scaled_All_CNN': \n",
    "                 transformer_filename = f'hybrid_kaggle_m1_selector_{best_fs_name_for_model}.pkl'\n",
    "                 TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                 try:\n",
    "                     joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                     print(f\"   Saved feature selection transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                 except Exception as e:\n",
    "                    print(f\"   Error saving transformer for {best_fs_name_for_model} to {TRANSFORMER_SAVE_PATH}: {e}\")\n",
    "            elif best_fs_name_for_model != 'Scaled_All_CNN': # Bu durumda bir transformer beklendiği halde bulunamadı uyarısı\n",
    "                print(f\"   Warning: Feature selection transformer not found for {best_fs_name_for_model}.\")\n",
    "        else:\n",
    "            print(f\"No valid results found for the best feature set '{best_fs_name_for_model}' for model {model_name}.\")\n",
    "\n",
    "print(\"\\n--- All Hybrid Model Training and Saving Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
