{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pipeline_description",
   "metadata": {},
   "source": [
    "6. Yöntem 1: Görüntü Özniteliklerine Dayali Siniflandirma Pipeline'i\n",
    "\n",
    "Bu notebook, yangin tespiti için görüntülerden çikarilan özniteliklerin kullanildiği ve çeşitli Makine Öğrenmesi modellerinin karşilaştirildiği Yöntem 1'in uygulamasini içermektedir. Pipeline, belirtilen adimlari takip eder:\n",
    "\n",
    "1.  **Gerekli Kütüphanelerin ve Yardimci Fonksiyonlarin İmport Edilmesi:** Veri işleme, öznitelik çikarimi, öznitelik mühendisliği, modelleme ve değerlendirme için gerekli kütüphaneler ve özel olarak tanimlanmiş yardimci fonksiyonlar yüklenir.\n",
    "2.  **Veri Yükleme ve Öznitelik Çikarimi:** Belirtilen veri setinden (D-Fire) görüntüler okunur, yeniden boyutlandirilir ve piksel değerleri normalize edilir. Farkli renk uzaylarina (BGR, HSV, YCbCr) dönüşümler yapilir. Ardindan, renk histogramlari (HSV ve YCbCr kanallarindan planlandi) ile gri tonlamali görüntülerden LBP ve HOG gibi el yapimi öznitelikler çikarilir. Öznitelik çikarimi sirasinda, hazir fonksiyonlardan kaçinilarak algoritmalarin baştan hazirlanmasi gereksinimi, mevcut koddaki skimage ve cv2 kullanimlari nedeniyle tam olarak karşilanmamaktadir; bu kisimlar temel görüntü işleme operasyonlari ile yeniden yazilabilir.\n",
    "3.  **Öznitelik Birleştirme ve Temel Ölçeklendirme:** Çikarilan farkli öznitelik vektörleri tek bir vektörde birleştirilir. Bu birleşik öznitelikler daha sonra model eğitimine uygun hale getirilmek üzere ölçeklendirilir (Standardization).\n",
    "4.  **Verinin Bölünmesi:** Ölçeklendirilmiş öznitelik vektörleri ve etiketler, eğitim ve test setlerine ayrilir. Sinif dengesini korumak için StratifiedKFold stratejisi kullanilir.\n",
    "5.  **Öznitelik Mühendisliği ve Farkli Öznitelik Setlerinin Oluşturulmasi:** Ölçeklendirilmiş eğitim verisi üzerinde öznitelik seçimi yöntemleri uygulanarak farkli öznitelik setleri oluşturulur. Planlanan yöntemler Korelasyon Analizi (ANOVA F-value) ve Recursive Feature Elimination (RFE) olarak belirlenmiştir. Her bir yöntem için kaç özniteliğin korunacaği bir hiperparametre olarak değerlendirilir. PCA, gereksinimlerde bahsedilse de bu implementasyonda yer almamaktadir.\n",
    "    *   Set 1: Sadece Ölçeklendirilmiş (Öznitelik Mühendisliği Yok - Temel Ölçekleme Hariç)\n",
    "    *   Set 2: Ölçeklendirilmiş + Korelasyon Analizi ile Seçilmiş Öznitelikler\n",
    "    *   Set 3: Ölçeklendirilmiş + RFE ile Seçilmiş Öznitelikler\n",
    "    Bu farkli setler, \"Feature Extraction var/yok\" veya daha doğrusu \"Feature Engineering var/yok ve hangi tip var\" senaryolarini temsil eder.\n",
    "6.  **Model Eğitimi ve Hiperparametre Optimizasyonu:** Belirlenen Makine Öğrenmesi modelleri (SVM, LightGBM, MLP), her bir öznitelik seti üzerinde GridSearchCV (veya isteğe bağli olarak RandomizedSearchCV) ve k-fold cross-validation (StratifiedKFold) kullanarak eğitilir ve hiperparametreleri optimize edilir. Her model-öznitelik seti kombinasyonu için en iyi hiperparametreler ve cross-validation performansi kaydedilir.\n",
    "7.  **Performans Karşilaştirmasi:** Tüm model-öznitelik seti kombinasyonlarinin cross-validation performanslari karşilaştirilir. En iyi performans veren kombinasyon belirlenir.\n",
    "8.  **En İyi Modelin Test Seti Üzerinde Değerlendirilmesi:** En iyi kombinasyonla eğitilmiş model, daha önce ayrilan nihai test seti üzerinde değerlendirilir. Standart metrikler (Accuracy, Precision, Recall, F1-Score) ve karişiklik matrisi hesaplanir ve raporlanir.\n",
    "9.  **Model ve Transformerlarin Kaydedilmesi:** Eğitilen en iyi model, öznitelik ölçekleyici ve kullanilan öznitelik seçici (varsa) gelecekte kullanilmak üzere kaydedilir.\n",
    "\n",
    "Hiperparametreler (HP) ve Ayarlanabilir Parametreler:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_and_helpers",
   "metadata": {},
   "source": [
    "1. Gerekli Kütüphaneler ve Yardimci Fonksiyonlar\n",
    "Veri işleme, öznitelik çikarimi, modelleme ve değerlendirme için gerekli tüm kütüphaneler import edilir. `prep_general.py` içeriği fonksiyonlar olarak buraya dahil edilmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    " \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts and len(parts) > 0:\n",
    "                    if parts[0].isdigit():\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id in fire_class_ids: return True\n",
    "    except Exception as e: pass\n",
    "    return False\n",
    "\n",
    "def extract_color_histograms(img_processed, color_space, bins):\n",
    "    histograms = []\n",
    "    ranges = {\n",
    "        'hsv': {'float': ([0, 1], [0, 1], [0, 1]), 'uint8': ([0, 180], [0, 256], [0, 256])},\n",
    "        'ycbcr': {'float': ([0, 1], [-0.5, 0.5], [-0.5, 0.5]), 'uint8': ([0, 256], [0, 256], [0, 256])}\n",
    "    }\n",
    "    channel_indices = {'hsv': [0, 1], 'ycbcr': [1, 2]} \n",
    "    dtype_key = 'float' if img_processed.dtype in [np.float32, np.float64] else 'uint8'\n",
    "\n",
    "    if color_space in ranges and color_space in channel_indices:\n",
    "        for i in channel_indices[color_space]:\n",
    "            current_range = ranges[color_space][dtype_key][i]\n",
    "            if img_processed.dtype != np.float32 and img_processed.dtype != np.uint8:\n",
    "                 img_processed = img_processed.astype(np.float32)\n",
    "                 dtype_key = 'float'\n",
    "                 current_range = ranges[color_space][dtype_key][i]\n",
    "\n",
    "            hist = cv2.calcHist([img_processed], [i], None, [bins], current_range)\n",
    "            histograms.append(hist.flatten())\n",
    "\n",
    "    if histograms: return np.concatenate(histograms)\n",
    "    else: return np.array([])\n",
    "\n",
    "def extract_lbp_features(img_gray, radius, n_points, method):\n",
    "    \n",
    "    if img_gray is None or img_gray.size == 0: return np.array([])\n",
    "    if n_points is None: n_points = 8 * radius\n",
    "     \n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)\n",
    "\n",
    "    try:\n",
    "        lbp_image = local_binary_pattern(img_gray, n_points, radius, method=method)\n",
    "        if method == 'uniform' or method == 'nri_uniform':\n",
    "            n_bins = int(n_points + 2)  \n",
    "            hist_range = (0, n_bins)\n",
    "        elif method == 'ror':\n",
    "            n_bins = int(n_points / radius + 2)  \n",
    "            hist_range = (0, n_bins)\n",
    "        else:  \n",
    "            n_bins = int(2**n_points)\n",
    "            hist_range = (0, n_bins)\n",
    "        lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=n_bins, range=hist_range)\n",
    "        lbp_hist = lbp_hist.astype(np.float32)\n",
    "        if lbp_hist.sum() > 0: lbp_hist /= lbp_hist.sum()  \n",
    "        return lbp_hist.flatten()\n",
    "\n",
    "    except Exception as e:\n",
    "         \n",
    "        return np.array([])  \n",
    "\n",
    "def extract_hog_features(img_gray, orientations, pixels_per_cell, cells_per_block, block_norm):\n",
    "    \n",
    "    if img_gray is None or img_gray.size == 0: return np.array([])\n",
    "\n",
    "     \n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)  \n",
    "\n",
    "     \n",
    "    img_h, img_w = img_gray.shape\n",
    "    cell_h, cell_w = pixels_per_cell\n",
    "    block_h, block_w = cells_per_block\n",
    "\n",
    "     \n",
    "    min_img_h = cell_h * block_h\n",
    "    min_img_w = cell_w * block_w\n",
    "\n",
    "    if img_h < min_img_h or img_w < min_img_w:\n",
    "         \n",
    "        return np.array([])\n",
    "\n",
    "    try:\n",
    "        hog_features = hog(img_gray, orientations=orientations,\n",
    "                           pixels_per_cell=pixels_per_cell,\n",
    "                           cells_per_block=cells_per_block,\n",
    "                           block_norm=block_norm,\n",
    "                           visualize=False, feature_vector=True)\n",
    "        return hog_features.flatten()\n",
    "\n",
    "    except Exception as e:\n",
    "         \n",
    "        return np.array([])  \n",
    "\n",
    "\n",
    "def combine_features(img_dict, feature_params):\n",
    "    \n",
    "    all_features = []\n",
    "\n",
    "     \n",
    "     \n",
    "     \n",
    "    if 'hsv' in img_dict and img_dict['hsv'] is not None:\n",
    "        hsv_hist = extract_color_histograms(img_dict['hsv'], 'hsv', bins=feature_params.get('hist_bins', 100))\n",
    "        if hsv_hist.size > 0: all_features.append(hsv_hist)\n",
    "    if 'ycbcr' in img_dict and img_dict['ycbcr'] is not None:\n",
    "        ycbcr_hist = extract_color_histograms(img_dict['ycbcr'], 'ycbcr', bins=feature_params.get('hist_bins', 100))\n",
    "        if ycbcr_hist.size > 0: all_features.append(ycbcr_hist)\n",
    "    if 'gray' in img_dict and img_dict['gray'] is not None:\n",
    "        img_gray_processed = img_dict['gray']\n",
    "\n",
    "        lbp_features = extract_lbp_features(img_gray_processed,\n",
    "                                            radius=feature_params.get('lbp_radius', 3),\n",
    "                                            n_points=feature_params.get('lbp_n_points', None),\n",
    "                                            method=feature_params.get('lbp_method', 'uniform'))\n",
    "        if lbp_features.size > 0: all_features.append(lbp_features)\n",
    "\n",
    "        hog_features = extract_hog_features(img_gray_processed,\n",
    "                                           orientations=feature_params.get('hog_orientations', 9),\n",
    "                                           pixels_per_cell=feature_params.get('hog_pixels_per_cell', (8, 8)),\n",
    "                                           cells_per_block=feature_params.get('hog_cells_per_block', (2, 2)),\n",
    "                                           block_norm=feature_params.get('hog_block_norm', 'L2-Hys'))\n",
    "        if hog_features.size > 0: all_features.append(hog_features)\n",
    "\n",
    "    if all_features:\n",
    "         \n",
    "        all_features = [f.astype(np.float32) for f in all_features]\n",
    "        combined_vector = np.concatenate(all_features)\n",
    "        return combined_vector\n",
    "    else:\n",
    "        return np.array([])  \n",
    "\n",
    "\n",
    "def load_and_extract_features_memory_safe(config, feature_params):\n",
    "    dataset_choice = config.get('dataset_choice', 'dfire')\n",
    "    data_root = config.get('data_root')\n",
    "    target_size = config.get('target_img_size')\n",
    "    color_spaces_to_load = config.get('color_spaces_to_load', ['bgr', 'hsv', 'ycbcr'])\n",
    "    normalize_pixels = config.get('normalize_pixels', 1)\n",
    "    fire_class_ids = config.get('fire_class_ids', [])\n",
    "\n",
    "    if not data_root or not target_size: return np.array([]), np.array([])\n",
    "\n",
    "    image_label_pairs = []\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    annotation_extension = '.txt'\n",
    "    images_dir = os.path.join(data_root, 'images')\n",
    "    labels_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir) or not os.path.isdir(labels_dir):\n",
    "        print(f\"Error: Images or Labels directory not found in {data_root}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    all_image_files = [f for f in os.listdir(images_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "\n",
    "    if not all_image_files:\n",
    "        print(f\"No image files found in {images_dir}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    print(\"Determining Labels...\")\n",
    "    for filename in tqdm(all_image_files, desc=\"Determining Labels\", leave=False):\n",
    "        image_name_without_ext = os.path.splitext(filename)[0]\n",
    "        annotation_path = os.path.join(labels_dir, image_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        image_label_pairs.append((os.path.join(images_dir, filename), label))\n",
    "    print(\"Label determination complete.\")\n",
    "\n",
    "    if not image_label_pairs:\n",
    "        print(\"No images with labels found.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped_reading = 0\n",
    "    total_images_skipped_feature = 0\n",
    "\n",
    "    print(\"Loading images and extracting features...\")\n",
    "    for image_path, label in tqdm(image_label_pairs, desc=\"Memory-safe Feature Extraction\", leave=False):\n",
    "        img_bgr = cv2.imread(image_path)\n",
    "\n",
    "        if img_bgr is None:\n",
    "            total_images_skipped_reading += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_dict_single = {}  \n",
    "            img_processed_bgr = None\n",
    "            if normalize_pixels:\n",
    "                img_processed_bgr = img_resized.astype(np.float32) / 255.0 \n",
    "                img_resized_uint8 = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0  \n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2HSV).astype(np.float32) / np.array([180, 255, 255], dtype=np.float32)  \n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2YCrCb).astype(np.float32) / 255.0  \n",
    "            else:  \n",
    "                img_processed_bgr = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2GRAY)\n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2HSV)\n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "             \n",
    "            img_dict_single['gray'] = img_gray  \n",
    "            if 'bgr' in color_spaces_to_load:\n",
    "                img_dict_single['bgr'] = img_processed_bgr\n",
    "\n",
    "            features_single = combine_features(img_dict_single, feature_params)\n",
    "\n",
    "            if features_single.size > 0:\n",
    "                all_features_list.append(features_single)\n",
    "                all_labels_list.append(label)\n",
    "                total_images_processed += 1\n",
    "            else:\n",
    "                total_images_skipped_feature += 1  \n",
    "\n",
    "        except Exception as e:  \n",
    "            total_images_skipped_feature += 1  \n",
    "\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n",
    "    print(f\"Total images initially found: {len(image_label_pairs)}\")\n",
    "    print(f\"Images skipped (read error): {total_images_skipped_reading}\")\n",
    "    print(f\"Images skipped (feature error): {total_images_skipped_feature}\")\n",
    "    print(f\"Images successfully processed: {total_images_processed}\")\n",
    "\n",
    "    if not all_features_list:\n",
    "        print(\"No features extracted from any image.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    features_array = np.array(all_features_list, dtype=np.float32)  \n",
    "    labels_array = np.array(all_labels_list, dtype=np.int32)\n",
    "\n",
    "    print(f\"Final features array shape: {features_array.shape}\")\n",
    "    print(f\"Final labels array shape: {labels_array.shape}\")\n",
    "\n",
    "    return features_array, labels_array\n",
    "\n",
    "def get_config(dataset_choice):\n",
    "    config = {}\n",
    "     \n",
    "    if dataset_choice == 'kaggle':\n",
    "        config['dataset_choice'] = 'kaggle'\n",
    "        config['data_root'] = os.path.join('..', 'fire_dataset')  \n",
    "        config['target_img_size'] = (128, 128)  \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']  \n",
    "        config['normalize_pixels'] = 1  \n",
    "        config['fire_class_ids'] = None  \n",
    "    elif dataset_choice == 'dfire':\n",
    "        config['dataset_choice'] = 'dfire'\n",
    "        config['dfire_root'] = os.path.join('..', 'data_subsets', 'D-Fire')  \n",
    "        config['split_name'] = \"train\"  \n",
    "        config['data_root'] = os.path.join(config['dfire_root'], config['split_name'])\n",
    "        config['target_img_size'] = (128, 128)  \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']  \n",
    "        config['normalize_pixels'] = 1  \n",
    "        config['fire_class_ids'] = [0, 1]  \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset choice: {dataset_choice}. Choose 'kaggle' or 'dfire'.\")\n",
    "\n",
    "    print(f\"Using dataset: {config['dataset_choice']}\")\n",
    "    print(f\"Data root: {config.get('data_root')}\")\n",
    "    print(f\"Target image size: {config['target_img_size']}\")\n",
    "    print(f\"Color spaces loaded: {config['color_spaces_to_load']}\")\n",
    "    print(f\"Normalize pixels: {bool(config['normalize_pixels'])}\")\n",
    "    if dataset_choice == 'dfire':\n",
    "         print(f\"D-Fire Split: {config['split_name']}\")\n",
    "         print(f\"D-Fire Fire Class IDs: {config['fire_class_ids']}\")\n",
    "    return config\n",
    "\n",
    "def get_feature_params():\n",
    "    feature_params = {\n",
    "        'hist_bins': 100,  \n",
    "        'lbp_radius': 3,  \n",
    "        'lbp_n_points': None,  \n",
    "        'lbp_method': 'uniform',  \n",
    "        'hog_orientations': 9,  \n",
    "        'hog_pixels_per_cell': (8, 8),  \n",
    "        'hog_cells_per_block': (2, 2),  \n",
    "        'hog_block_norm': 'L2-Hys'  \n",
    "    }\n",
    "    print(\"\\nFeature extraction parameters:\", feature_params)\n",
    "    return feature_params\n",
    "\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    \n",
    "    if features_array.shape[0] == 0:\n",
    "        print(\"Feature array is empty, cannot split.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(f\"\\nSplitting data: training ({1-test_size:.0%}) testing ({test_size:.0%})\")\n",
    "     \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Testing features shape: {X_test.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Testing labels shape: {y_test.shape}\")\n",
    "    \n",
    "    train_labels, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
    "    print(f\"Train label distribution: {dict(zip(train_labels, train_counts))}\")\n",
    "    print(f\"Test label distribution: {dict(zip(test_labels, test_counts))}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"Scaling skipped: train or test data is empty.\")\n",
    "         return None, None, None\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaling complete.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    " \n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    \"\"\"\n",
    "    Selects features based on ANOVA F-value (correlation for binary classes).\n",
    "    Fits SelectKBest on X_train and transforms both X_train and X_test.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Scaled training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        X_test (np.ndarray): Scaled testing features.\n",
    "        k_features (int or 'all' or str ending with '%'): Number or percentage of top features to select.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train_selected, X_test_selected, selector_object)\n",
    "               Returns (X_train, X_test, None) if selection is skipped or 'all' is chosen.\n",
    "    \"\"\"\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"Correlation Selection skipped: train or test data is empty.\")\n",
    "         return None, None, None\n",
    "\n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "\n",
    "     \n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))  \n",
    "            print(f\"Selecting top {k_features_int} features based on {percentage_str} percentage using Correlation...\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for k_features: {k_features}. Skipping selection.\")\n",
    "            return X_train, X_test, None\n",
    "    elif k_features == 'all':\n",
    "         print(\"Selecting all features (no correlation selection)...\")\n",
    "         return X_train, X_test, None  \n",
    "    elif isinstance(k_features, int) and k_features > 0:\n",
    "        k_features_int = min(k_features, n_total_features)  \n",
    "        print(f\"Selecting top {k_features_int} features by correlation...\")\n",
    "    else:\n",
    "        print(f\"Invalid k_features value: {k_features}. Must be int > 0, 'all', or percentage string (e.g., '75%'). Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "     \n",
    "    if k_features_int <= 0 or k_features_int > n_total_features:\n",
    "         print(f\"Calculated number of features to select ({k_features_int}) is invalid. Skipping selection.\")\n",
    "         return X_train, X_test, None\n",
    "    if k_features_int == n_total_features:\n",
    "         print(\"Number of features to select is equal to total features. Skipping selection.\")\n",
    "         return X_train, X_test, None\n",
    "\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)  \n",
    "\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"Original feature shape: {X_train.shape}\")\n",
    "    print(f\"Selected feature shape: {X_train_selected.shape}\")\n",
    "\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"RFE Selection skipped: train or test data is empty.\")\n",
    "         return None, None, None\n",
    "\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "\n",
    "    if estimator is None:\n",
    "        estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "         \n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))  \n",
    "            print(f\"Selecting top {n_features_int} features based on {percentage_str} percentage using RFE...\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for n_features_to_select: {n_features_to_select}. Skipping RFE.\")\n",
    "            return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)  \n",
    "        print(f\"Selecting {n_features_int} features using RFE...\")\n",
    "    elif n_features_to_select == 'auto':\n",
    "        print(\"RFE with 'auto' feature selection requires RFECV, which is not implemented in this helper. Skipping selection.\")\n",
    "        return X_train, X_test, None  \n",
    "    else:\n",
    "        print(f\"Invalid n_features_to_select value: {n_features_to_select}. Skipping RFE.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "     \n",
    "    if n_features_int <= 0 or n_features_int > n_total_features:\n",
    "        print(f\"Calculated number of features for RFE ({n_features_int}) is invalid. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "    if n_features_int == n_total_features:\n",
    "        print(\"Number of features to select is equal to total features. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "    try:         \n",
    "        if n_features_int >= n_total_features:\n",
    "            print(\"Number of features to select is >= total features. Skipping RFE selection.\")\n",
    "            return X_train, X_test, None  \n",
    "\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)  \n",
    "\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "\n",
    "        print(f\"Original feature shape: {X_train.shape}\")\n",
    "        print(f\"Selected feature shape: {X_train_selected.shape}\")\n",
    "\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE fit/transform: {e}\")\n",
    "        return X_train, X_test, None  \n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='GridSearch'):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0:\n",
    "        print(\"Hyperparameter tuning skipped: training data is empty.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nPerforming {search_method} tuning (scoring='{scoring}')...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if search_method == 'GridSearch':\n",
    "        search_cv = GridSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=4,\n",
    "            verbose=1\n",
    "        )\n",
    "    elif search_method == 'RandomSearch':\n",
    "          \n",
    "         n_iter_search = 20  \n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,  \n",
    "            n_iter=n_iter_search,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=4,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else:\n",
    "        print(f\"Unknown search_method: {search_method}. Use 'GridSearch' or 'RandomSearch'.\")\n",
    "        return None\n",
    "\n",
    "    search_cv.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nBest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "\n",
    "    return search_cv\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0:\n",
    "        print(f\"{model_name} evaluation skipped on {feature_set_name}: model not trained or test data is empty.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name} on the test set using {feature_set_name}...\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction duration: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()  \n",
    "    }\n",
    "\n",
    "print(\"Imports and helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_extract",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Konfigürasyon belirlenir, D-Fire veri seti yüklenir ve öznitelikler (Histogramlar, LBP, HOG) çikarilir. Çikarilan öznitelikler birleştirilir ve Standard Scaler ile ölçeklendirilir. Bu, tüm öznitelik mühendisliği adimlarinin başlangiç noktasi olacaktir.\n",
    "\n",
    "**Öznitelik Ayirt Ediciliği Gözlemi (Raporda Detaylandirilacak):**\n",
    "Gereksinimde belirtilen \"hangi kanallarin daha ayirt edici olduğu gözlenir\" maddesi, bu otomatize pipeline içinde doğrudan bir kod çiktisi üretmez. Bu gözlem genellikle her bir renk kanalina veya özniteliğe ait histogramlarin, görsellerin, veya basit korelasyon analizlerinin görsel veya istatistiksel incelenmesi ile yapilir. Örneğin, yangin ve yangin olmayan görüntüler için H, S, Cb, Cr kanallarinin histogramlari çizilerek karşilaştirilabilir. Bu tür analizler raporun 'Veri Keşfi' veya 'Öznitelik Analizi' bölümünde detaylandirilmalidir. Bu notebook'taki kod, belirtilen kanallardan (HSV'den H, S; YCbCr'den Cb, Cr) histogramlari birleştirerek modelin bu bilgiyi kullanmasini sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load_extract_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: dfire\n",
      "Data root: ..\\data_subsets\\D-Fire\\train\n",
      "Target image size: (128, 128)\n",
      "Color spaces loaded: ['bgr', 'hsv', 'ycbcr']\n",
      "Normalize pixels: True\n",
      "D-Fire Split: train\n",
      "D-Fire Fire Class IDs: [0, 1]\n",
      "\n",
      "Feature extraction parameters: {'hist_bins': 100, 'lbp_radius': 3, 'lbp_n_points': None, 'lbp_method': 'uniform', 'hog_orientations': 9, 'hog_pixels_per_cell': (8, 8), 'hog_cells_per_block': (2, 2), 'hog_block_norm': 'L2-Hys'}\n",
      "Determining Labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label determination complete.\n",
      "Loading images and extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete.\n",
      "Total images initially found: 1629\n",
      "Images skipped (read error): 0\n",
      "Images skipped (feature error): 0\n",
      "Images successfully processed: 1629\n",
      "Final features array shape: (1629, 8526)\n",
      "Final labels array shape: (1629,)\n",
      "\n",
      "--- Splitting Data ---\n",
      "\n",
      "Splitting data: training (80%) testing (20%)\n",
      "Training features shape: (1303, 8526)\n",
      "Testing features shape: (326, 8526)\n",
      "Training labels shape: (1303,)\n",
      "Testing labels shape: (326,)\n",
      "Train label distribution: {np.int32(0): np.int64(663), np.int32(1): np.int64(640)}\n",
      "Test label distribution: {np.int32(0): np.int64(166), np.int32(1): np.int64(160)}\n",
      "\n",
      "--- Scaling Features (Initial) ---\n",
      "\n",
      "Scaling features...\n",
      "Scaling complete.\n"
     ]
    }
   ],
   "source": [
    "dataset_choice = 'dfire'  \n",
    "\n",
    "try:\n",
    "    config = get_config(dataset_choice)\n",
    "    feature_params = get_feature_params()\n",
    "    features_array_orig, labels_array = load_and_extract_features_memory_safe(config, feature_params)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Configuration Error: {e}\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File Not Found Error: {e}. Please check data paths in get_config.\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading and feature extraction: {e}\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "\n",
    "if features_array_orig.shape[0] == 0:\n",
    "    print(\"No features loaded. Cannot proceed with splitting or modeling.\")\n",
    "    X_train_orig, X_test_orig, y_train, y_test = None, None, None, None\n",
    "    X_train_scaled, X_test_scaled, scaler = None, None, None\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "else:\n",
    "    print(\"\\n--- Splitting Data ---\")\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, labels_array, test_size=0.2, random_state=42)\n",
    "    print(\"\\n--- Scaling Features (Initial) ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {'Scaled_All': (X_train_scaled, X_test_scaled)}\n",
    "    feature_transformers = {'Scaled_All': scaler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Bu adimda, ölçeklendirilmiş öznitelik vektörlerinden farkli öznitelik seçim yöntemleri (Korelasyon Analizi, RFE) kullanilarak yeni öznitelik setleri oluşturulur. Bu setler daha sonra model tuning aşamasinda karşilaştirilacaktir.\n",
    "\n",
    "*   **Korelasyon Analizi (ANOVA F-value ile):** `SelectKBest` ve `f_classif` kullanarak sinif etiketi ile en yüksek korelasyona sahip öznitelikler seçilir.\n",
    "*   **Recursive Feature Elimination (RFE):** Bir estimator (burada Logistic Regression kullanildi) kullanarak öznitelikler iteratif olarak elenir.\n",
    "\n",
    "Kaç özniteliğin seçileceği, her bir yöntem için bir hiperparametre olarak ele alinir. Burada, farkli yüzdeler (%75, %50) denenmiştir. Bu adim, \"Feature Engineering var/yok\" senaryolarini (hiçbiri - 'Scaled_All', Correlation ile var, RFE ile var) oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feature_engineering_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Feature Engineering (Selection) ---\n",
      "Starting with 8526 features after scaling.\n",
      "\n",
      "Attempting Correlation Selection with 75%...\n",
      "Selecting top 6394 features based on 75% percentage using Correlation...\n",
      "Original feature shape: (1303, 8526)\n",
      "Selected feature shape: (1303, 6394)\n",
      "\n",
      "Attempting Correlation Selection with 50%...\n",
      "Selecting top 4263 features based on 50% percentage using Correlation...\n",
      "Original feature shape: (1303, 8526)\n",
      "Selected feature shape: (1303, 4263)\n",
      "\n",
      "Attempting RFE Selection with 75% (step=0.1)...\n",
      "Selecting top 6394 features based on 75% percentage using RFE...\n",
      "Original feature shape: (1303, 8526)\n",
      "Selected feature shape: (1303, 6394)\n",
      "\n",
      "Attempting RFE Selection with 50% (step=0.1)...\n",
      "Selecting top 4263 features based on 50% percentage using RFE...\n",
      "Original feature shape: (1303, 8526)\n",
      "Selected feature shape: (1303, 4263)\n",
      "\n",
      "--- Available Feature Sets for Tuning ---\n",
      "- Scaled_All: 8526 features\n",
      "- Scaled_Corr75%: 6394 features\n",
      "- Scaled_Corr50%: 4263 features\n",
      "- Scaled_RFE75%: 6394 features\n",
      "- Scaled_RFE50%: 4263 features\n"
     ]
    }
   ],
   "source": [
    "if X_train_scaled is not None and y_train is not None:\n",
    "    print(\"\\n--- Performing Feature Engineering (Selection) ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "    print(f\"Starting with {original_feature_count} features after scaling.\") \n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\nAttempting Correlation Selection with {percentage_str}...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}'] = corr_selector\n",
    "            else:\n",
    "                 print(f\"Correlation Selection with {percentage_str} did not reduce features or failed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Correlation Selection ({percentage_str}): {e}\")\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']     \n",
    "    rfe_step_val = 0.1  \n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "         print(f\"\\nAttempting RFE Selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "         try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}'] = rfe_selector\n",
    "            else:\n",
    "                 print(f\"RFE Selection with {percentage_str} did not reduce features or failed.\")\n",
    "         except Exception as e:\n",
    "            print(f\"Error during RFE Selection ({percentage_str}): {e}\")\n",
    "\n",
    "    print(\"\\n--- Available Feature Sets for Tuning ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "else:\n",
    "    print(\"Skipping feature engineering as scaled data is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training_tuning",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Bu adimda, tanimlanan modeller (SVM, LightGBM, MLP) her bir öznitelik seti üzerinde GridSearchCV ile optimize edilir. Her model ve öznitelik seti kombinasyonu için en iyi hiperparametreler ve cross-validation (Stratified K-Fold) performansi bulunur ve kaydedilir.\n",
    "\n",
    "Bu süreç, gereksinimdeki \"Her model-öznitelik seti kombinasyonu ile performans karşilaştirilacaktir\" maddesini karşilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "model_training_tuning_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training and Hyperparameter Tuning ---\n",
      "\n",
      "\n",
      "=== Training and Tuning SVM ===\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 279.69 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.7852542037010484\n",
      "Best CV f1 for SVM on Scaled_All: 0.7853\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_All...\n",
      "Prediction duration: 2.1388 seconds\n",
      "Accuracy: 0.8313\n",
      "Precision: 0.7933\n",
      "Recall (Sensitivity): 0.8875\n",
      "F1 Score: 0.8378\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_All):\n",
      "[[129  37]\n",
      " [ 18 142]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 222.21 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.7823153151349909\n",
      "Best CV f1 for SVM on Scaled_Corr75%: 0.7823\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_Corr75%...\n",
      "Prediction duration: 1.5286 seconds\n",
      "Accuracy: 0.8282\n",
      "Precision: 0.7989\n",
      "Recall (Sensitivity): 0.8688\n",
      "F1 Score: 0.8323\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr75%):\n",
      "[[131  35]\n",
      " [ 21 139]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 131.58 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.786027957575913\n",
      "Best CV f1 for SVM on Scaled_Corr50%: 0.7860\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_Corr50%...\n",
      "Prediction duration: 1.0706 seconds\n",
      "Accuracy: 0.8252\n",
      "Precision: 0.8012\n",
      "Recall (Sensitivity): 0.8562\n",
      "F1 Score: 0.8278\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr50%):\n",
      "[[132  34]\n",
      " [ 23 137]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 201.30 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.8100075258010172\n",
      "Best CV f1 for SVM on Scaled_RFE75%: 0.8100\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_RFE75%...\n",
      "Prediction duration: 1.5173 seconds\n",
      "Accuracy: 0.8344\n",
      "Precision: 0.8011\n",
      "Recall (Sensitivity): 0.8812\n",
      "F1 Score: 0.8393\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE75%):\n",
      "[[131  35]\n",
      " [ 19 141]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 130.53 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 0.1, 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.8994826819230688\n",
      "Best CV f1 for SVM on Scaled_RFE50%: 0.8995\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_RFE50%...\n",
      "Prediction duration: 0.4495 seconds\n",
      "Accuracy: 0.7270\n",
      "Precision: 0.6940\n",
      "Recall (Sensitivity): 0.7937\n",
      "F1 Score: 0.7405\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE50%):\n",
      "[[110  56]\n",
      " [ 33 127]]\n",
      "\n",
      "\n",
      "=== Training and Tuning LightGBM ===\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2116897\n",
      "[LightGBM] [Info] Number of data points in the train set: 1303, number of used features: 8403\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491174 -> initscore=-0.035307\n",
      "[LightGBM] [Info] Start training from score -0.035307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomSearch duration: 4947.44 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 40, 'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.7643325128380913\n",
      "Best CV f1 for LightGBM on Scaled_All: 0.7643\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_All...\n",
      "Prediction duration: 0.0330 seconds\n",
      "Accuracy: 0.8098\n",
      "Precision: 0.7952\n",
      "Recall (Sensitivity): 0.8250\n",
      "F1 Score: 0.8098\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_All):\n",
      "[[132  34]\n",
      " [ 28 132]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1612509\n",
      "[LightGBM] [Info] Number of data points in the train set: 1303, number of used features: 6381\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491174 -> initscore=-0.035307\n",
      "[LightGBM] [Info] Start training from score -0.035307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomSearch duration: 3478.94 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 250, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.7790598292869156\n",
      "Best CV f1 for LightGBM on Scaled_Corr75%: 0.7791\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_Corr75%...\n",
      "Prediction duration: 0.0393 seconds\n",
      "Accuracy: 0.8190\n",
      "Precision: 0.7988\n",
      "Recall (Sensitivity): 0.8438\n",
      "F1 Score: 0.8207\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr75%):\n",
      "[[132  34]\n",
      " [ 25 135]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1080190\n",
      "[LightGBM] [Info] Number of data points in the train set: 1303, number of used features: 4263\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491174 -> initscore=-0.035307\n",
      "[LightGBM] [Info] Start training from score -0.035307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomSearch duration: 2315.88 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 40, 'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.7744051753148777\n",
      "Best CV f1 for LightGBM on Scaled_Corr50%: 0.7744\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_Corr50%...\n",
      "Prediction duration: 0.0377 seconds\n",
      "Accuracy: 0.8098\n",
      "Precision: 0.7784\n",
      "Recall (Sensitivity): 0.8562\n",
      "F1 Score: 0.8155\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr50%):\n",
      "[[127  39]\n",
      " [ 23 137]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1610066\n",
      "[LightGBM] [Info] Number of data points in the train set: 1303, number of used features: 6382\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491174 -> initscore=-0.035307\n",
      "[LightGBM] [Info] Start training from score -0.035307\n",
      "RandomSearch duration: 3634.51 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 40, 'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.7769756606942483\n",
      "Best CV f1 for LightGBM on Scaled_RFE75%: 0.7770\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_RFE75%...\n",
      "Prediction duration: 0.4425 seconds\n",
      "Accuracy: 0.7914\n",
      "Precision: 0.7674\n",
      "Recall (Sensitivity): 0.8250\n",
      "F1 Score: 0.7952\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE75%):\n",
      "[[126  40]\n",
      " [ 28 132]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[LightGBM] [Info] Number of positive: 640, number of negative: 663\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1072929\n",
      "[LightGBM] [Info] Number of data points in the train set: 1303, number of used features: 4252\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491174 -> initscore=-0.035307\n",
      "[LightGBM] [Info] Start training from score -0.035307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "RandomSearch duration: 2427.80 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 250, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.7887190020337116\n",
      "Best CV f1 for LightGBM on Scaled_RFE50%: 0.7887\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_RFE50%...\n",
      "Prediction duration: 0.0383 seconds\n",
      "Accuracy: 0.7945\n",
      "Precision: 0.7751\n",
      "Recall (Sensitivity): 0.8187\n",
      "F1 Score: 0.7964\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE50%):\n",
      "[[128  38]\n",
      " [ 29 131]]\n",
      "\n",
      "\n",
      "=== Training and Tuning MLP ===\n",
      "\n",
      "--- Tuning MLP on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 275.36 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'solver': 'adam', 'learning_rate_init': 0.01, 'hidden_layer_sizes': (100, 50), 'alpha': 0.001, 'activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.7759770611204508\n",
      "Best CV f1 for MLP on Scaled_All: 0.7760\n",
      "\n",
      "Evaluating MLP on the test set using Scaled_All...\n",
      "Prediction duration: 0.0163 seconds\n",
      "Accuracy: 0.8252\n",
      "Precision: 0.8047\n",
      "Recall (Sensitivity): 0.8500\n",
      "F1 Score: 0.8267\n",
      "\n",
      "Confusion Matrix (MLP on Scaled_All):\n",
      "[[133  33]\n",
      " [ 24 136]]\n",
      "\n",
      "--- Tuning MLP on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 192.77 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'solver': 'adam', 'learning_rate_init': 0.001, 'hidden_layer_sizes': (50, 50), 'alpha': 0.0001, 'activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.7786587188624773\n",
      "Best CV f1 for MLP on Scaled_Corr75%: 0.7787\n",
      "\n",
      "Evaluating MLP on the test set using Scaled_Corr75%...\n",
      "Prediction duration: 0.0091 seconds\n",
      "Accuracy: 0.8067\n",
      "Precision: 0.7771\n",
      "Recall (Sensitivity): 0.8500\n",
      "F1 Score: 0.8119\n",
      "\n",
      "Confusion Matrix (MLP on Scaled_Corr75%):\n",
      "[[127  39]\n",
      " [ 24 136]]\n",
      "\n",
      "--- Tuning MLP on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 113.89 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'solver': 'adam', 'learning_rate_init': 0.01, 'hidden_layer_sizes': (100, 50), 'alpha': 0.0001, 'activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.7794947369487621\n",
      "Best CV f1 for MLP on Scaled_Corr50%: 0.7795\n",
      "\n",
      "Evaluating MLP on the test set using Scaled_Corr50%...\n",
      "Prediction duration: 0.0054 seconds\n",
      "Accuracy: 0.8037\n",
      "Precision: 0.8038\n",
      "Recall (Sensitivity): 0.7937\n",
      "F1 Score: 0.7987\n",
      "\n",
      "Confusion Matrix (MLP on Scaled_Corr50%):\n",
      "[[135  31]\n",
      " [ 33 127]]\n",
      "\n",
      "--- Tuning MLP on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 181.65 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'solver': 'adam', 'learning_rate_init': 0.001, 'hidden_layer_sizes': (100, 50), 'alpha': 0.0001, 'activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.8000284801115681\n",
      "Best CV f1 for MLP on Scaled_RFE75%: 0.8000\n",
      "\n",
      "Evaluating MLP on the test set using Scaled_RFE75%...\n",
      "Prediction duration: 0.0010 seconds\n",
      "Accuracy: 0.8221\n",
      "Precision: 0.7898\n",
      "Recall (Sensitivity): 0.8688\n",
      "F1 Score: 0.8274\n",
      "\n",
      "Confusion Matrix (MLP on Scaled_RFE75%):\n",
      "[[129  37]\n",
      " [ 21 139]]\n",
      "\n",
      "--- Tuning MLP on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomSearch duration: 95.20 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'solver': 'adam', 'learning_rate_init': 0.001, 'hidden_layer_sizes': (50, 50), 'alpha': 0.001, 'activation': 'tanh'}\n",
      "\n",
      "Best CV score:\n",
      "0.8747897997451706\n",
      "Best CV f1 for MLP on Scaled_RFE50%: 0.8748\n",
      "\n",
      "Evaluating MLP on the test set using Scaled_RFE50%...\n",
      "Prediction duration: 0.0018 seconds\n",
      "Accuracy: 0.7393\n",
      "Precision: 0.7095\n",
      "Recall (Sensitivity): 0.7937\n",
      "F1 Score: 0.7493\n",
      "\n",
      "Confusion Matrix (MLP on Scaled_RFE50%):\n",
      "[[114  52]\n",
      " [ 33 127]]\n",
      "\n",
      "--- Model Training and Hyperparameter Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "if not feature_sets or y_train is None:\n",
    "    print(\"Skipping model training and tuning: No feature sets available or labels are missing.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Model Training and Hyperparameter Tuning ---\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': SVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss'),  \n",
    "            'param_grid': {\n",
    "                'n_estimators': [150, 250],      \n",
    "                'learning_rate': [0.05, 0.1],    \n",
    "                'max_depth': [-1, 15],           \n",
    "                'num_leaves': [40, 63],          \n",
    "                'subsample': [0.9],              \n",
    "                'colsample_bytree': [0.9],       \n",
    "            }\n",
    "        },\n",
    "        'MLP': {  \n",
    "             'estimator': MLPClassifier(random_state=42, max_iter=500),  \n",
    "             'param_grid': {\n",
    "                'hidden_layer_sizes': [(100,), (50, 50), (100, 50)],  \n",
    "                'activation': ['relu', 'tanh'],  \n",
    "                'solver': ['adam'],  \n",
    "                'alpha': [0.0001, 0.001],  \n",
    "                'learning_rate_init': [0.001, 0.01],  \n",
    "             }\n",
    "        }\n",
    "    }\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'   \n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf  \n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None  \n",
    "    best_overall_transformer = None  \n",
    " \n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_grid = model_config['param_grid']\n",
    "\n",
    "        print(f\"\\n\\n=== Training and Tuning {model_name} ===\")\n",
    "        for fs_name, (X_train_fs, X_test_fs) in feature_sets.items():\n",
    "            print(f\"\\n--- Tuning {model_name} on Feature Set: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0:\n",
    "                print(f\"Skipping tuning for {model_name} on {fs_name}: Training data is empty.\")\n",
    "                continue\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_grid,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch'  \n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"Best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)  \n",
    "                }\n",
    "\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs  \n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)  \n",
    "    print(\"\\n--- Model Training and Hyperparameter Tuning Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_comparison",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Eğitim ve tuning sonuçlari özetlenir. Farkli model-öznitelik seti kombinasyonlarinin hem Cross-Validation performanslari hem de nihai test seti performanslari karşilaştirilir. Gereksinimde belirtilen standart metrikler (Accuracy, Precision, Recall, F1-Score) ve confusion matrix dikkate alinir.\n",
    "\n",
    "Bu adim, \"Her model-öznitelik seti kombinasyonu ile performans karşilaştirilacaktir ve raporlanacaktir\" gereksinimini karşilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_comparison_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Results Summary Across Models and Feature Sets ===\n",
      "\n",
      "Cross-Validation Results (Best CV F1 Score):\n",
      "-------------------------------------------------\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All: 0.7853\n",
      "  - Scaled_Corr75%: 0.7823\n",
      "  - Scaled_Corr50%: 0.7860\n",
      "  - Scaled_RFE75%: 0.8100\n",
      "  - Scaled_RFE50%: 0.8995\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All: 0.7643\n",
      "  - Scaled_Corr75%: 0.7791\n",
      "  - Scaled_Corr50%: 0.7744\n",
      "  - Scaled_RFE75%: 0.7770\n",
      "  - Scaled_RFE50%: 0.7887\n",
      "\n",
      "MLP:\n",
      "  - Scaled_All: 0.7760\n",
      "  - Scaled_Corr75%: 0.7787\n",
      "  - Scaled_Corr50%: 0.7795\n",
      "  - Scaled_RFE75%: 0.8000\n",
      "  - Scaled_RFE50%: 0.8748\n",
      "\n",
      "Test Set Results (F1 Score):\n",
      "----------------------------\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All: 0.8378\n",
      "  - Scaled_Corr75%: 0.8323\n",
      "  - Scaled_Corr50%: 0.8278\n",
      "  - Scaled_RFE75%: 0.8393\n",
      "  - Scaled_RFE50%: 0.7405\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All: 0.8098\n",
      "  - Scaled_Corr75%: 0.8207\n",
      "  - Scaled_Corr50%: 0.8155\n",
      "  - Scaled_RFE75%: 0.7952\n",
      "  - Scaled_RFE50%: 0.7964\n",
      "\n",
      "MLP:\n",
      "  - Scaled_All: 0.8267\n",
      "  - Scaled_Corr75%: 0.8119\n",
      "  - Scaled_Corr50%: 0.7987\n",
      "  - Scaled_RFE75%: 0.8274\n",
      "  - Scaled_RFE50%: 0.7493\n",
      "\n",
      "=== Overall Best Combination on Test Set (Based on F1 Score) ===\n",
      "Best Model: SVM\n",
      "Best Feature Set: Scaled_RFE75% (6394 features)\n",
      "Best CV F1 Score: 0.8100\n",
      "Test F1 Score: 0.8393\n",
      "Test Accuracy: 0.8344\n",
      "Test Precision: 0.8011\n",
      "Test Recall: 0.8812\n",
      "Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "Confusion Matrix:\n",
      "[[131  35]\n",
      " [ 19 141]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n=== Results Summary Across Models and Feature Sets ===\")\n",
    "\n",
    "if 'all_results' not in locals() or not all_results:\n",
    "    print(\"No results available to summarize.\")\n",
    "else:\n",
    "    print(\"\\nCross-Validation Results (Best CV F1 Score):\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            for fs_name, result in fs_results.items():\n",
    "                 cv_score = result.get('best_cv_score', float('nan'))\n",
    "                 print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "        else:\n",
    "            print(\"  No results for this model.\")\n",
    "     \n",
    "    print(\"\\nTest Set Results (F1 Score):\")\n",
    "    print(\"----------------------------\")\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            for fs_name, result in fs_results.items():\n",
    "                 test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                 print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "        else:\n",
    "            print(\"  No test results for this model.\")\n",
    "            \n",
    "    print(\"\\n=== Overall Best Combination on Test Set (Based on F1 Score) ===\")\n",
    "    if best_overall_combination:\n",
    "        model_name, fs_name = best_overall_combination\n",
    "        best_result = all_results[model_name][fs_name]\n",
    "        test_metrics = best_result['test_metrics']\n",
    "        print(f\"Best Model: {model_name}\")\n",
    "        print(f\"Best Feature Set: {fs_name} ({feature_sets[fs_name][0].shape[1]} features)\")\n",
    "        print(f\"Best CV F1 Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_params']}\")\n",
    "        print(f\"Confusion Matrix:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "    else:\n",
    "        print(\"No successful model tuning and evaluation completed to determine the best combination.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Eğitilen en iyi model (nihai test setindeki en yüksek F1 skoruna göre belirlenen), öznitelik ölçekleyici (`StandardScaler`) ve öznitelik seçici (`SelectKBest` veya `RFE`) nesnesi (eğer en iyi model için kullanildiysa) daha sonra tahmin yapmak için kullanilmak üzere `joblib` ile disk üzerine kaydedilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save_model_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Best Model Per Algorithm ---\n",
      "\n",
      "Finding best feature set for SVM...\n",
      "Best feature set for SVM is 'Scaled_RFE75%' with test F1: 0.8393\n",
      "   Saved model: ..\\models\\svm_best_model_Scaled_RFE75%.pkl\n",
      "   Saved initial scaler: ..\\models\\scaler_initial.pkl\n",
      "   Saved feature selection transformer: ..\\models\\selector_Scaled_RFE75%.pkl\n",
      "\n",
      "Finding best feature set for LightGBM...\n",
      "Best feature set for LightGBM is 'Scaled_Corr75%' with test F1: 0.8207\n",
      "   Saved model: ..\\models\\lightgbm_best_model_Scaled_Corr75%.pkl\n",
      "   Saved feature selection transformer: ..\\models\\selector_Scaled_Corr75%.pkl\n",
      "\n",
      "Finding best feature set for MLP...\n",
      "Best feature set for MLP is 'Scaled_RFE75%' with test F1: 0.8274\n",
      "   Saved model: ..\\models\\mlp_best_model_Scaled_RFE75%.pkl\n",
      "   Saved feature selection transformer: ..\\models\\selector_Scaled_RFE75%.pkl\n",
      "\n",
      "--- Saving Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n--- Saving Best Model Per Algorithm ---\")\n",
    "\n",
    "if not all_results:\n",
    "    print(\"No results found from model training and tuning. Nothing to save.\")\n",
    "else:\n",
    "    for model_name, results_for_model in all_results.items():\n",
    "        print(f\"\\nFinding best feature set for {model_name}...\")\n",
    "\n",
    "        best_fs_name_for_model = None\n",
    "        best_fs_score_for_model = -np.inf\n",
    "        best_combination_results = None # Store the results dict for the winning combination\n",
    "\n",
    "        # Find the feature set that gave the best test score (F1) for this model\n",
    "        if results_for_model: # Check if any feature sets were processed for this model\n",
    "            for fs_name, combination_results in results_for_model.items():\n",
    "                # Check if combination results are valid and have test metrics with f1_score\n",
    "                if combination_results and 'test_metrics' in combination_results and 'f1_score' in combination_results['test_metrics']:\n",
    "                    current_test_score = combination_results['test_metrics']['f1_score']\n",
    "                    if current_test_score > best_fs_score_for_model:\n",
    "                        best_fs_score_for_model = current_test_score\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                        best_combination_results = combination_results\n",
    "\n",
    "        if best_fs_name_for_model and best_combination_results:\n",
    "            print(f\"Best feature set for {model_name} is '{best_fs_name_for_model}' with test F1: {best_fs_score_for_model:.4f}\")\n",
    "            model_to_save = best_combination_results['trained_model']\n",
    "            transformer_to_save = best_combination_results.get('transformer') \n",
    "            model_filename = f'{model_name.lower()}_best_model_{best_fs_name_for_model}.pkl'\n",
    "            scaler_filename = f'scaler_initial.pkl'\n",
    "\n",
    "            MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "            SCALER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, scaler_filename)\n",
    "            TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, f'selector_{best_fs_name_for_model}.pkl') if best_fs_name_for_model != 'Scaled_All' else None\n",
    "            try:\n",
    "                joblib.dump(model_to_save, MODEL_SAVE_PATH)\n",
    "                print(f\"   Saved model: {MODEL_SAVE_PATH}\")\n",
    "                initial_scaler_for_saving = feature_transformers.get('Scaled_All')\n",
    "                if initial_scaler_for_saving:\n",
    "                    if not os.path.exists(SCALER_SAVE_PATH):\n",
    "                       joblib.dump(initial_scaler_for_saving, SCALER_SAVE_PATH)\n",
    "                       print(f\"   Saved initial scaler: {SCALER_SAVE_PATH}\")\n",
    "                    else:\n",
    "                       pass\n",
    "                else:\n",
    "                    print(\"   Initial scaler not found in feature_transformers['Scaled_All']. Cannot save scaler.\")\n",
    "\n",
    "                if transformer_to_save and TRANSFORMER_SAVE_PATH:\n",
    "                    joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                    print(f\"   Saved feature selection transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                elif best_fs_name_for_model != 'Scaled_All':\n",
    "                    print(f\"   Warning: Feature selection transformer not found for {best_fs_name_for_model}. Cannot save it.\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   Error saving files for {model_name} on {best_fs_name_for_model}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No successful training/tuning results found for {model_name} across any feature set.\")\n",
    "\n",
    "print(\"\\n--- Saving Process Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
