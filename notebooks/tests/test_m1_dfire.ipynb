{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85772f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense,\n",
    "    LeakyReLU, ReLU\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DFIRE_CONFIG = {\n",
    "    'fire_class_ids': [0, 1],\n",
    "    'target_size': (128, 128),\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'img_extensions': ('.png', '.jpg', '.jpeg', '.bmp', '.gif'),\n",
    "    'annotation_extension': '.txt',\n",
    "    'model_dir': os.path.join('..', '..', 'models')\n",
    "}\n",
    "\n",
    "MODEL_DIR = DFIRE_CONFIG['model_dir']\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "DFIRE_ROOT = os.path.join('..', '..', 'data_subsets', 'D-Fire')\n",
    "DFIRE_TEST_ROOT = os.path.join(DFIRE_ROOT, 'test')\n",
    "DFIRE_TEST_IMAGES_DIR = os.path.join(DFIRE_TEST_ROOT, 'images')\n",
    "DFIRE_TEST_LABELS_DIR = os.path.join(DFIRE_TEST_ROOT, 'labels')\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path):\n",
    "        return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts and len(parts) > 0:\n",
    "                    if parts[0].isdigit():\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id in fire_class_ids:\n",
    "                            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file {annotation_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, config):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    target_size = config['target_size']\n",
    "    img_extensions = config['img_extensions']\n",
    "    annotation_extension = config['annotation_extension']\n",
    "    fire_class_ids = config['fire_class_ids']\n",
    "\n",
    "    if not os.path.isdir(images_dir):\n",
    "        print(f\"Warning: Images directory not found at {images_dir}. Skipping.\")\n",
    "        return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Warning: Labels directory not found at {labels_dir}. Skipping.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files:\n",
    "        print(f\"Warning: No image files found in {images_dir}. Skipping.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=f\"Loading & Preprocessing {os.path.basename(data_dir)}\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            continue\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def load_all_artifacts(dataset_choice, config):\n",
    "    print(f\"\\n--- Loading saved CNN model for {dataset_choice} dataset ---\")\n",
    "    artifacts = {}\n",
    "    model_path = os.path.join(config['model_dir'], 'dfire_cnn_best_model.keras')\n",
    "    try:\n",
    "        artifacts['cnn_model'] = tf.keras.models.load_model(model_path)\n",
    "        print(f\"Loaded CNN model: {os.path.basename(model_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CNN model from {model_path}: {e}\")\n",
    "        return None\n",
    "    return artifacts\n",
    "\n",
    "def fetch_original_test_split_for_cnn(data_root, config):\n",
    "    print(f\"\\n--- Fetching original test split from '{data_root}' ---\")\n",
    "    all_images, all_labels = load_prep_4_cnn(data_root, config)\n",
    "\n",
    "    if all_images.size == 0:\n",
    "        print(\"No images loaded for test split. Check data_root and config.\")\n",
    "        return None, None\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        all_images, all_labels,\n",
    "        test_size=config['test_size'],\n",
    "        random_state=config['random_state'],\n",
    "        stratify=all_labels\n",
    "    )\n",
    "    print(f\"Successfully recreated test split with {X_test.shape[0]} samples.\")\n",
    "    return X_test, y_test\n",
    "\n",
    "def preprocess_image_for_cnn(image_path, config):\n",
    "    target_size = config['target_size']\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image {image_path}. Skipping preprocessing.\")\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, target_size)\n",
    "    img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "    return np.expand_dims(img_normalized, axis=0)\n",
    "\n",
    "def evaluate_folder(images_folder_path, labels_folder_path, artifacts, config):\n",
    "    print(f\"\\n--- Processing images in folder: {images_folder_path} using CNN ---\")\n",
    "    cnn_model = artifacts.get('cnn_model')\n",
    "    if cnn_model is None:\n",
    "        print(\"CNN model not loaded. Cannot evaluate folder.\")\n",
    "        return\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_predictions = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if not os.path.isdir(images_folder_path):\n",
    "        print(f\"Error: Images folder path does not exist: {images_folder_path}\")\n",
    "        return\n",
    "    if not os.path.isdir(labels_folder_path):\n",
    "        print(f\"Error: Labels folder path does not exist: {labels_folder_path}\")\n",
    "        return\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_folder_path) if f.lower().endswith(config['img_extensions'])]\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in folder: {images_folder_path}\")\n",
    "        return\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"Processing folder images\"):\n",
    "        image_path = os.path.join(images_folder_path, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_folder_path, img_name_without_ext + config['annotation_extension'])\n",
    "        true_label = 1 if is_dfire_image_fire(annotation_path, config['fire_class_ids']) else 0\n",
    "        img_preprocessed = preprocess_image_for_cnn(image_path, config)\n",
    "        if img_preprocessed is None: continue\n",
    "        try:\n",
    "            prediction_proba = cnn_model.predict(img_preprocessed, verbose=0)\n",
    "            prediction = (prediction_proba > 0.5).astype(int)[0][0]\n",
    "\n",
    "            all_true_labels.append(true_label)\n",
    "            all_predictions.append(prediction)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessed {processed_count} images from the folder.\")\n",
    "\n",
    "    if not all_true_labels:\n",
    "        print(\"No successful predictions for folder. No metrics to display.\")\n",
    "        return\n",
    "\n",
    "    y_true = np.array(all_true_labels)\n",
    "    y_pred = np.array(all_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n--- Performance for CNN on Folder Data ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "def process_single_image(image_path, labels_root_dir, artifacts, config):\n",
    "    print(f\"\\n--- Processing single image: {os.path.basename(image_path)} ---\")\n",
    "    cnn_model = artifacts.get('cnn_model')\n",
    "    if cnn_model is None:\n",
    "        print(\"CNN model not loaded. Cannot process single image.\")\n",
    "        return\n",
    "\n",
    "    img_display = cv2.imread(image_path)\n",
    "    if img_display is None:\n",
    "        print(f\"Error: Could not read image for display: {image_path}\")\n",
    "        return\n",
    "\n",
    "    img_name_without_ext = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    annotation_path = os.path.join(labels_root_dir, img_name_without_ext + config['annotation_extension'])\n",
    "    true_label_num = 1 if is_dfire_image_fire(annotation_path, config['fire_class_ids']) else 0\n",
    "    true_label_text = 'Fire' if true_label_num == 1 else 'Non-Fire'\n",
    "\n",
    "    img_preprocessed = preprocess_image_for_cnn(image_path, config)\n",
    "    if img_preprocessed is None: return\n",
    "\n",
    "    try:\n",
    "        prediction_proba = cnn_model.predict(img_preprocessed, verbose=0)\n",
    "        prediction = (prediction_proba > 0.5).astype(int)[0][0]\n",
    "        prediction_text = \"Fire\" if prediction == 1 else \"Non-Fire\"\n",
    "        print(f\"True Label: {true_label_text} ({true_label_num})\")\n",
    "        print(f\"CNN Prediction: {prediction_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction for {image_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Image: {os.path.basename(image_path)}\\nTrue: {true_label_text}\\nCNN Prediction: {prediction_text}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def reproduce_original_test_results(artifacts, dataset_root, config):\n",
    "    print(\"\\n--- Reproducing Original Test Set Results using train_test_split ---\")\n",
    "    cnn_model = artifacts.get('cnn_model')\n",
    "    if cnn_model is None:\n",
    "        print(\"CNN model not loaded. Cannot reproduce test results.\")\n",
    "        return\n",
    "\n",
    "    X_sample_test, y_sample_test = fetch_original_test_split_for_cnn(\n",
    "        os.path.join('..', '..', 'D-Fire', 'train'),\n",
    "        config\n",
    "    )\n",
    "\n",
    "    if X_sample_test is None or X_sample_test.size == 0:\n",
    "        print(\"Original test split could not be recreated or is empty. Cannot evaluate.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Evaluating CNN on recreated original test data ({X_sample_test.shape[0]} samples) ---\")\n",
    "    try:\n",
    "        prediction_proba = cnn_model.predict(X_sample_test, verbose=0)\n",
    "        y_pred = (prediction_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_sample_test, y_pred)\n",
    "        f1 = f1_score(y_sample_test, y_pred, zero_division=0)\n",
    "        precision = precision_score(y_sample_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_sample_test, y_pred, zero_division=0)\n",
    "        conf_matrix = confusion_matrix(y_sample_test, y_pred)\n",
    "\n",
    "        print(f\" - CNN Accuracy: {accuracy:.4f}\")\n",
    "        print(f\" - CNN Precision: {precision:.4f}\")\n",
    "        print(f\" - CNN Recall: {recall:.4f}\")\n",
    "        print(f\" - CNN F1 Score: {f1:.4f}\")\n",
    "        print(f\"Confusion Matrix for recreated test set:\\n{conf_matrix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation of recreated test set: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting D-Fire CNN evaluation script.\")\n",
    "    print(f\"DFIRE_ROOT set to: {DFIRE_ROOT}\")\n",
    "    print(f\"DFIRE_TEST_ROOT set to: {DFIRE_TEST_ROOT}\")\n",
    "    print(f\"MODEL_DIR set to: {MODEL_DIR}\")\n",
    "    artifacts = load_all_artifacts('DFire', DFIRE_CONFIG)\n",
    "\n",
    "    if artifacts and artifacts['cnn_model'] is not None:\n",
    "        print(\"\\n--- CNN Model loaded successfully. Proceeding with evaluations. ---\")\n",
    "        reproduce_original_test_results(artifacts, DFIRE_ROOT, DFIRE_CONFIG)\n",
    "        evaluate_folder(DFIRE_TEST_IMAGES_DIR, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "        print(\"\\n--- Preparing sample images for single image processing ---\")\n",
    "        all_test_images, all_test_labels = load_prep_4_cnn(DFIRE_TEST_ROOT, DFIRE_CONFIG)\n",
    "        if all_test_images.size == 0:\n",
    "            print(\"No test data loaded for single image processing. Skipping.\")\n",
    "        else:\n",
    "            test_image_filenames = [f for f in os.listdir(DFIRE_TEST_IMAGES_DIR) if f.lower().endswith(DFIRE_CONFIG['img_extensions'])]\n",
    "            test_image_filenames.sort()\n",
    "            fire_image_paths_for_display = []\n",
    "            non_fire_image_paths_for_display = []\n",
    "\n",
    "            for idx, label in enumerate(all_test_labels):\n",
    "                if idx < len(test_image_filenames):\n",
    "                    image_name = test_image_filenames[idx]\n",
    "                    image_path = os.path.join(DFIRE_TEST_IMAGES_DIR, image_name)\n",
    "                    if label == 1: fire_image_paths_for_display.append(image_path)\n",
    "                    else: non_fire_image_paths_for_display.append(image_path)\n",
    "                else:\n",
    "                    print(f\"Warning: Mismatch between loaded labels count ({len(all_test_labels)}) and image filenames count ({len(test_image_filenames)}).\")\n",
    "                    break\n",
    "\n",
    "            num_samples_to_show = min(5, len(fire_image_paths_for_display))\n",
    "            if num_samples_to_show > 0:\n",
    "                print(f\"\\n--- Processing {num_samples_to_show} sample fire images ---\")\n",
    "                for i in range(num_samples_to_show):\n",
    "                    sample_fire_image = fire_image_paths_for_display[i]\n",
    "                    process_single_image(sample_fire_image, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "            else:\n",
    "                print(\"\\nNo fire images identified in the test set for single image processing.\")\n",
    "\n",
    "            num_samples_to_show = min(5, len(non_fire_image_paths_for_display)) # Show up to 5 examples\n",
    "            if num_samples_to_show > 0:\n",
    "                print(f\"\\n--- Processing {num_samples_to_show} sample non-fire images ---\")\n",
    "                for i in range(num_samples_to_show):\n",
    "                    sample_non_fire_image = non_fire_image_paths_for_display[i]\n",
    "                    process_single_image(sample_non_fire_image, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "            else:\n",
    "                print(\"\\nNo non-fire images identified in the test set for single image processing.\")\n",
    "\n",
    "    else: pass\n",
    "    print(f\"Starting D-Fire CNN evaluation script.\")\n",
    "    print(f\"DFIRE_ROOT set to: {DFIRE_ROOT}\")\n",
    "    print(f\"DFIRE_TEST_ROOT set to: {DFIRE_TEST_ROOT}\")\n",
    "    print(f\"MODEL_DIR set to: {MODEL_DIR}\")\n",
    "    artifacts = load_all_artifacts('DFire', DFIRE_CONFIG)\n",
    "\n",
    "    if artifacts and artifacts['cnn_model'] is not None:\n",
    "        print(\"\\n--- CNN Model loaded successfully. Proceeding with evaluations. ---\")\n",
    "        reproduce_original_test_results(artifacts, DFIRE_ROOT, DFIRE_CONFIG)\n",
    "        evaluate_folder(DFIRE_TEST_IMAGES_DIR, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "        print(\"\\n--- Preparing sample images for single image processing ---\")\n",
    "        all_test_images, all_test_labels = load_prep_4_cnn(DFIRE_TEST_ROOT, DFIRE_CONFIG)\n",
    "        if all_test_images.size == 0:\n",
    "            print(\"No test data loaded for single image processing. Skipping.\")\n",
    "        else:\n",
    "            test_image_filenames = [f for f in os.listdir(DFIRE_TEST_IMAGES_DIR) if f.lower().endswith(DFIRE_CONFIG['img_extensions'])]\n",
    "            test_image_filenames.sort()\n",
    "            fire_image_paths_for_display = []\n",
    "            non_fire_image_paths_for_display = []\n",
    "\n",
    "            for idx, label in enumerate(all_test_labels):\n",
    "                if idx < len(test_image_filenames):\n",
    "                    image_name = test_image_filenames[idx]\n",
    "                    image_path = os.path.join(DFIRE_TEST_IMAGES_DIR, image_name)\n",
    "                    if label == 1: fire_image_paths_for_display.append(image_path)\n",
    "                    else: non_fire_image_paths_for_display.append(image_path)\n",
    "                else:\n",
    "                    print(f\"Warning: Mismatch between loaded labels count ({len(all_test_labels)}) and image filenames count ({len(test_image_filenames)}).\")\n",
    "                    break\n",
    "\n",
    "            num_samples_to_show = min(5, len(fire_image_paths_for_display))\n",
    "            if num_samples_to_show > 0:\n",
    "                print(f\"\\n--- Processing {num_samples_to_show} sample fire images ---\")\n",
    "                for i in range(num_samples_to_show):\n",
    "                    sample_fire_image = fire_image_paths_for_display[i]\n",
    "                    process_single_image(sample_fire_image, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "            else:\n",
    "                print(\"\\nNo fire images identified in the test set for single image processing.\")\n",
    "\n",
    "            num_samples_to_show = min(5, len(non_fire_image_paths_for_display)) # Show up to 5 examples\n",
    "            if num_samples_to_show > 0:\n",
    "                print(f\"\\n--- Processing {num_samples_to_show} sample non-fire images ---\")\n",
    "                for i in range(num_samples_to_show):\n",
    "                    sample_non_fire_image = non_fire_image_paths_for_display[i]\n",
    "                    process_single_image(sample_non_fire_image, DFIRE_TEST_LABELS_DIR, artifacts, DFIRE_CONFIG)\n",
    "            else:\n",
    "                print(\"\\nNo non-fire images identified in the test set for single image processing.\")\n",
    "\n",
    "    else: pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
