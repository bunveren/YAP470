{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861d420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dfire prep: 100%|██████████| 1629/1629 [00:07<00:00, 225.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- extracting cnn features: ---\n",
      "training features shape: (1221, 65536)\n",
      "testing features shape: (408, 65536)\n",
      "training labels shape: (1221,)\n",
      "testing labels shape: (408,)\n",
      "\n",
      "--- scaling cnn features: ---\n",
      "\n",
      "--- selection & pca: ---\n",
      "\n",
      "corr selection: 75%...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "corr selection: 50%...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\n",
      "rfe selection with 75% (step=0.1)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "rfe selection with 50% (step=0.1)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\\pca with n_components=0.95...\n",
      "original feature shape: (1221, 65536)\n",
      "PCA transformed feature shape: (1221, 536)\n",
      "variance ratio with 536 components: 0.9500\n",
      "\\pca with n_components=500...\n",
      "original feature shape: (1221, 65536)\n",
      "PCA transformed feature shape: (1221, 500)\n",
      "variance ratio with 500 components: 0.9411\n",
      "\n",
      "--- feat sets for tuning: ---\n",
      "- Scaled_All_CNN: 65536 features\n",
      "- Scaled_Corr75%_CNN: 49152 features\n",
      "- Scaled_Corr50%_CNN: 32768 features\n",
      "- Scaled_RFE75%_CNN: 49152 features\n",
      "- Scaled_RFE50%_CNN: 32768 features\n",
      "- Scaled_PCA_95%_CNN: 536 features\n",
      "- Scaled_PCA_500_CNN: 500 features\n",
      "\n",
      "--- model training and randomsearchcv: ---\n",
      "\n",
      "\n",
      "=== train&tune SVM (Hybrid) ===\n",
      "\n",
      "--- tune SVM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 577.01 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7814428070114751\n",
      "best CV f1 for SVM on Scaled_All_CNN: 0.7814\n",
      "\\SVM on the test set using Scaled_All_CNN.\n",
      "duration: 59.4208 seconds\n",
      "accuracy: 0.7917\n",
      "precision: 0.7489\n",
      "recall: 0.8650\n",
      "f1 score: 0.8028\n",
      "\n",
      "confusion matrix (SVM on Scaled_All_CNN):\n",
      "[[150  58]\n",
      " [ 27 173]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 280.90 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7978781518653227\n",
      "best CV f1 for SVM on Scaled_Corr50%_CNN: 0.7979\n",
      "\\SVM on the test set using Scaled_Corr50%_CNN.\n",
      "duration: 36.7173 seconds\n",
      "accuracy: 0.7892\n",
      "precision: 0.7591\n",
      "recall: 0.8350\n",
      "f1 score: 0.7952\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr50%_CNN):\n",
      "[[155  53]\n",
      " [ 33 167]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 414.06 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7886303470837056\n",
      "best CV f1 for SVM on Scaled_Corr75%_CNN: 0.7886\n",
      "\\SVM on the test set using Scaled_Corr75%_CNN.\n",
      "duration: 46.8695 seconds\n",
      "accuracy: 0.7917\n",
      "precision: 0.7511\n",
      "recall: 0.8600\n",
      "f1 score: 0.8019\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr75%_CNN):\n",
      "[[151  57]\n",
      " [ 28 172]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_500_CNN (500 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 2.62 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7504817266125231\n",
      "best CV f1 for SVM on Scaled_PCA_500_CNN: 0.7505\n",
      "\\SVM on the test set using Scaled_PCA_500_CNN.\n",
      "duration: 0.1168 seconds\n",
      "accuracy: 0.7672\n",
      "precision: 0.7612\n",
      "recall: 0.7650\n",
      "f1 score: 0.7631\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_500_CNN):\n",
      "[[160  48]\n",
      " [ 47 153]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_95%_CNN (536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 2.73 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7538325787445678\n",
      "best CV f1 for SVM on Scaled_PCA_95%_CNN: 0.7538\n",
      "\\SVM on the test set using Scaled_PCA_95%_CNN.\n",
      "duration: 0.1207 seconds\n",
      "accuracy: 0.7721\n",
      "precision: 0.7610\n",
      "recall: 0.7800\n",
      "f1 score: 0.7704\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_95%_CNN):\n",
      "[[159  49]\n",
      " [ 44 156]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 270.50 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.8443118234891078\n",
      "best CV f1 for SVM on Scaled_RFE50%_CNN: 0.8443\n",
      "\\SVM on the test set using Scaled_RFE50%_CNN.\n",
      "duration: 22.9621 seconds\n",
      "accuracy: 0.7770\n",
      "precision: 0.7583\n",
      "recall: 0.8000\n",
      "f1 score: 0.7786\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE50%_CNN):\n",
      "[[157  51]\n",
      " [ 40 160]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 454.83 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7977707835408038\n",
      "best CV f1 for SVM on Scaled_RFE75%_CNN: 0.7978\n",
      "\\SVM on the test set using Scaled_RFE75%_CNN.\n",
      "duration: 47.7426 seconds\n",
      "accuracy: 0.7990\n",
      "precision: 0.7565\n",
      "recall: 0.8700\n",
      "f1 score: 0.8093\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE75%_CNN):\n",
      "[[152  56]\n",
      " [ 26 174]]\n",
      "\n",
      "\n",
      "=== train&tune LightGBM (Hybrid) ===\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTerminatedWorkerError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 435\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- tune \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on fs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_fs.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_train_fs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m X_train_fs.shape[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m tuned_search = \u001b[43mtune_model_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_fs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscoring_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRandomSearch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split_keras\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tuned_search:\n\u001b[32m    448\u001b[39m     best_model_for_combination = tuned_search.best_estimator_\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mtune_model_hyperparameters\u001b[39m\u001b[34m(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring, search_method, n_iter, validation_split_keras)\u001b[39m\n\u001b[32m    224\u001b[39m      search_cv = RandomizedSearchCV(\n\u001b[32m    225\u001b[39m         estimator=model_estimator,\n\u001b[32m    226\u001b[39m         param_distributions=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m         random_state=\u001b[32m42\u001b[39m\n\u001b[32m    233\u001b[39m      )\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[43msearch_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m end_time = time.time()\n\u001b[32m    237\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    964\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    965\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    966\u001b[39m     )\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    974\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1913\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    910\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    912\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    913\u001b[39m         )\n\u001b[32m    914\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    936\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    937\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    938\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    939\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     62\u001b[39m config = get_config()\n\u001b[32m     63\u001b[39m iterable_with_config = (\n\u001b[32m     64\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2071\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1681\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1678\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1680\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1684\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1783\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1778\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1784\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1786\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1858\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1854\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1858\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:757\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    751\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    754\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:772\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    771\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mTerminatedWorkerError\u001b[39m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nDetailed tracebacks of the workers should have been printed to stderr in the executor process if faulthandler was not disabled."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern, hog \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None \n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:] \n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                if class_id in fire_class_ids: return True\n",
    "    except (ValueError, IOError): pass\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir): return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir): return np.array([]), np.array([])\n",
    "\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    annotation_extension = '.txt'\n",
    "    fire_class_ids = [0, 1] \n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files: return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"dfire prep\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0 \n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e: pass \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = k_features\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif k_features == 'all': return X_train, X_test, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train, X_test, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train, X_test, None\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"original feature shape: {X_train.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train, X_test, None\n",
    "    else: return X_train, X_test, None\n",
    "    \n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train, X_test, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "    search_cv.fit(X_train, y_train, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:        \n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "            \n",
    "    if flatten_layer is None: raise ValueError()\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = os.path.join('..', 'data_subsets', 'D-Fire', 'train')\n",
    "    target_image_width = 128\n",
    "    target_image_height = 128 #? i think i dont have time for image w/h opt.\n",
    "\n",
    "    X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "    if X_images.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- extracting cnn features: ---\")\n",
    "    cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "    features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- scaling cnn features: ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "    if X_train_scaled is not None:\n",
    "        feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "        feature_transformers['Scaled_All_CNN'] = scaler\n",
    "    else: exit()\n",
    "\n",
    "    print(\"\\n--- selection & pca: ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\ncorr selection: {percentage_str}...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']\n",
    "    rfe_step_val = 0.1\n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "        print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "        try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "    \n",
    "    pca_components = [0.95, 500]\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\pca with n_components={n_comp}...\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            if X_train_pca is not None and (isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count or isinstance(n_comp, float)):\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "        \n",
    "    print(\"\\n--- feat sets for tuning: ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "\n",
    "    print(\"\\n--- model training and randomsearchcv: ---\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': SVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [-1, 10, 20],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],\n",
    "                'min_child_samples': [5]\n",
    "            }\n",
    "        },\n",
    "        'Custom_MLP': {\n",
    "            'estimator': SklearnKerasClassifier( \n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0, \n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128], \n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'\n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 8) \n",
    "        print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\")\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch',\n",
    "                n_iter=n_iter_search,\n",
    "                validation_split_keras=0.2 \n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)\n",
    "                }\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "    print(\"\\n\\n=== results summary for all models ===\")\n",
    "    if not all_results: pass\n",
    "    else:\n",
    "        print(\"\\nbest cv f1 scores:\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    cv_score = result.get('best_cv_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "            else: pass\n",
    "\n",
    "        print(\"\\ntest results - f1:\")\n",
    "        print(\"----------------------------\")\n",
    "        best_f1_per_model = {}\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                best_test_f1_for_model = -np.inf\n",
    "                best_fs_name_for_model = None\n",
    "\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                    if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                        best_test_f1_for_model = test_f1\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                if best_fs_name_for_model:\n",
    "                    best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "            else: continue\n",
    "\n",
    "        print(\"\\n=== best combo based on f1's ===\")\n",
    "        if best_overall_combination:\n",
    "            model_name, fs_name = best_overall_combination\n",
    "            best_result = all_results[model_name][fs_name]\n",
    "            test_metrics = best_result['test_metrics']\n",
    "\n",
    "            print(f\"best model: {model_name}\")\n",
    "            actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "            print(f\"best fs: {fs_name} ({actual_feature_count} features)\")\n",
    "            print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\")\n",
    "            print(f\"test f1: {test_metrics['f1_score']:.4f}\")\n",
    "            print(f\"test acc: {test_metrics['accuracy']:.4f}\")\n",
    "            print(f\"test prec: {test_metrics['precision']:.4f}\")\n",
    "            print(f\"test rec: {test_metrics['recall']:.4f}\")\n",
    "            print(f\"params: {best_result['best_params']}\\n\")\n",
    "            print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "        else: pass\n",
    "    \n",
    "    MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    print(\"\\n--- saving best hybrids ---\")\n",
    "    if 'best_f1_per_model' not in locals() or not best_f1_per_model: pass    \n",
    "    else:\n",
    "        for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "            print(f\"\\nprocessing {model_name}...\")\n",
    "            if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "                best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "                model_to_save = best_combination_results.get('trained_model')\n",
    "                transformer_to_save = best_combination_results.get('transformer')\n",
    "                if model_to_save:\n",
    "                    is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                    file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                    model_filename = f'Dfire_hybrid_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                    MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                    try:\n",
    "                        if is_keras_model: model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                        else: joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                        print(f\"   saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                    except Exception as e: pass\n",
    "                else:\n",
    "                    print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "\n",
    "                if transformer_to_save and 'All_CNN' not in best_fs_name_for_model: \n",
    "                     transformer_filename = f'Dfire_hybrid_transformer_{best_fs_name_for_model}.pkl'\n",
    "                     TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                     try:\n",
    "                         joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                         print(f\"   saved feature s/r transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                     except Exception as e: pass\n",
    "            \n",
    "    print(\"\\n--- all done!! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
