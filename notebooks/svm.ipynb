{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e298e0",
   "metadata": {},
   "source": [
    "DATA PREP \n",
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bffc2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import cv2, os, numpy as np, time, joblib\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold \n",
    "# i asked ai about this and it suggested me this type of kfold. \n",
    "# i will provide some info abt it in the latter markdown\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from utils.prep_general import (\n",
    "    load_and_preprocess_data, extract_features, split_data, scale_features, apply_pca, get_config, get_feature_params\n",
    ")\n",
    "from utils.prep_svm import (\n",
    "    train_svm, tune_svm_hyperparameters, evaluate_model\n",
    ")\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff92f0",
   "metadata": {},
   "source": [
    "-> https://aistudio.google.com/prompts/new_chat \n",
    "    Why StratifiedKFold instead of standard KFold?\n",
    "\n",
    "        StratifiedKFold: This is similar to KFold, but it makes the splits by preserving the percentage of samples for each class. If your overall dataset has 75% class A and 25% class B, StratifiedKFold will ensure that each of the k folds also has approximately 75% class A and 25% class B samples. Each fold is roughly the same size and has a representative class distribution.\n",
    "\n",
    "        Why StratifiedKFold for classification (especially with imbalance)?\n",
    "\n",
    "            Reliable Evaluation: In classification problems, especially with imbalanced datasets (like yours, where Fire=1 is the minority class at ~24% in the Kaggle data), a standard KFold might accidentally create one or more folds that have very few or even zero samples from the minority class. If a fold used for validation has no fire images, the model's performance (like Recall or F1-score) on that fold will be undefined or misleadingly high/low.\n",
    "            Robust Training: Similarly, if a fold used for training has very few fire images, the model might not learn to identify the minority class effectively from that fold.\n",
    "            StratifiedKFold avoids this by ensuring each fold is a miniature version of the overall dataset's class distribution. This means the performance metric calculated on each fold is a more reliable estimate, and the average metric across all folds (which GridSearchCV uses) is a better overall performance estimate for your hyperparameter tuning.\n",
    "\n",
    "    In your project's context: Your Kaggle dataset has class imbalance (755 fire, 244 non-fire). Using StratifiedKFold for the cross-validation within GridSearchCV ensures that when GridSearchCV tests different hyperparameter combinations, it gets a stable and realistic F1-score (or whichever scoring metric you choose) for each combination by evaluating on folds that accurately represent the imbalance you expect the model to face. If you used standard KFold, the F1-scores from different folds could vary wildly just due to random chance in how the minority class samples were distributed across folds.\n",
    "\n",
    "\n",
    "\n",
    "-> https://www.datacamp.com/tutorial/k-fold-cross-validation\n",
    "Stratified Group K-Fold Cross-Validation \tPrimarily classification with grouped data \tCombines stratification and group integrity, ensuring that groups are not split across folds. \tGreat for grouped and imbalanced datasets to maintain both class and group integrity.\n",
    "just before conclusion there is a table for kfold types, i took the info from there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c396c",
   "metadata": {},
   "source": [
    "2. Loading & Extracting Features\n",
    "TODO: pca yi unuttum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3502a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: kaggle\n",
      "Data root: fire_dataset\n",
      "Target image size: (128, 128)\n",
      "Color spaces loaded: ['bgr', 'hsv', 'ycbcr']\n",
      "Normalize pixels: True\n",
      "\n",
      "Feature extraction parameters: {'hist_bins': 100, 'lbp_radius': 3, 'lbp_n_points': None, 'lbp_method': 'uniform', 'hog_orientations': 9, 'hog_pixels_per_cell': (8, 8), 'hog_cells_per_block': (2, 2), 'hog_block_norm': 'L2-Hys'}\n",
      "\n",
      "Kaggle data_root güncellendi: ..\\fire_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fire_images: 100%|██████████| 755/755 [00:07<00:00, 102.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "toplam islenen: 755 atlanan: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing non_fire_images: 100%|██████████| 244/244 [00:04<00:00, 60.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "toplam islenen: 998 atlanan: 1\n",
      "\n",
      "Successfully loaded and preprocessed 998 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0%|          | 0/998 [00:00<?, ?it/s]C:\\Users\\BerenÜnveren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\skimage\\feature\\texture.py:385: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n",
      "Extracting Features: 100%|██████████| 998/998 [00:16<00:00, 59.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total featurei cikarilabilmis resim s.: 998\n",
      "feature arr shape: (998, 8626)\n",
      "label arr shape: (998,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_choice = 'kaggle' \n",
    "try:\n",
    "    config = get_config(dataset_choice)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    config = None\n",
    "feature_params = get_feature_params()\n",
    "\n",
    "if config:\n",
    "    if dataset_choice == 'kaggle':\n",
    "        config['data_root'] = os.path.join('..', 'fire_dataset') # todo change : os.path.join('..', 'data_subsets', 'fire_dataset')\n",
    "    elif dataset_choice == 'dfire' and config['split_name'] == 'train':\n",
    "        config['data_root'] = os.path.join('..', 'D-Fire', 'train') # todo also change (add data_subsets)\n",
    "\n",
    "processed_data_dict = None\n",
    "if config:\n",
    "    processed_data_dict = load_and_preprocess_data(config)\n",
    "    if processed_data_dict is None or processed_data_dict.get('labels', []).shape[0] == 0:\n",
    "        processed_data_dict = None\n",
    "\n",
    "features_array = np.array([])\n",
    "labels_array = np.array([])\n",
    "if processed_data_dict:\n",
    "    features_array, labels_array = extract_features(processed_data_dict, feature_params)\n",
    "    if features_array.shape[0] == 0:\n",
    "        features_array = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0803a",
   "metadata": {},
   "source": [
    "3. Data Split & Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b171622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training split: (80%) testing split: (20%)\n",
      "training features shape: (798, 8626)\n",
      "testing features shape: (200, 8626)\n",
      "training labels shape: (798,)\n",
      "testing labels shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, X_test_orig, y_train, y_test = None, None, None, None\n",
    "if features_array.shape[0] > 0:\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array, labels_array)\n",
    "    \n",
    "X_train_pca, X_test_pca, pca_model = None, None, None\n",
    "if X_train_orig is not None:\n",
    "    n_pca_components = 0.95\n",
    "    #X_train_pca, X_test_pca, pca_model = apply_pca(X_train_orig, X_test_orig, n_components=n_pca_components)\n",
    "\n",
    "X_train_for_scaling = X_train_pca if X_train_pca is not None else X_train_orig\n",
    "X_test_for_scaling = X_test_pca if X_test_pca is not None else X_test_orig\n",
    "\n",
    "X_train_scaled, X_test_scaled, scaler = None, None, None\n",
    "if X_train_for_scaling is not None:\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_for_scaling, X_test_for_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811590fe",
   "metadata": {},
   "source": [
    "4. Model Training & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01c8ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm model training suresi: 3.00 saniye\n",
      "Accuracy: 0.8900\n",
      "Precision: 0.9006\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9295\n",
      "\n",
      "Confusion Matrix (SVM):\n",
      "[[ 33  16]\n",
      " [  6 145]]\n",
      "\n",
      "class report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.67      0.75        49\n",
      "           1       0.90      0.96      0.93       151\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.87      0.82      0.84       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_svm_model = None\n",
    "if X_train_scaled is not None:\n",
    "    initial_svm_model = train_svm(X_train_scaled, y_train, C=1.0, gamma='scale', kernel='rbf')\n",
    "    if initial_svm_model:\n",
    "        evaluate_model(initial_svm_model, X_test_scaled, y_test, model_name=\"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badab83",
   "metadata": {},
   "source": [
    "5. HP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b94a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "GridSearchCV 194.47 saniye surdu\n",
      "\n",
      "en iyi parametreler:\n",
      "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "en iyi cv f1 skoru:\n",
      "0.9448913558489259\n",
      "Accuracy: 0.9100\n",
      "Precision: 0.9346\n",
      "Recall (Sensitivity): 0.9470\n",
      "F1 Score: 0.9408\n",
      "\n",
      "Confusion Matrix (Tuned SVM):\n",
      "[[ 39  10]\n",
      " [  8 143]]\n",
      "\n",
      "class report (Tuned SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81        49\n",
      "           1       0.93      0.95      0.94       151\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.88      0.87      0.88       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_grid_search = None\n",
    "if X_train_scaled is not None:\n",
    "    param_grid_svm = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    tuned_grid_search = tune_svm_hyperparameters(X_train_scaled, y_train, param_grid_svm)\n",
    "    if tuned_grid_search:\n",
    "        best_svm_model_tuned = tuned_grid_search.best_estimator_\n",
    "        evaluate_model(best_svm_model_tuned, X_test_scaled, y_test, model_name=\"Tuned SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff39b79",
   "metadata": {},
   "source": [
    "6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76c288c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "model_filename = 'best_svm_model_pca.pkl' if pca_model is not None else 'best_svm_model.pkl'\n",
    "scaler_filename = 'scaler_pca.pkl' if pca_model is not None else 'scaler.pkl'\n",
    "pca_filename = 'pca_model.pkl' if pca_model is not None else None \n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "SCALER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, scaler_filename)\n",
    "PCA_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, pca_filename) if pca_filename else None\n",
    "\n",
    "if tuned_grid_search is not None and scaler is not None: \n",
    "   best_svm_model_to_save = tuned_grid_search.best_estimator_\n",
    "   try: joblib.dump(best_svm_model_to_save, MODEL_SAVE_PATH)\n",
    "   except Exception as e: pass\n",
    "\n",
    "   try: joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "   except Exception as e: pass\n",
    "\n",
    "   if pca_model is not None and PCA_SAVE_PATH is not None:\n",
    "        try: joblib.dump(pca_model, PCA_SAVE_PATH)\n",
    "        except Exception as e: pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
