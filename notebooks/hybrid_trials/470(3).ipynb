{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fae291-b690-42b8-9396-f51361f4f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 17:49:57.965464: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-24 17:49:58.001164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.executable: /home/bunveren/miniconda3/envs/rapids-24.02/bin/python\n",
      "sys.version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\n",
      "\n",
      "pip list output:\n",
      " Package                      Version\n",
      "---------------------------- --------------\n",
      "absl-py                      2.1.0\n",
      "anyio                        4.9.0\n",
      "argon2-cffi                  25.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    3.0.0\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.5\n",
      "attrs                        25.3.0\n",
      "babel                        2.17.0\n",
      "beautifulsoup4               4.13.4\n",
      "bleach                       6.2.0\n",
      "Bottleneck                   1.4.2\n",
      "Brotli                       1.0.9\n",
      "cached-property              1.5.2\n",
      "certifi                      2025.7.14\n",
      "cffi                         1.17.1\n",
      "charset-normalizer           3.4.2\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.2\n",
      "debugpy                      1.8.15\n",
      "decorator                    5.2.1\n",
      "defusedxml                   0.7.1\n",
      "exceptiongroup               1.3.0\n",
      "executing                    2.2.0\n",
      "fastjsonschema               2.21.1\n",
      "flatbuffers                  24.3.25\n",
      "fqdn                         1.5.1\n",
      "gast                         0.6.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.62.2\n",
      "h11                          0.16.0\n",
      "h2                           4.2.0\n",
      "h5py                         3.14.0\n",
      "hpack                        4.1.0\n",
      "httpcore                     1.0.9\n",
      "httpx                        0.28.1\n",
      "hyperframe                   6.1.0\n",
      "idna                         3.10\n",
      "imageio                      2.37.0\n",
      "importlib_metadata           8.7.0\n",
      "ipykernel                    6.29.5\n",
      "ipython                      8.37.0\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.2\n",
      "Jinja2                       3.1.6\n",
      "joblib                       1.4.2\n",
      "json5                        0.12.0\n",
      "jsonpointer                  3.0.0\n",
      "jsonschema                   4.25.0\n",
      "jsonschema-specifications    2025.4.1\n",
      "jupyter_client               8.6.3\n",
      "jupyter_core                 5.8.1\n",
      "jupyter-events               0.12.0\n",
      "jupyter-lsp                  2.2.6\n",
      "jupyter_server               2.16.0\n",
      "jupyter_server_terminals     0.5.3\n",
      "jupyterlab                   4.4.5\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.27.3\n",
      "keras                        3.10.0\n",
      "lark                         1.2.2\n",
      "lazy_loader                  0.4\n",
      "libclang                     18.1.1\n",
      "lightgbm                     4.6.0\n",
      "Markdown                     3.8\n",
      "markdown-it-py               2.2.0\n",
      "MarkupSafe                   3.0.2\n",
      "matplotlib-inline            0.1.7\n",
      "mdurl                        0.1.0\n",
      "mistune                      3.1.3\n",
      "mkl_fft                      1.3.11\n",
      "mkl_random                   1.2.8\n",
      "mkl-service                  2.4.0\n",
      "ml-dtypes                    0.3.2\n",
      "mpi4py                       4.0.3\n",
      "namex                        0.0.7\n",
      "nbclient                     0.10.2\n",
      "nbconvert                    7.16.6\n",
      "nbformat                     5.10.4\n",
      "nest_asyncio                 1.6.0\n",
      "networkx                     3.4.2\n",
      "notebook                     7.4.4\n",
      "notebook_shim                0.2.4\n",
      "numexpr                      2.11.0\n",
      "numpy                        1.26.4\n",
      "opencv-python                4.12.0.88\n",
      "opt-einsum                   3.3.0\n",
      "optree                       0.14.1\n",
      "overrides                    7.7.0\n",
      "packaging                    25.0\n",
      "pandas                       2.2.2\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.4\n",
      "pexpect                      4.9.0\n",
      "pickleshare                  0.7.5\n",
      "pillow                       11.3.0\n",
      "pip                          25.1.1\n",
      "platformdirs                 4.3.8\n",
      "prometheus_client            0.22.1\n",
      "prompt_toolkit               3.0.51\n",
      "protobuf                     4.25.3\n",
      "psutil                       7.0.0\n",
      "ptyprocess                   0.7.0\n",
      "pure_eval                    0.2.3\n",
      "py4j                         0.10.9.7\n",
      "pyarrow                      13.0.0\n",
      "pycparser                    2.22\n",
      "Pygments                     2.19.2\n",
      "PySocks                      1.7.1\n",
      "pyspark                      3.5.0\n",
      "python-dateutil              2.9.0.post0\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2025.2\n",
      "PyYAML                       6.0.2\n",
      "pyzmq                        27.0.0\n",
      "referencing                  0.36.2\n",
      "requests                     2.32.4\n",
      "rfc3339_validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rfc3987-syntax               1.1.0\n",
      "rich                         13.9.4\n",
      "rpds-py                      0.26.0\n",
      "scikeras                     0.13.0\n",
      "scikit-image                 0.25.2\n",
      "scikit-learn                 1.7.1\n",
      "scipy                        1.15.3\n",
      "Send2Trash                   1.8.3\n",
      "setuptools                   80.9.0\n",
      "six                          1.17.0\n",
      "sniffio                      1.3.1\n",
      "soupsieve                    2.7\n",
      "stack_data                   0.6.3\n",
      "tensorboard                  2.16.2\n",
      "tensorboard_data_server      0.7.0\n",
      "tensorflow                   2.16.1\n",
      "tensorflow_estimator         2.15.0\n",
      "tensorflow-io-gcs-filesystem 0.37.1\n",
      "termcolor                    2.1.0\n",
      "terminado                    0.18.1\n",
      "tf_keras                     2.19.0\n",
      "threadpoolctl                3.6.0\n",
      "tifffile                     2025.5.10\n",
      "tinycss2                     1.4.0\n",
      "tomli                        2.2.1\n",
      "tornado                      6.5.1\n",
      "tqdm                         4.67.1\n",
      "traitlets                    5.14.3\n",
      "types-python-dateutil        2.9.0.20250708\n",
      "typing_extensions            4.14.1\n",
      "typing_utils                 0.1.0\n",
      "tzdata                       2025.2\n",
      "uri-template                 1.3.0\n",
      "urllib3                      1.26.19\n",
      "wcwidth                      0.2.13\n",
      "webcolors                    24.11.1\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.8.0\n",
      "Werkzeug                     3.1.3\n",
      "wheel                        0.45.1\n",
      "wrapt                        1.17.0\n",
      "zipp                         3.23.0\n",
      "\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 17:50:01.039240: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.062385: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.062436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.064689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.064812: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.064848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.179763: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.179876: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.179884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-07-24 17:50:01.179926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-24 17:50:01.179947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "\n",
    "import sys\n",
    "print(\"sys.executable:\", sys.executable)\n",
    "print(\"sys.version:\", sys.version)\n",
    "\n",
    "import subprocess\n",
    "try:\n",
    "    process = subprocess.run([sys.executable, '-m', 'pip', 'list'], capture_output=True, text=True, check=True)\n",
    "    print(\"\\npip list output:\\n\", process.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nError running pip list: {e}\")\n",
    "    print(e.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n'pip' command not found. Ensure pip is installed and in PATH.\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9ffa7-e578-49b0-96f2-94621d071a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dfire prep: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1629/1629 [00:38<00:00, 42.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- extracting cnn features: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753368644.276700    3268 service.cc:145] XLA service 0x79137c015f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753368644.276808    3268 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-07-24 17:50:44.517379: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-24 17:50:44.983106: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1753368647.899959    3268 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape: (1221, 65536)\n",
      "testing features shape: (408, 65536)\n",
      "training labels shape: (1221,)\n",
      "testing labels shape: (408,)\n",
      "\n",
      "--- scaling cnn features: ---\n",
      "\n",
      "--- selection & pca: ---\n",
      "\n",
      "corr selection: 75%...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "corr selection: 50%...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\n",
      "rfe selection with 75% (step=0.1)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "rfe selection with 50% (step=0.1)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\\pca with n_components=0.95...\n",
      "original feature shape: (1221, 65536)\n",
      "PCA transformed feature shape: (1221, 544)\n",
      "variance ratio with 544 components: 0.9502\n",
      "\\pca with n_components=500...\n",
      "original feature shape: (1221, 65536)\n",
      "PCA transformed feature shape: (1221, 500)\n",
      "variance ratio with 500 components: 0.9393\n",
      "\n",
      "--- feat sets for tuning: ---\n",
      "- Scaled_All_CNN: 65536 features\n",
      "- Scaled_Corr75%_CNN: 49152 features\n",
      "- Scaled_Corr50%_CNN: 32768 features\n",
      "- Scaled_RFE75%_CNN: 49152 features\n",
      "- Scaled_RFE50%_CNN: 32768 features\n",
      "- Scaled_PCA_95%_CNN: 544 features\n",
      "- Scaled_PCA_500_CNN: 500 features\n",
      "\n",
      "--- model training and randomsearchcv: ---\n",
      "\n",
      "\n",
      "=== train&tune SVM (Hybrid) ===\n",
      "\n",
      "--- tune SVM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 727.43 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7841485519699197\n",
      "best CV f1 for SVM on Scaled_All_CNN: 0.7841\n",
      "\\SVM on the test set using Scaled_All_CNN.\n",
      "duration: 40.7663 seconds\n",
      "accuracy: 0.7868\n",
      "precision: 0.7958\n",
      "recall: 0.7600\n",
      "f1 score: 0.7775\n",
      "\n",
      "confusion matrix (SVM on Scaled_All_CNN):\n",
      "[[169  39]\n",
      " [ 48 152]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 395.96 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7990487373690951\n",
      "best CV f1 for SVM on Scaled_Corr50%_CNN: 0.7990\n",
      "\\SVM on the test set using Scaled_Corr50%_CNN.\n",
      "duration: 17.3499 seconds\n",
      "accuracy: 0.8039\n",
      "precision: 0.8061\n",
      "recall: 0.7900\n",
      "f1 score: 0.7980\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr50%_CNN):\n",
      "[[170  38]\n",
      " [ 42 158]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_Corr75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 527.47 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7894723417809333\n",
      "best CV f1 for SVM on Scaled_Corr75%_CNN: 0.7895\n",
      "\\SVM on the test set using Scaled_Corr75%_CNN.\n",
      "duration: 25.4148 seconds\n",
      "accuracy: 0.7843\n",
      "precision: 0.8011\n",
      "recall: 0.7450\n",
      "f1 score: 0.7720\n",
      "\n",
      "confusion matrix (SVM on Scaled_Corr75%_CNN):\n",
      "[[171  37]\n",
      " [ 51 149]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_500_CNN (500 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 4.55 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7523179702367329\n",
      "best CV f1 for SVM on Scaled_PCA_500_CNN: 0.7523\n",
      "\\SVM on the test set using Scaled_PCA_500_CNN.\n",
      "duration: 0.0653 seconds\n",
      "accuracy: 0.7672\n",
      "precision: 0.7638\n",
      "recall: 0.7600\n",
      "f1 score: 0.7619\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_500_CNN):\n",
      "[[161  47]\n",
      " [ 48 152]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_PCA_95%_CNN (544 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 3.78 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.7469088645559233\n",
      "best CV f1 for SVM on Scaled_PCA_95%_CNN: 0.7469\n",
      "\\SVM on the test set using Scaled_PCA_95%_CNN.\n",
      "duration: 0.0657 seconds\n",
      "accuracy: 0.7574\n",
      "precision: 0.7563\n",
      "recall: 0.7450\n",
      "f1 score: 0.7506\n",
      "\n",
      "confusion matrix (SVM on Scaled_PCA_95%_CNN):\n",
      "[[160  48]\n",
      " [ 51 149]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE50%_CNN (32768 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 343.16 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.8472324411908496\n",
      "best CV f1 for SVM on Scaled_RFE50%_CNN: 0.8472\n",
      "\\SVM on the test set using Scaled_RFE50%_CNN.\n",
      "duration: 3.7415 seconds\n",
      "accuracy: 0.7745\n",
      "precision: 0.7547\n",
      "recall: 0.8000\n",
      "f1 score: 0.7767\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE50%_CNN):\n",
      "[[156  52]\n",
      " [ 40 160]]\n",
      "\n",
      "--- tune SVM on fs: Scaled_RFE75%_CNN (49152 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "RandomSearch duration: 512.15 seconds\n",
      "\n",
      "best params:\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "best CV score:\n",
      "0.8060976379941897\n",
      "best CV f1 for SVM on Scaled_RFE75%_CNN: 0.8061\n",
      "\\SVM on the test set using Scaled_RFE75%_CNN.\n",
      "duration: 22.4437 seconds\n",
      "accuracy: 0.8015\n",
      "precision: 0.7668\n",
      "recall: 0.8550\n",
      "f1 score: 0.8085\n",
      "\n",
      "confusion matrix (SVM on Scaled_RFE75%_CNN):\n",
      "[[156  52]\n",
      " [ 29 171]]\n",
      "\n",
      "\n",
      "=== train&tune LightGBM (Hybrid) ===\n",
      "\n",
      "--- tune LightGBM on fs: Scaled_All_CNN (65536 features) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from skimage.feature import local_binary_pattern, hog \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None \n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:] \n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                if class_id in fire_class_ids: return True\n",
    "    except (ValueError, IOError): pass\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir): return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir): return np.array([]), np.array([])\n",
    "\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    annotation_extension = '.txt'\n",
    "    fire_class_ids = [0, 1] \n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files: return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"dfire prep\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0 \n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e: pass \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    \n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = k_features\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif k_features == 'all': return X_train, X_test, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train, X_test, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train, X_test, None\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"original feature shape: {X_train.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train, X_test, None\n",
    "    else: return X_train, X_test, None\n",
    "    \n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train, X_test, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "    search_cv.fit(X_train, y_train, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:        \n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e: return X_train, X_test, None\n",
    "\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "            \n",
    "    if flatten_layer is None: raise ValueError()\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    data_directory = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/data_subsets/D-Fire/train/\"\n",
    "    target_image_width = 128\n",
    "    target_image_height = 128 #? i think i dont have time for image w/h opt.\n",
    "\n",
    "    X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "    if X_images.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- extracting cnn features: ---\")\n",
    "    cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "    features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- scaling cnn features: ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "    if X_train_scaled is not None:\n",
    "        feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "        feature_transformers['Scaled_All_CNN'] = scaler\n",
    "    else: exit()\n",
    "\n",
    "    print(\"\\n--- selection & pca: ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\ncorr selection: {percentage_str}...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']\n",
    "    rfe_step_val = 0.1\n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "        print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "        try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "    \n",
    "    pca_components = [0.95, 500]\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\pca with n_components={n_comp}...\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            if X_train_pca is not None and (isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count or isinstance(n_comp, float)):\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else: continue\n",
    "        except Exception as e: pass\n",
    "        \n",
    "    print(\"\\n--- feat sets for tuning: ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "\n",
    "    print(\"\\n--- model training and randomsearchcv: ---\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': SVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1, n_jobs=1, device='gpu'),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [10, 20],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],\n",
    "                'min_child_samples': [5]\n",
    "            }\n",
    "        },\n",
    "        'Custom_MLP': {\n",
    "            'estimator': SklearnKerasClassifier( \n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0, \n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128], \n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'\n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 8) \n",
    "        print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\")\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch',\n",
    "                n_iter=n_iter_search,\n",
    "                validation_split_keras=0.2 \n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)\n",
    "                }\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "    print(\"\\n\\n=== results summary for all models ===\")\n",
    "    if not all_results: pass\n",
    "    else:\n",
    "        print(\"\\nbest cv f1 scores:\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    cv_score = result.get('best_cv_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "            else: pass\n",
    "\n",
    "        print(\"\\ntest results - f1:\")\n",
    "        print(\"----------------------------\")\n",
    "        best_f1_per_model = {}\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                best_test_f1_for_model = -np.inf\n",
    "                best_fs_name_for_model = None\n",
    "\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                    if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                        best_test_f1_for_model = test_f1\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                if best_fs_name_for_model:\n",
    "                    best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "            else: continue\n",
    "\n",
    "        print(\"\\n=== best combo based on f1's ===\")\n",
    "        if best_overall_combination:\n",
    "            model_name, fs_name = best_overall_combination\n",
    "            best_result = all_results[model_name][fs_name]\n",
    "            test_metrics = best_result['test_metrics']\n",
    "\n",
    "            print(f\"best model: {model_name}\")\n",
    "            actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "            print(f\"best fs: {fs_name} ({actual_feature_count} features)\")\n",
    "            print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\")\n",
    "            print(f\"test f1: {test_metrics['f1_score']:.4f}\")\n",
    "            print(f\"test acc: {test_metrics['accuracy']:.4f}\")\n",
    "            print(f\"test prec: {test_metrics['precision']:.4f}\")\n",
    "            print(f\"test rec: {test_metrics['recall']:.4f}\")\n",
    "            print(f\"params: {best_result['best_params']}\\n\")\n",
    "            print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "        else: pass\n",
    "    \n",
    "    MODEL_SAVE_DIR = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/models/\"\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    print(\"\\n--- saving best hybrids ---\")\n",
    "    if 'best_f1_per_model' not in locals() or not best_f1_per_model: pass    \n",
    "    else:\n",
    "        for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "            print(f\"\\nprocessing {model_name}...\")\n",
    "            if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "                best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "                model_to_save = best_combination_results.get('trained_model')\n",
    "                transformer_to_save = best_combination_results.get('transformer')\n",
    "                if model_to_save:\n",
    "                    is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                    file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                    model_filename = f'Dfire_hybrid_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                    MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                    try:\n",
    "                        if is_keras_model: model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                        else: joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                        print(f\"   saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                    except Exception as e: pass\n",
    "                else:\n",
    "                    print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "\n",
    "                if transformer_to_save and 'All_CNN' not in best_fs_name_for_model: \n",
    "                     transformer_filename = f'Dfire_hybrid_transformer_{best_fs_name_for_model}.pkl'\n",
    "                     TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                     try:\n",
    "                         joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                         print(f\"   saved feature s/r transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                     except Exception as e: pass\n",
    "            \n",
    "    print(\"\\n--- all done!! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d5dbf-4ceb-45c2-9fc6-6f3db827751b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAPIDS 24.02)",
   "language": "python",
   "name": "rapids-24.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
