{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d8cad0-98a8-4a25-911a-8ff13a258685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:34:51.311309: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 17:34:53.965084: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cuda module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.driver module instead.\n",
      "dfire prep: 100%|███████████████████████████████████████████████████████████████████| 1629/1629 [02:29<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- extracting cnn features: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:38:36.754808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:36.774056: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:36.774311: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:36.806251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:36.806631: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:36.806851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:38.553233: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:38.564877: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:38.564917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-07-23 17:38:38.565121: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 17:38:38.565166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753281524.324143    1088 service.cc:145] XLA service 0x70c2540054a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753281524.324324    1088 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-07-23 17:38:44.670899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-23 17:38:45.225216: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1753281528.959688    1088 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape: (1221, 65536)\n",
      "testing features shape: (408, 65536)\n",
      "training labels shape: (1221,)\n",
      "testing labels shape: (408,)\n",
      "\n",
      "--- scaling cnn features (GPU accelerated with cuML StandardScaler): ---\n",
      "\n",
      "--- selection & pca: ---\n",
      "\n",
      "corr selection: 75% (CPU-bound SelectKBest)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "corr selection: 50% (CPU-bound SelectKBest)...\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "[I] [17:39:01.188292] Unused keyword parameter: random_state during cuML estimator initialization\n",
      "\n",
      "rfe selection with 75% (step=0.1, GPU-accelerated estimator with cuML LogisticRegression)...\n",
      "RFE error: __sklearn_tags__\n",
      "\n",
      "rfe selection with 50% (step=0.1, GPU-accelerated estimator with cuML LogisticRegression)...\n",
      "RFE error: __sklearn_tags__\n",
      "\n",
      "pca with n_components=0.95 (GPU accelerated with cuML PCA)...\n",
      "PCA error: exception occurred! file=/opt/conda/conda-bld/work/cpp/src/pca/pca.cuh line=102: Parameter n_components: number of components cannot be less than one\n",
      "Obtained 64 stack frames\n",
      "#0 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x84) [0x70c3d02c8734]\n",
      "#1 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/../../../../libcuml++.so(+0x1c950a) [0x70c3d01c950a]\n",
      "#2 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/decomposition/pca.cpython-310-x86_64-linux-gnu.so(+0x38e22) [0x70c2f9304e22]\n",
      "#3 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0xfa9f) [0x70c2f97e8a9f]\n",
      "#4 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0x2e401) [0x70c2f9807401]\n",
      "#5 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#6 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1506d8) [0x5ce0685fb6d8]\n",
      "#7 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#8 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#9 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#10 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#11 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/decomposition/pca.cpython-310-x86_64-linux-gnu.so(+0x35471) [0x70c2f9301471]\n",
      "#12 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0xfa9f) [0x70c2f97e8a9f]\n",
      "#13 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0x2e401) [0x70c2f9807401]\n",
      "#14 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#15 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1506d8) [0x5ce0685fb6d8]\n",
      "#16 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#17 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#18 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#19 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x5ce0685fb582]\n",
      "#20 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x4c12) [0x5ce0685e4142]\n",
      "#21 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#22 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x13ca) [0x5ce0685e08fa]\n",
      "#23 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1d7c60) [0x5ce068682c60]\n",
      "#24 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(PyEval_EvalCode+0x87) [0x5ce068682ba7]\n",
      "#25 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1dedaa) [0x5ce068689daa]\n",
      "#26 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x144bf3) [0x5ce0685efbf3]\n",
      "#27 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x320) [0x5ce0685df850]\n",
      "#28 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#29 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#30 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#31 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#32 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#33 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1f5a37) [0x5ce0686a0a37]\n",
      "#34 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x14f53d) [0x5ce0685fa53d]\n",
      "#35 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x5ce0685dfc5c]\n",
      "#36 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#37 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x320) [0x5ce0685df850]\n",
      "#38 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#39 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x5ce0685dfc5c]\n",
      "#40 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x5ce0685fb582]\n",
      "#41 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(PyObject_Call+0xbc) [0x5ce0685fbf1c]\n",
      "#42 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x5ce0685e22b3]\n",
      "#43 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x5ce0685fb582]\n",
      "#44 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x13ca) [0x5ce0685e08fa]\n",
      "#45 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#46 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#47 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#48 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#49 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#50 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#51 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#52 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#53 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#54 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x5ce0685e10ed]\n",
      "#55 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x5ce06868c384]\n",
      "#56 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x7bf6) [0x70c4eae01bf6]\n",
      "#57 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x143e8a) [0x5ce0685eee8a]\n",
      "#58 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x25f60c) [0x5ce06870a60c]\n",
      "#59 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0xfdd90) [0x5ce0685a8d90]\n",
      "#60 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x13c2a3) [0x5ce0685e72a3]\n",
      "#61 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x5cd5) [0x5ce0685e5205]\n",
      "#62 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x5ce0685efa2c]\n",
      "#63 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x5ce0685dfc5c]\n",
      "\n",
      "\n",
      "pca with n_components=500 (GPU accelerated with cuML PCA)...\n",
      "PCA error: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n",
      "- Scaled_All_CNN: 65536 features (CuPy)\n",
      "- Scaled_Corr75%_CNN: 49152 features (NumPy)\n",
      "- Scaled_Corr50%_CNN: 32768 features (NumPy)\n",
      "- Scaled_PCA_95%_CNN: 65536 features (CuPy)\n",
      "\n",
      "--- model training and randomsearchcv: ---\n",
      "\n",
      "\n",
      "=== train&tune SVM (Hybrid) ===\n",
      "\n",
      "--- tune SVM on fs: Scaled_All_CNN (65536 features, CuPy) ---\n",
      "\\RandomSearch tuning (scoring='f1')... with 8 iterations\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__sklearn_tags__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 476\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- tune \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on fs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_fs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs_data_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train_fs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m X_train_fs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m tuned_search \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_fs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRandomSearch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split_keras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tuned_search:\n\u001b[1;32m    489\u001b[0m     best_model_for_combination \u001b[38;5;241m=\u001b[39m tuned_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "Cell \u001b[0;32mIn[1], line 256\u001b[0m, in \u001b[0;36mtune_model_hyperparameters\u001b[0;34m(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring, search_method, n_iter, validation_split_keras)\u001b[0m\n\u001b[1;32m    244\u001b[0m      search_cv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m    245\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mmodel_estimator,\n\u001b[1;32m    246\u001b[0m         param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    253\u001b[0m      )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[43msearch_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_for_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_for_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:960\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    956\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    958\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n\u001b[0;32m--> 960\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39m\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    961\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m    963\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/sklearn/base.py:1213\u001b[0m, in \u001b[0;36mis_classifier\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1209\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1210\u001b[0m     )\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/sklearn/utils/_tags.py:325\u001b[0m, in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get estimator tags.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m:class:`~sklearn.BaseEstimator` provides the estimator tags machinery.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    The estimator tags.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# TODO(1.8): turn the warning into an error\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc):\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;66;03m# Fall back to the default tags if the estimator does not\u001b[39;00m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;66;03m# implement __sklearn_tags__.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;66;03m# method in the base class. Typically happens when only inheriting\u001b[39;00m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;66;03m# from Mixins.\u001b[39;00m\n",
      "File \u001b[0;32mbase.pyx:330\u001b[0m, in \u001b[0;36mcuml.internals.base.Base.__getattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __sklearn_tags__"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "import lightgbm as lgb\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml.preprocessing import StandardScaler as CUMLStandardScaler\n",
    "from cuml.decomposition import PCA as CUMLPCA \n",
    "from cuml.svm import SVC as CUMLSVC\n",
    "from cuml.linear_model import LogisticRegression as CUML_LogisticRegression # GPU-accelerated Logistic Regression for RFE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None\n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:]\n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                if class_id in fire_class_ids: return True\n",
    "    except (ValueError, IOError): pass\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir): return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir): return np.array([]), np.array([])\n",
    "\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    annotation_extension = '.txt'\n",
    "    fire_class_ids = [0, 1]\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files: return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"dfire prep\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e: pass\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    X_train_cp = cp.asarray(X_train)\n",
    "    X_test_cp = cp.asarray(X_test)\n",
    "    scaler = CUMLStandardScaler()\n",
    "    X_train_scaled_cp = scaler.fit_transform(X_train_cp)\n",
    "    X_test_scaled_cp = scaler.transform(X_test_cp)\n",
    "    return X_train_scaled_cp, X_test_scaled_cp, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    X_train_np = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "    X_test_np = X_test.get() if isinstance(X_test, cp.ndarray) else X_test\n",
    "    y_train_np = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "    n_total_features = X_train_np.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train_np, X_test_np, None\n",
    "    elif k_features == 'all': return X_train_np, X_test_np, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train_np, X_test_np, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train_np, X_test_np, None\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train_np, y_train_np)\n",
    "    X_train_selected = selector.transform(X_train_np)\n",
    "    X_test_selected = selector.transform(X_test_np)\n",
    "\n",
    "    print(f\"original feature shape: {X_train_np.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector # Returns NumPy arrays\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    if estimator is None: estimator = CUML_LogisticRegression(solver='qn', random_state=42, max_iter=2000) # Use cuML LogisticRegression\n",
    "    X_train_cp = cp.asarray(X_train) if not isinstance(X_train, cp.ndarray) else X_train\n",
    "    y_train_cp = cp.asarray(y_train) if not isinstance(y_train, cp.ndarray) else y_train\n",
    "    X_test_cp = cp.asarray(X_test) if not isinstance(X_test, cp.ndarray) else X_test\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train_cp, X_test_cp, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train_cp, X_test_cp, None\n",
    "    else: return X_train_cp, X_test_cp, None\n",
    "\n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train_cp, X_test_cp, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train_cp, y_train_cp) \n",
    "        X_train_selected = rfe.transform(X_train_cp)\n",
    "        X_test_selected = rfe.transform(X_test_cp)\n",
    "        print(f\"original feature shape: {X_train_cp.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e:\n",
    "        print(f\"RFE error: {e}\")\n",
    "        return X_train_cp, X_test_cp, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    X_train_for_fit = X_train\n",
    "    y_train_for_fit = y_train\n",
    "\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "        X_train_for_fit = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "        y_train_for_fit = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "    elif isinstance(model_estimator, (CUMLSVC, lgb.LGBMClassifier)):\n",
    "        X_train_for_fit = cp.asarray(X_train) if not isinstance(X_train, cp.ndarray) else X_train\n",
    "        y_train_for_fit = cp.asarray(y_train) if not isinstance(y_train, cp.ndarray) else y_train\n",
    "        \n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=4, \n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "\n",
    "    search_cv.fit(X_train_for_fit, y_train_for_fit, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_test_for_predict = X_test\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        X_test_for_predict = X_test.get() if isinstance(X_test, cp.ndarray) else X_test\n",
    "    elif isinstance(model, (CUMLSVC, lgb.LGBMClassifier)):\n",
    "        X_test_for_predict = cp.asarray(X_test) if not isinstance(X_test, cp.ndarray) else X_test\n",
    "\n",
    "    y_pred_raw = model.predict(X_test_for_predict)\n",
    "    y_pred = y_pred_raw.get() if isinstance(y_pred_raw, cp.ndarray) else y_pred_raw\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_test_np = y_test.get() if isinstance(y_test, cp.ndarray) else y_test\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "    precision = precision_score(y_test_np, y_pred)\n",
    "    recall = recall_score(y_test_np, y_pred)\n",
    "    f1 = f1_score(y_test_np, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_np, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:\n",
    "        X_train_cp = cp.asarray(X_train) if not isinstance(X_train, cp.ndarray) else X_train\n",
    "        X_test_cp = cp.asarray(X_test) if not isinstance(X_test, cp.ndarray) else X_test\n",
    "        pca = CUMLPCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train_cp)\n",
    "        X_test_pca = pca.transform(X_test_cp)\n",
    "        print(f\"original feature shape: {X_train_cp.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {cp.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "        return X_train_pca, X_test_pca, pca \n",
    "    except Exception as e:\n",
    "        print(f\"PCA error: {e}\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "    if flatten_layer is None: raise ValueError(\"Flatten layer not found in CNN architecture.\")\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/data_subsets/D-Fire/train/\"\n",
    "    target_image_width = 128\n",
    "    target_image_height = 128\n",
    "\n",
    "    X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "    if X_images.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- extracting cnn features: ---\")\n",
    "    cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "    features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "    if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- scaling cnn features (GPU accelerated with cuML StandardScaler): ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "    if X_train_scaled is not None:\n",
    "        feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "        feature_transformers['Scaled_All_CNN'] = scaler\n",
    "    else: exit()\n",
    "\n",
    "    print(\"\\n--- selection & pca: ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "\n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\ncorr selection: {percentage_str} (CPU-bound SelectKBest)...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correlation selection: {e}\")\n",
    "            pass\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']\n",
    "    rfe_step_val = 0.1\n",
    "    rfe_estimator_cuml = CUML_LogisticRegression(solver='qn', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "        print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val}, GPU-accelerated estimator with cuML LogisticRegression)...\")\n",
    "        try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator_cuml\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe) # Store CuPy arrays\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in RFE selection: {e}\")\n",
    "            pass\n",
    "\n",
    "    pca_components = [0.95, 500]\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\npca with n_components={n_comp} (GPU accelerated with cuML PCA)...\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            pca_successful = X_train_pca is not None and \\\n",
    "                             ((isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count) or \\\n",
    "                              (isinstance(n_comp, float) and X_train_pca.shape[1] > 0))\n",
    "\n",
    "            if pca_successful:\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca) # Store CuPy arrays\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in PCA reduction: {e}\")\n",
    "            pass\n",
    "\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        data_type = \"CuPy\" if isinstance(X_train_fs, cp.ndarray) else \"NumPy\"\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features ({data_type})\")\n",
    "\n",
    "    print(\"\\n--- model training and randomsearchcv: ---\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': CUMLSVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1, n_jobs=1, device='gpu'),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [-1, 10, 20],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],\n",
    "                'min_child_samples': [5]\n",
    "            }\n",
    "        },\n",
    "        'Custom_MLP': {\n",
    "            'estimator': SklearnKerasClassifier(\n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128],\n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'\n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 8)\n",
    "        print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\")\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            fs_data_type = \"CuPy\" if isinstance(X_train_fs, cp.ndarray) else \"NumPy\"\n",
    "            print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features, {fs_data_type}) ---\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch',\n",
    "                n_iter=n_iter_search,\n",
    "                validation_split_keras=0.2\n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)\n",
    "                }\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "    print(\"\\n\\n=== results summary for all models ===\")\n",
    "    if not all_results: pass\n",
    "    else:\n",
    "        print(\"\\nbest cv f1 scores:\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    cv_score = result.get('best_cv_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "            else: pass\n",
    "\n",
    "        print(\"\\ntest results - f1:\")\n",
    "        print(\"----------------------------\")\n",
    "        best_f1_per_model = {}\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            if fs_results:\n",
    "                best_test_f1_for_model = -np.inf\n",
    "                best_fs_name_for_model = None\n",
    "\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "                    if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                        best_test_f1_for_model = test_f1\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                if best_fs_name_for_model:\n",
    "                    best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "            else: continue\n",
    "\n",
    "        print(\"\\n=== best combo based on f1's ===\")\n",
    "        if best_overall_combination:\n",
    "            model_name, fs_name = best_overall_combination\n",
    "            best_result = all_results[model_name][fs_name]\n",
    "            test_metrics = best_result['test_metrics']\n",
    "\n",
    "            print(f\"best model: {model_name}\")\n",
    "            actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "            print(f\"best fs: {fs_name} ({actual_feature_count} features)\")\n",
    "            print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\")\n",
    "            print(f\"test f1: {test_metrics['f1_score']:.4f}\")\n",
    "            print(f\"test acc: {test_metrics['accuracy']:.4f}\")\n",
    "            print(f\"test prec: {test_metrics['precision']:.4f}\")\n",
    "            print(f\"test rec: {test_metrics['recall']:.4f}\")\n",
    "            print(f\"params: {best_result['best_params']}\\n\")\n",
    "            print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "        else: pass\n",
    "\n",
    "    MODEL_SAVE_DIR = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/models/\"\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    print(\"\\n--- saving best hybrids ---\")\n",
    "    if 'best_f1_per_model' not in locals() or not best_f1_per_model: pass\n",
    "    else:\n",
    "        for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "            print(f\"\\nprocessing {model_name}...\")\n",
    "            if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "                best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "                model_to_save = best_combination_results.get('trained_model')\n",
    "                transformer_to_save = best_combination_results.get('transformer')\n",
    "                if model_to_save:\n",
    "                    is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                    file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                    model_filename = f'Dfire_hybrid_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                    MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                    try:\n",
    "                        if is_keras_model: model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                        else: joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                        print(f\"   saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error saving model {model_filename}: {e}\")\n",
    "                else:\n",
    "                    print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "\n",
    "                if transformer_to_save and 'All_CNN' not in best_fs_name_for_model:\n",
    "                     transformer_filename = f'Dfire_hybrid_transformer_{best_fs_name_for_model}.pkl'\n",
    "                     TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                     try:\n",
    "                         joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                         print(f\"   saved feature s/r transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"   Error saving transformer {transformer_filename}: {e}\")\n",
    "                         print(\"   (Note: Saving cuML transformers with joblib might require specific handling or conversion to CPU state for complex objects.)\")\n",
    "\n",
    "    print(\"\\n--- all done!! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
