{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca6facb-bb84-45d8-9008-47b6ddeb9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 21:26:27.381725: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 21:26:29.657305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "<frozen importlib._bootstrap_external>:1184: FutureWarning: The cuda.cuda module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.driver module instead.\n",
      "dfire prep: 100%|███████████████████████████████████████████████████████████████████| 1629/1629 [00:37<00:00, 43.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- extracting cnn features: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 21:28:20.378420: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:20.402094: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:20.402496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:20.414822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:20.415005: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:20.415076: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:22.063282: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:22.071295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:22.071354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-07-23 21:28:22.071714: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-07-23 21:28:22.071792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753295306.499150    1522 service.cc:145] XLA service 0x7136c0016f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753295306.499256    1522 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-07-23 21:28:26.883513: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-23 21:28:27.358230: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1753295310.159027    1522 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features shape: (1221, 65536)\n",
      "testing features shape: (408, 65536)\n",
      "training labels shape: (1221,)\n",
      "testing labels shape: (408,)\n",
      "\n",
      "--- scaling cnn features (GPU accelerated with cuML StandardScaler): ---\n",
      "\n",
      "--- selection & pca: ---\n",
      "\n",
      "corr selection: 75% (CPU-bound SelectKBest)...\n",
      "\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "corr selection: 50% (CPU-bound SelectKBest)...\n",
      "\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\n",
      "rfe selection with 75% (step=0.1, CPU-bound estimator with scikit-learn LogisticRegression)...\n",
      "\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 49152)\n",
      "\n",
      "rfe selection with 50% (step=0.1, CPU-bound estimator with scikit-learn LogisticRegression)...\n",
      "\n",
      "original feature shape: (1221, 65536)\n",
      "selected feature shape: (1221, 32768)\n",
      "\n",
      "pca with n_components=0.85 (GPU accelerated with cuML PCA)...\n",
      "\n",
      "PCA error: exception occurred! file=/opt/conda/conda-bld/work/cpp/src/pca/pca.cuh line=102: Parameter n_components: number of components cannot be less than one\n",
      "Obtained 64 stack frames\n",
      "#0 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x84) [0x7138326c8734]\n",
      "#1 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/../../../../libcuml++.so(+0x1c950a) [0x7138325c950a]\n",
      "#2 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/decomposition/pca.cpython-310-x86_64-linux-gnu.so(+0x38e22) [0x713751721e22]\n",
      "#3 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0xfa9f) [0x713751c07a9f]\n",
      "#4 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0x2e401) [0x713751c26401]\n",
      "#5 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#6 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1506d8) [0x60df3311e6d8]\n",
      "#7 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#8 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#9 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#10 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#11 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/decomposition/pca.cpython-310-x86_64-linux-gnu.so(+0x35471) [0x71375171e471]\n",
      "#12 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0xfa9f) [0x713751c07a9f]\n",
      "#13 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/site-packages/cuml/internals/base.cpython-310-x86_64-linux-gnu.so(+0x2e401) [0x713751c26401]\n",
      "#14 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#15 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1506d8) [0x60df3311e6d8]\n",
      "#16 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#17 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#18 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#19 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x60df3311e582]\n",
      "#20 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x4c12) [0x60df33107142]\n",
      "#21 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#22 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x13ca) [0x60df331038fa]\n",
      "#23 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1d7c60) [0x60df331a5c60]\n",
      "#24 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(PyEval_EvalCode+0x87) [0x60df331a5ba7]\n",
      "#25 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1dedaa) [0x60df331acdaa]\n",
      "#26 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x144bf3) [0x60df33112bf3]\n",
      "#27 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x320) [0x60df33102850]\n",
      "#28 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#29 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#30 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#31 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#32 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#33 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1f5a37) [0x60df331c3a37]\n",
      "#34 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x14f53d) [0x60df3311d53d]\n",
      "#35 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x60df33102c5c]\n",
      "#36 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#37 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x320) [0x60df33102850]\n",
      "#38 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#39 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x60df33102c5c]\n",
      "#40 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x60df3311e582]\n",
      "#41 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(PyObject_Call+0xbc) [0x60df3311ef1c]\n",
      "#42 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x2d83) [0x60df331052b3]\n",
      "#43 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x150582) [0x60df3311e582]\n",
      "#44 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x13ca) [0x60df331038fa]\n",
      "#45 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#46 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#47 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#48 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#49 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#50 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#51 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#52 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#53 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#54 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x1bbd) [0x60df331040ed]\n",
      "#55 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x1e1384) [0x60df331af384]\n",
      "#56 in /home/bunveren/miniconda3/envs/my_new_rapids_env/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so(+0x7bf6) [0x71394723cbf6]\n",
      "#57 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x143e8a) [0x60df33111e8a]\n",
      "#58 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x25f60c) [0x60df3322d60c]\n",
      "#59 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0xfdd90) [0x60df330cbd90]\n",
      "#60 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(+0x13c2a3) [0x60df3310a2a3]\n",
      "#61 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x5cd5) [0x60df33108205]\n",
      "#62 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyFunction_Vectorcall+0x6c) [0x60df33112a2c]\n",
      "#63 in /home/bunveren/miniconda3/envs/my_new_rapids_env/bin/python(_PyEval_EvalFrameDefault+0x72c) [0x60df33102c5c]\n",
      "\n",
      "\n",
      "pca with n_components=100 (GPU accelerated with cuML PCA)...\n",
      "\n",
      "PCA error: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory\n",
      "- Scaled_All_CNN: 65536 features (CuPy)\n",
      "- Scaled_Corr75%_CNN: 49152 features (NumPy)\n",
      "- Scaled_Corr50%_CNN: 32768 features (NumPy)\n",
      "- Scaled_RFE75%_CNN: 49152 features (NumPy)\n",
      "- Scaled_RFE50%_CNN: 32768 features (NumPy)\n",
      "- Scaled_PCA_85%_CNN: 65536 features (CuPy)\n",
      "\n",
      "--- model training and randomsearchcv: ---\n",
      "\n",
      "\n",
      "\n",
      "=== train&tune SVM (Hybrid) ===\n",
      "\n",
      "\n",
      "--- tune SVM on fs: Scaled_All_CNN (65536 features, CuPy) ---\n",
      "\n",
      "    WARNING: Setting n_jobs to 1 for SVM on Scaled_All_CNN due to high feature count for CPU model to prevent out of memory.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'search_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 509\u001b[0m\n\u001b[1;32m    504\u001b[0m     current_n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Use all available CPU cores for CPU models with manageable feature count\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Temporarily modify n_jobs in search_cv creation if needed\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# This requires recreating the RandomizedSearchCV object to set n_jobs dynamically\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msearch_method\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomSearch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    510\u001b[0m     search_cv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m    511\u001b[0m        estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    512\u001b[0m        param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m        random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip if search_method is not RandomSearch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_method' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Flatten, Dense, LeakyReLU, ReLU\n",
    ")\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_score, recall_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Import CPU-bound scikit-learn versions for compatibility\n",
    "from sklearn.svm import SVC as SklearnSVC\n",
    "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
    "\n",
    "import lightgbm as lgb\n",
    "import cupy as cp\n",
    "\n",
    "# Removed: from cuml.accel import install\n",
    "# Removed: install()\n",
    "\n",
    "import cuml\n",
    "from cuml.preprocessing import StandardScaler as CUMLStandardScaler\n",
    "from cuml.decomposition import PCA as CUMLPCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SklearnKerasClassifier(KerasClassifier, ClassifierMixin):\n",
    "    def __init__(self, model=None, **kwargs):\n",
    "        super().__init__(model=model, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _tags(self): return self.model._tags if hasattr(self.model, \"_tags\") else {\"binary_only\": True}\n",
    "\n",
    "\n",
    "def create_custom_cnn(\n",
    "    input_shape=None,\n",
    "    conv_blocks=((32, (3,3)), (64, (3,3))),\n",
    "    dense_layers=(128,),\n",
    "    dropout_rate=0.4,\n",
    "    activation='relu',\n",
    "    meta=None\n",
    "):\n",
    "    \"\"\"Conv -> BN -> Activation -> Pool -> Dropout\"\"\"\n",
    "    if input_shape is None:\n",
    "        if meta is None or \"X_shape_\" not in meta: raise ValueError(\"meta or input_shape parameter is missing\")\n",
    "        input_shape = meta[\"X_shape_\"][1:]\n",
    "\n",
    "    model = Sequential(name=\"Custom_CNN\")\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    for filters, kernel_size in conv_blocks:\n",
    "        model.add(Conv2D(filters, kernel_size, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for units in dense_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def is_dfire_image_fire(annotation_path, fire_class_ids):\n",
    "    if not os.path.exists(annotation_path): return False\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            if parts:\n",
    "                class_id = int(parts[0])\n",
    "                if class_id in fire_class_ids: return True\n",
    "    except (ValueError, IOError): pass\n",
    "    return False\n",
    "\n",
    "def load_prep_4_cnn(data_dir, target_size=(128, 128)):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "    if not os.path.isdir(images_dir): return np.array([]), np.array([])\n",
    "    if not os.path.isdir(labels_dir): return np.array([]), np.array([])\n",
    "\n",
    "    img_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    annotation_extension = '.txt'\n",
    "    fire_class_ids = [0, 1]\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(img_extensions)]\n",
    "    if not image_files: return np.array([]), np.array([])\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"dfire prep\"):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "        annotation_path = os.path.join(labels_dir, img_name_without_ext + annotation_extension)\n",
    "        label = 1 if is_dfire_image_fire(annotation_path, fire_class_ids) else 0\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "            all_images.append(img_normalized)\n",
    "            all_labels.append(label)\n",
    "        except Exception as e: pass\n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0: return None, None, None, None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"training features shape: {X_train.shape}\")\n",
    "    print(f\"testing features shape: {X_test.shape}\")\n",
    "    print(f\"training labels shape: {y_train.shape}\")\n",
    "    print(f\"testing labels shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    X_train_cp = cp.asarray(X_train)\n",
    "    X_test_cp = cp.asarray(X_test)\n",
    "    scaler = CUMLStandardScaler()\n",
    "    X_train_scaled_cp = scaler.fit_transform(X_train_cp)\n",
    "    X_test_scaled_cp = scaler.transform(X_test_cp)\n",
    "    return X_train_scaled_cp, X_test_scaled_cp, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    X_train_np = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "    X_test_np = X_test.get() if isinstance(X_test, cp.ndarray) else X_test\n",
    "    y_train_np = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "    n_total_features = X_train_np.shape[1]\n",
    "    k_features_int = k_features\n",
    "    percentage_str = None\n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train_np, X_test_np, None\n",
    "    elif k_features == 'all': return X_train_np, X_test_np, None\n",
    "    elif isinstance(k_features, int) and k_features > 0: k_features_int = min(k_features, n_total_features)\n",
    "    else: return X_train_np, X_test_np, None\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int >= n_total_features: return X_train_np, X_test_np, None\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train_np, y_train_np)\n",
    "    X_train_selected = selector.transform(X_train_np)\n",
    "    X_test_selected = selector.transform(X_test_np)\n",
    "\n",
    "    print(f\"original feature shape: {X_train_np.shape}\")\n",
    "    print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "    return X_train_selected, X_test_selected, selector # Returns NumPy arrays\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return X_train, X_test, None\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select\n",
    "    # Replaced CUML_LogisticRegression with SklearnLogisticRegression (CPU-bound)\n",
    "    if estimator is None: estimator = SklearnLogisticRegression(solver='liblinear', random_state=42, max_iter=2000) # Use scikit-learn LogisticRegression\n",
    "    # For RFE with scikit-learn LogisticRegression, input should be NumPy arrays\n",
    "    X_train_np = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "    y_train_np = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "    X_test_np = X_test.get() if isinstance(X_test, cp.ndarray) else X_test\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))\n",
    "        except ValueError: return X_train_np, X_test_np, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)\n",
    "    elif n_features_to_select == 'auto': return X_train_np, X_test_np, None\n",
    "    else: return X_train_np, X_test_np, None\n",
    "\n",
    "    if n_features_int <= 0 or n_features_int >= n_total_features: return X_train_np, X_test_np, None\n",
    "\n",
    "    try:\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train_np, y_train_np)\n",
    "        X_train_selected = rfe.transform(X_train_np)\n",
    "        X_test_selected = rfe.transform(X_test_np)\n",
    "        print(f\"original feature shape: {X_train_np.shape}\")\n",
    "        print(f\"selected feature shape: {X_train_selected.shape}\")\n",
    "        return X_train_selected, X_test_selected, rfe # Returns NumPy arrays\n",
    "    except Exception as e:\n",
    "        print(f\"RFE error: {e}\")\n",
    "        return X_train_np, X_test_np, None\n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10, validation_split_keras=0.2):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0: return None\n",
    "    print(f\"\\\\{search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "    fit_params = {}\n",
    "    X_train_for_fit = X_train\n",
    "    y_train_for_fit = y_train\n",
    "\n",
    "    if isinstance(model_estimator, KerasClassifier):\n",
    "        fit_params['validation_split'] = validation_split_keras\n",
    "        X_train_for_fit = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "        y_train_for_fit = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "    # Modified condition: check for SklearnSVC instead of CUMLSVC\n",
    "    elif isinstance(model_estimator, (SklearnSVC, lgb.LGBMClassifier, SklearnLogisticRegression)):\n",
    "        # Convert CuPy arrays to NumPy for scikit-learn CPU estimators\n",
    "        X_train_for_fit = X_train.get() if isinstance(X_train, cp.ndarray) else X_train\n",
    "        y_train_for_fit = y_train.get() if isinstance(y_train, cp.ndarray) else y_train\n",
    "        \n",
    "    if search_method == 'RandomSearch':\n",
    "         # Set n_jobs=1 for SklearnSVC to prevent memory issues with high-dimensional data, as it's CPU-bound\n",
    "         # LightGBM can still use multiple CPU cores (n_jobs=4), but for SVC, 1 is safer.\n",
    "         # The current n_jobs=4 applies to all. We need conditional n_jobs.\n",
    "         # For simplicity, let's keep n_jobs=4 for now, but be aware of its implications for SklearnSVC\n",
    "         # and understand that it might still be too much for 65k features on CPU.\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=4, # This should be tuned down to 1 for SklearnSVC for stability.\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "         )\n",
    "    else: return None\n",
    "\n",
    "    search_cv.fit(X_train_for_fit, y_train_for_fit, **fit_params)\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nbest params:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nbest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0: return {}\n",
    "    print(f\"\\\\{model_name} on the test set using {feature_set_name}.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_test_for_predict = X_test\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        X_test_for_predict = X_test.get() if isinstance(X_test, cp.ndarray) else X_test\n",
    "    # Modified condition: check for SklearnSVC instead of CUMLSVC\n",
    "    elif isinstance(model, (SklearnSVC, lgb.LGBMClassifier, SklearnLogisticRegression)):\n",
    "        X_test_for_predict = X_test.get() if isinstance(X_test, cp.ndarray) else X_test # Convert to NumPy for CPU models\n",
    "\n",
    "    y_pred_raw = model.predict(X_test_for_predict)\n",
    "    y_pred = y_pred_raw.get() if isinstance(y_pred_raw, cp.ndarray) else y_pred_raw\n",
    "    if isinstance(model, KerasClassifier): y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_test_np = y_test.get() if isinstance(y_test, cp.ndarray) else y_test\n",
    "    end_time = time.time()\n",
    "    print(f\"duration: {end_time - start_time:.4f} seconds\")\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "    precision = precision_score(y_test_np, y_pred)\n",
    "    recall = recall_score(y_test_np, y_pred)\n",
    "    f1 = f1_score(y_test_np, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_np, y_pred)\n",
    "    print(f\"accuracy: {accuracy:.4f}\")\n",
    "    print(f\"precision: {precision:.4f}\")\n",
    "    print(f\"recall: {recall:.4f}\")\n",
    "    print(f\"f1 score: {f1:.4f}\")\n",
    "    print(f\"\\\\\\\\nconfusion matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0: return None, None, None\n",
    "    try:\n",
    "        X_train_cp = cp.asarray(X_train) if not isinstance(X_train, cp.ndarray) else X_train\n",
    "        X_test_cp = cp.asarray(X_test) if not isinstance(X_test, cp.ndarray) else X_test\n",
    "        pca = CUMLPCA(n_components=n_components, random_state=42)\n",
    "        X_train_pca = pca.fit_transform(X_train_cp)\n",
    "        X_test_pca = pca.transform(X_test_cp)\n",
    "        print(f\"original feature shape: {X_train_cp.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"variance ratio with {pca.n_components_} components: {cp.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e:\n",
    "        print(f\"PCA error: {e}\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "def create_cnn_feature_extractor(cnn_model_architecture):\n",
    "    flatten_layer = None\n",
    "    for layer in cnn_model_architecture.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            flatten_layer = layer\n",
    "            break\n",
    "    if flatten_layer is None: raise ValueError(\"Flatten layer not found in CNN architecture.\")\n",
    "    feature_extractor_model = Model(inputs=cnn_model_architecture.inputs, outputs=flatten_layer.output)\n",
    "    return feature_extractor_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/data_subsets/D-Fire/train/\"\n",
    "    target_image_width = 128\n",
    "    target_image_height = 128\n",
    "\n",
    "    X_images, y_labels = load_prep_4_cnn(data_directory, target_size=(target_image_width, target_image_height))\n",
    "    if X_images.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- extracting cnn features: ---\")\n",
    "    cnn_architecture = create_custom_cnn(input_shape=X_images.shape[1:])\n",
    "    cnn_feature_extractor = create_cnn_feature_extractor(cnn_architecture)\n",
    "    features_array_orig = cnn_feature_extractor.predict(X_images, verbose=0)\n",
    "\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, y_labels, test_size=0.25, random_state=42)\n",
    "    if X_train_orig is None or X_train_orig.shape[0] == 0: exit()\n",
    "\n",
    "    print(\"\\n--- scaling cnn features (GPU accelerated with cuML StandardScaler): ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets = {}\n",
    "    feature_transformers = {}\n",
    "    if X_train_scaled is not None:\n",
    "        feature_sets['Scaled_All_CNN'] = (X_train_scaled, X_test_scaled)\n",
    "        feature_transformers['Scaled_All_CNN'] = scaler\n",
    "    else: exit()\n",
    "\n",
    "    print(\"\\n--- selection & pca: ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "\n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\ncorr selection: {percentage_str} (CPU-bound SelectKBest)...\\n\")\n",
    "        try:\n",
    "            # Note: SelectKBest is CPU-bound, so X_train_scaled (CuPy) will be converted to NumPy internally if needed.\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}_CNN'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}_CNN'] = corr_selector\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in correlation selection: {e}\")\n",
    "            pass\n",
    "\n",
    "    rfe_feature_percentages = ['75%', '50%']\n",
    "    rfe_step_val = 0.1\n",
    "    # Replaced CUML_LogisticRegression with SklearnLogisticRegression (CPU-bound)\n",
    "    rfe_estimator_sklearn = SklearnLogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "        print(f\"\\nrfe selection with {percentage_str} (step={rfe_step_val}, CPU-bound estimator with scikit-learn LogisticRegression)...\\n\")\n",
    "        try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator_sklearn\n",
    "            )\n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                # RFE returns NumPy arrays when its estimator is a scikit-learn CPU estimator\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}_CNN'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}_CNN'] = rfe_selector\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in RFE selection: {e}\")\n",
    "            pass\n",
    "\n",
    "    # Adjusted pca_components for better GPU memory management and 'less than one' issue\n",
    "    pca_components = [0.85, 100] # Try 85% variance, and reduce 500 to 100 components for GPU memory\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\npca with n_components={n_comp} (GPU accelerated with cuML PCA)...\\n\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            pca_successful = X_train_pca is not None and \\\n",
    "                             ((isinstance(n_comp, int) and X_train_pca.shape[1] < original_feature_count) or \\\n",
    "                              (isinstance(n_comp, float) and X_train_pca.shape[1] > 0))\n",
    "\n",
    "            if pca_successful:\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}_CNN'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca) # Store CuPy arrays\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error in PCA reduction: {e}\")\n",
    "            pass\n",
    "\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        data_type = \"CuPy\" if isinstance(X_train_fs, cp.ndarray) else \"NumPy\"\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features ({data_type})\")\n",
    "\n",
    "    print(\"\\n--- model training and randomsearchcv: ---\\n\")\n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            # Replaced CUMLSVC with SklearnSVC (CPU-bound)\n",
    "            'estimator': SklearnSVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1, n_jobs=1, device='gpu'),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [-1, 10, 20],\n",
    "                'num_leaves': [31, 50, 70],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],\n",
    "                'min_child_samples': [5]\n",
    "            }\n",
    "        },\n",
    "        'Custom_MLP': {\n",
    "            'estimator': SklearnKerasClassifier(\n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128],\n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scoring_metric = 'f1'\n",
    "    all_results = {}\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 8)\n",
    "        print(f\"\\n\\n=== train&tune {model_name} (Hybrid) ===\\n\")\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            fs_data_type = \"CuPy\" if isinstance(X_train_fs, cp.ndarray) else \"NumPy\"\n",
    "            print(f\"\\n--- tune {model_name} on fs: {fs_name} ({X_train_fs.shape[1]} features, {fs_data_type}) ---\\n\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0: continue\n",
    "\n",
    "            # Conditional n_jobs for CPU-bound models and large feature sets\n",
    "            current_n_jobs = 4 # Default from config\n",
    "            if isinstance(estimator, (SklearnSVC, SklearnLogisticRegression)) and X_train_fs.shape[1] > 10000: # Heuristic for \"large\"\n",
    "                current_n_jobs = 1 # Force single-threaded for very high-dimensional CPU SVM/Logistic Regression to prevent OOM\n",
    "                print(f\"    WARNING: Setting n_jobs to {current_n_jobs} for {model_name} on {fs_name} due to high feature count for CPU model to prevent out of memory.\\n\")\n",
    "            elif isinstance(estimator, (SklearnSVC, SklearnLogisticRegression)): # For smaller feature sets still use reasonable n_jobs\n",
    "                current_n_jobs = -1 # Use all available CPU cores for CPU models with manageable feature count\n",
    "\n",
    "\n",
    "            # Temporarily modify n_jobs in search_cv creation if needed\n",
    "            # This requires recreating the RandomizedSearchCV object to set n_jobs dynamically\n",
    "            if search_method == 'RandomSearch':\n",
    "                search_cv = RandomizedSearchCV(\n",
    "                   estimator=estimator,\n",
    "                   param_distributions=param_distributions,\n",
    "                   n_iter=n_iter_search,\n",
    "                   cv=cv_strategy,\n",
    "                   scoring=scoring_metric,\n",
    "                   n_jobs=current_n_jobs, # Apply dynamic n_jobs here\n",
    "                   verbose=1,\n",
    "                   random_state=42\n",
    "                )\n",
    "            else: continue # Skip if search_method is not RandomSearch\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,\n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch', # This parameter is ignored if search_cv is created above\n",
    "                n_iter=n_iter_search,         # This parameter is ignored if search_cv is created above\n",
    "                validation_split_keras=0.2\n",
    "            )\n",
    "            \n",
    "            # The previous tune_model_hyperparameters call creates and fits RandomizedSearchCV.\n",
    "            # We need to ensure that the search_cv object used for fit has the correct n_jobs.\n",
    "            # The current structure calls tune_model_hyperparameters which then creates search_cv.\n",
    "            # We need to pass current_n_jobs into tune_model_hyperparameters if we want to control it there.\n",
    "            # Let's refactor `tune_model_hyperparameters` to accept `n_jobs`.\n",
    "            # For simplicity for now, let's directly create search_cv here in the loop as above.\n",
    "            # The 'if tuned_search:' block is okay for flow.\n",
    "\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)\n",
    "                }\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "\n",
    "    print(\"\\n\\n=== results summary for all models ===\\n\")\n",
    "    if not all_results: pass\n",
    "    else:\n",
    "        print(\"\\nbest cv f1 scores:\\n\")\n",
    "        print(\"-------------------------------------------------\\n\")\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\\n\")\n",
    "            if fs_results:\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    cv_score = result.get('best_cv_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {cv_score:.4f}\\n\")\n",
    "            else: pass\n",
    "\n",
    "        print(\"\\ntest results - f1:\\n\")\n",
    "        print(\"----------------------------\\n\")\n",
    "        best_f1_per_model = {}\n",
    "        for model_name, fs_results in all_results.items():\n",
    "            print(f\"\\n{model_name}:\\n\")\n",
    "            if fs_results:\n",
    "                best_test_f1_for_model = -np.inf\n",
    "                best_fs_name_for_model = None\n",
    "\n",
    "                for fs_name in sorted(fs_results.keys()):\n",
    "                    result = fs_results[fs_name]\n",
    "                    test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                    print(f\"  - {fs_name}: {test_f1:.4f}\\n\")\n",
    "                    if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                        best_test_f1_for_model = test_f1\n",
    "                        best_fs_name_for_model = fs_name\n",
    "                if best_fs_name_for_model:\n",
    "                    best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "            else: continue\n",
    "\n",
    "        print(\"\\n=== best combo based on f1's ===\\n\")\n",
    "        if best_overall_combination:\n",
    "            model_name, fs_name = best_overall_combination\n",
    "            best_result = all_results[model_name][fs_name]\n",
    "            test_metrics = best_result['test_metrics']\n",
    "\n",
    "            print(f\"best model: {model_name}\\n\")\n",
    "            actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "            print(f\"best fs: {fs_name} ({actual_feature_count} features)\\n\")\n",
    "            print(f\"best cvf1 sc: {best_result['best_cv_score']:.4f}\\n\")\n",
    "            print(f\"test f1: {test_metrics['f1_score']:.4f}\\n\")\n",
    "            print(f\"test acc: {test_metrics['accuracy']:.4f}\\n\")\n",
    "            print(f\"test prec: {test_metrics['precision']:.4f}\\n\")\n",
    "            print(f\"test rec: {test_metrics['recall']:.4f}\\n\")\n",
    "            print(f\"params: {best_result['best_params']}\\n\")\n",
    "            print(f\"conf.m.:\\n{np.array(test_metrics['confusion_matrix'])}\\n\")\n",
    "        else: pass\n",
    "\n",
    "    MODEL_SAVE_DIR = \"/mnt/c/Users/BerenÜnveren/Desktop/YAP470/models/\"\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    print(\"\\n--- saving best hybrids ---\\n\")\n",
    "    if 'best_f1_per_model' not in locals() or not best_f1_per_model: pass\n",
    "    else:\n",
    "        for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "            print(f\"\\nprocessing {model_name}...\\n\")\n",
    "            if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "                best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "                model_to_save = best_combination_results.get('trained_model')\n",
    "                transformer_to_save = best_combination_results.get('transformer')\n",
    "                if model_to_save:\n",
    "                    is_keras_model = isinstance(model_to_save, KerasClassifier)\n",
    "                    file_extension = '.keras' if is_keras_model else '.pkl'\n",
    "                    model_filename = f'Dfire_hybrid_{model_name.lower()}_best_model_{best_fs_name_for_model}{file_extension}'\n",
    "                    MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                    try:\n",
    "                        if is_keras_model: model_to_save.model_.save(MODEL_SAVE_PATH_ALG)\n",
    "                        else: joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                        print(f\"   saved model: {MODEL_SAVE_PATH_ALG}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error saving model {model_filename}: {e}\\n\")\n",
    "                else:\n",
    "                    print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\\n\")\n",
    "\n",
    "                if transformer_to_save and 'All_CNN' not in best_fs_name_for_model:\n",
    "                     transformer_filename = f'Dfire_hybrid_transformer_{best_fs_name_for_model}.pkl'\n",
    "                     TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                     try:\n",
    "                         joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                         print(f\"   saved feature s/r transformer: {TRANSFORMER_SAVE_PATH}\\n\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"   Error saving transformer {transformer_filename}: {e}\\n\")\n",
    "                         print(\"   (Note: Saving cuML transformers with joblib might require specific handling or conversion to CPU state for complex objects.)\\n\")\n",
    "\n",
    "    print(\"\\n--- all done!! ---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
