{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and helper functions loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from skimage.feature import local_binary_pattern, hog\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def is_kaggle_image_fire(image_path):\n",
    "    parent_dir = os.path.basename(os.path.dirname(image_path))\n",
    "    return parent_dir == 'fire_images'\n",
    "\n",
    "def extract_color_histograms(img_processed, color_space, bins):\n",
    "    histograms = []\n",
    "    ranges = {\n",
    "        'hsv': {'float': ([0, 1], [0, 1], [0, 1]), 'uint8': ([0, 180], [0, 256], [0, 256])},\n",
    "        'ycbcr': {'float': ([0, 1], [-0.5, 0.5], [-0.5, 0.5]), 'uint8': ([0, 256], [0, 256], [0, 256])}\n",
    "    }\n",
    "    channel_indices = {'hsv': [0, 1], 'ycbcr': [1, 2]}\n",
    "    dtype_key = 'float' if img_processed.dtype in [np.float32, np.float64] else 'uint8'\n",
    "\n",
    "    if color_space in ranges and color_space in channel_indices:\n",
    "        for i in channel_indices[color_space]:\n",
    "            current_range = ranges[color_space][dtype_key][i]\n",
    "            if img_processed.dtype != np.float32 and img_processed.dtype != np.uint8:\n",
    "                 img_processed = img_processed.astype(np.float32)\n",
    "                 dtype_key = 'float'\n",
    "                 current_range = ranges[color_space][dtype_key][i]\n",
    "\n",
    "            hist = cv2.calcHist([img_processed], [i], None, [bins], current_range)\n",
    "            histograms.append(hist.flatten())\n",
    "\n",
    "    if histograms: return np.concatenate(histograms)\n",
    "    else: return np.array([])\n",
    "\n",
    "def extract_lbp_features(img_gray, radius, n_points, method):\n",
    "\n",
    "    if img_gray is None or img_gray.size == 0: return np.array([])\n",
    "    if n_points is None: n_points = 8 * radius\n",
    "\n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)\n",
    "\n",
    "    try:\n",
    "        lbp_image = local_binary_pattern(img_gray, n_points, radius, method=method)\n",
    "        if method == 'uniform' or method == 'nri_uniform':\n",
    "            n_bins = int(n_points + 2)\n",
    "            hist_range = (0, n_bins)\n",
    "        elif method == 'ror':\n",
    "            n_bins = int(n_points / radius + 2)\n",
    "            hist_range = (0, n_bins)\n",
    "        else:\n",
    "            n_bins = int(2**n_points)\n",
    "            hist_range = (0, n_bins)\n",
    "        lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=n_bins, range=hist_range)\n",
    "        lbp_hist = lbp_hist.astype(np.float32)\n",
    "        if lbp_hist.sum() > 0: lbp_hist /= lbp_hist.sum()\n",
    "        return lbp_hist.flatten()\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        return np.array([])\n",
    "\n",
    "def extract_hog_features(img_gray, orientations, pixels_per_cell, cells_per_block, block_norm):\n",
    "    if img_gray is None or img_gray.size == 0: return np.array([])\n",
    "    if img_gray.dtype != np.uint8 and img_gray.dtype != np.float64:\n",
    "         img_gray = img_gray.astype(np.float64)\n",
    "    img_h, img_w = img_gray.shape\n",
    "    cell_h, cell_w = pixels_per_cell\n",
    "    block_h, block_w = cells_per_block\n",
    "    min_img_h = cell_h * block_h\n",
    "    min_img_w = cell_w * block_w\n",
    "\n",
    "    if img_h < min_img_h or img_w < min_img_w:\n",
    "        return np.array([])\n",
    "    try:\n",
    "        hog_features = hog(img_gray, orientations=orientations,\n",
    "                           pixels_per_cell=pixels_per_cell,\n",
    "                           cells_per_block=cells_per_block,\n",
    "                           block_norm=block_norm,\n",
    "                           visualize=False, feature_vector=True)\n",
    "        return hog_features.flatten()\n",
    "    except Exception as e:\n",
    "        return np.array([])\n",
    "\n",
    "def combine_features(img_dict, feature_params):\n",
    "    all_features = []\n",
    "    if 'hsv' in img_dict and img_dict['hsv'] is not None:\n",
    "        hsv_hist = extract_color_histograms(img_dict['hsv'], 'hsv', bins=feature_params.get('hist_bins', 100))\n",
    "        if hsv_hist.size > 0: all_features.append(hsv_hist)\n",
    "    if 'ycbcr' in img_dict and img_dict['ycbcr'] is not None:\n",
    "        ycbcr_hist = extract_color_histograms(img_dict['ycbcr'], 'ycbcr', bins=feature_params.get('hist_bins', 100))\n",
    "        if ycbcr_hist.size > 0: all_features.append(ycbcr_hist)\n",
    "    if 'gray' in img_dict and img_dict['gray'] is not None:\n",
    "        img_gray_processed = img_dict['gray']\n",
    "\n",
    "        lbp_features = extract_lbp_features(img_gray_processed,\n",
    "                                            radius=feature_params.get('lbp_radius', 3),\n",
    "                                            n_points=feature_params.get('lbp_n_points', None),\n",
    "                                            method=feature_params.get('lbp_method', 'uniform'))\n",
    "        if lbp_features.size > 0: all_features.append(lbp_features)\n",
    "\n",
    "        hog_features = extract_hog_features(img_gray_processed,\n",
    "                                           orientations=feature_params.get('hog_orientations', 9),\n",
    "                                           pixels_per_cell=feature_params.get('hog_pixels_per_cell', (8, 8)),\n",
    "                                           cells_per_block=feature_params.get('hog_cells_per_block', (2, 2)),\n",
    "                                           block_norm=feature_params.get('hog_block_norm', 'L2-Hys'))\n",
    "        if hog_features.size > 0: all_features.append(hog_features)\n",
    "\n",
    "    if all_features:\n",
    "        all_features = [f.astype(np.float32) for f in all_features]\n",
    "        combined_vector = np.concatenate(all_features)\n",
    "        return combined_vector\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def load_and_extract_features_memory_safe(config, feature_params):\n",
    "    dataset_choice = config.get('dataset_choice', 'kaggle')\n",
    "    data_root = config.get('data_root')\n",
    "    target_size = config.get('target_img_size')\n",
    "    color_spaces_to_load = config.get('color_spaces_to_load', ['bgr', 'hsv', 'ycbcr'])\n",
    "    normalize_pixels = config.get('normalize_pixels', 1)\n",
    "\n",
    "    if not data_root or not target_size: return np.array([]), np.array([])\n",
    "\n",
    "    image_label_pairs = []\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    fire_dir = os.path.join(data_root, 'fire_images')\n",
    "    non_fire_dir = os.path.join(data_root, 'non_fire_images')\n",
    "\n",
    "    if not os.path.isdir(fire_dir) or not os.path.isdir(non_fire_dir):\n",
    "        print(f\"Error: 'fire_images' or 'non_fire_images' directory not found in {data_root}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    print(\"Determining Labels...\")\n",
    "     \n",
    "    fire_image_files = [os.path.join(fire_dir, f) for f in os.listdir(fire_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "    for img_path in tqdm(fire_image_files, desc=\"Processing Fire Images\", leave=False):\n",
    "        image_label_pairs.append((img_path, 1))  \n",
    "\n",
    "     \n",
    "    non_fire_image_files = [os.path.join(non_fire_dir, f) for f in os.listdir(non_fire_dir) if os.path.splitext(f)[1].lower() in img_extensions]\n",
    "    for img_path in tqdm(non_fire_image_files, desc=\"Processing Non-Fire Images\", leave=False):\n",
    "        image_label_pairs.append((img_path, 0))  \n",
    "\n",
    "    print(\"Label determination complete.\")\n",
    "\n",
    "    if not image_label_pairs:\n",
    "        print(\"No images with labels found.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "    total_images_processed = 0\n",
    "    total_images_skipped_reading = 0\n",
    "    total_images_skipped_feature = 0\n",
    "\n",
    "    print(\"Loading images and extracting features...\")\n",
    "    for image_path, label in tqdm(image_label_pairs, desc=\"Memory-safe Feature Extraction\", leave=False):\n",
    "        img_bgr = cv2.imread(image_path)\n",
    "\n",
    "        if img_bgr is None:\n",
    "            total_images_skipped_reading += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img_resized = cv2.resize(img_bgr, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            img_dict_single = {}  \n",
    "            img_processed_bgr = None\n",
    "            if normalize_pixels:\n",
    "                img_processed_bgr = img_resized.astype(np.float32) / 255.0 \n",
    "                img_resized_uint8 = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0  \n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2HSV).astype(np.float32) / np.array([180, 255, 255], dtype=np.float32)   \n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_resized_uint8, cv2.COLOR_BGR2YCrCb).astype(np.float32) / 255.0  \n",
    "            else:  \n",
    "                img_processed_bgr = img_resized.astype(np.uint8)\n",
    "                img_gray = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2GRAY)\n",
    "                if 'hsv' in color_spaces_to_load:\n",
    "                    img_dict_single['hsv'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2HSV)\n",
    "                if 'ycbcr' in color_spaces_to_load:\n",
    "                    img_dict_single['ycbcr'] = cv2.cvtColor(img_processed_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "            img_dict_single['gray'] = img_gray\n",
    "            if 'bgr' in color_spaces_to_load:\n",
    "                img_dict_single['bgr'] = img_processed_bgr\n",
    "\n",
    "            features_single = combine_features(img_dict_single, feature_params)\n",
    "\n",
    "            if features_single.size > 0:\n",
    "                all_features_list.append(features_single)\n",
    "                all_labels_list.append(label)\n",
    "                total_images_processed += 1\n",
    "            else:\n",
    "                total_images_skipped_feature += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            total_images_skipped_feature += 1\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n",
    "    print(f\"Total images initially found: {len(image_label_pairs)}\")\n",
    "    print(f\"Images skipped (read error): {total_images_skipped_reading}\")\n",
    "    print(f\"Images skipped (feature error): {total_images_skipped_feature}\")\n",
    "    print(f\"Images successfully processed: {total_images_processed}\")\n",
    "    if not all_features_list:\n",
    "        print(\"No features extracted from any image.\")\n",
    "        return np.array([]), np.array([])\n",
    "    features_array = np.array(all_features_list, dtype=np.float32)\n",
    "    labels_array = np.array(all_labels_list, dtype=np.int32)\n",
    "    print(f\"Final features array shape: {features_array.shape}\")\n",
    "    print(f\"Final labels array shape: {labels_array.shape}\")\n",
    "\n",
    "    return features_array, labels_array\n",
    "\n",
    "def get_config(dataset_choice):\n",
    "    config = {}\n",
    "    if dataset_choice == 'kaggle':\n",
    "        config['dataset_choice'] = 'kaggle'\n",
    "        config['data_root'] = os.path.join('..', 'data_subsets', 'fire_dataset')   \n",
    "        config['target_img_size'] = (128, 128)  \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']  \n",
    "        config['normalize_pixels'] = 1  \n",
    "        config['fire_class_ids'] = None   \n",
    "    elif dataset_choice == 'dfire':\n",
    "        config['dataset_choice'] = 'dfire'\n",
    "        config['dfire_root'] = os.path.join('..', 'data_subsets', 'D-Fire')  \n",
    "        config['split_name'] = \"train\"  \n",
    "        config['data_root'] = os.path.join(config['dfire_root'], config['split_name'])\n",
    "        config['target_img_size'] = (128, 128)  \n",
    "        config['color_spaces_to_load'] = ['bgr', 'hsv', 'ycbcr']  \n",
    "        config['normalize_pixels'] = 1  \n",
    "        config['fire_class_ids'] = [0, 1]  \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset choice: {dataset_choice}. Choose 'kaggle' or 'dfire'.\")\n",
    "\n",
    "    print(f\"Using dataset: {config['dataset_choice']}\")\n",
    "    print(f\"Data root: {config.get('data_root')}\")\n",
    "    print(f\"Target image size: {config['target_img_size']}\")\n",
    "    print(f\"Color spaces loaded: {config['color_spaces_to_load']}\")\n",
    "    print(f\"Normalize pixels: {bool(config['normalize_pixels'])}\")\n",
    "    if dataset_choice == 'dfire':\n",
    "         print(f\"D-Fire Split: {config['split_name']}\")\n",
    "         print(f\"D-Fire Fire Class IDs: {config['fire_class_ids']}\")\n",
    "    return config\n",
    "\n",
    "def get_feature_params():\n",
    "    feature_params = {\n",
    "        'hist_bins': 100,  \n",
    "        'lbp_radius': 3,  \n",
    "        'lbp_n_points': None,  \n",
    "        'lbp_method': 'uniform',  \n",
    "        'hog_orientations': 9,  \n",
    "        'hog_pixels_per_cell': (8, 8),  \n",
    "        'hog_cells_per_block': (2, 2),  \n",
    "        'hog_block_norm': 'L2-Hys'  \n",
    "    }\n",
    "    print(\"\\nFeature extraction parameters:\", feature_params)\n",
    "    return feature_params\n",
    "\n",
    "def split_data(features_array, labels_array, test_size=0.2, random_state=42):\n",
    "    if features_array.shape[0] == 0:\n",
    "        print(\"Feature array is empty, cannot split.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(f\"\\nSplitting data: training ({1-test_size:.0%}) testing ({test_size:.0%})\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_array,\n",
    "        labels_array,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels_array\n",
    "    )\n",
    "\n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Testing features shape: {X_test.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Testing labels shape: {y_test.shape}\")\n",
    "\n",
    "    train_labels, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
    "    print(f\"Train label distribution: {dict(zip(train_labels, train_counts))}\")\n",
    "    print(f\"Test label distribution: {dict(zip(test_labels, test_counts))}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"Scaling skipped: train or test data is empty.\")\n",
    "         return None, None, None\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaling complete.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def perform_correlation_selection(X_train, y_train, X_test, k_features):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"Correlation Selection skipped: train or test data is empty.\")\n",
    "         return X_train, X_test, None  \n",
    "\n",
    "    n_total_features = X_train.shape[1]\n",
    "    k_features_int = k_features  \n",
    "    percentage = None  \n",
    "    percentage_str = None  \n",
    "\n",
    "    if isinstance(k_features, str) and k_features.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = k_features  \n",
    "            percentage = float(k_features[:-1]) / 100.0\n",
    "            k_features_int = max(1, int(n_total_features * percentage))\n",
    "            print(f\"Selecting top {k_features_int} features based on {percentage_str} percentage using Correlation...\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for k_features: {k_features}. Skipping selection.\")\n",
    "            return X_train, X_test, None\n",
    "    elif k_features == 'all':\n",
    "         print(\"Selecting all features (no correlation selection)...\")\n",
    "         return X_train, X_test, None   \n",
    "    elif isinstance(k_features, int) and k_features > 0:\n",
    "        k_features_int = min(k_features, n_total_features)\n",
    "        print(f\"Selecting top {k_features_int} features by correlation...\")\n",
    "    else:\n",
    "        print(f\"Invalid k_features value: {k_features}. Must be int > 0, 'all', or percentage string (e.g., '75%'). Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "\n",
    "    if k_features_int <= 0 or k_features_int > n_total_features:\n",
    "         print(f\"Calculated number of features to select ({k_features_int}) is invalid. Skipping selection.\")\n",
    "         return X_train, X_test, None\n",
    "    if k_features_int == n_total_features:\n",
    "         print(\"Number of features to select is equal to total features. Skipping selection.\")\n",
    "         return X_train, X_test, None\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features_int)\n",
    "    selector.fit(X_train, y_train)  \n",
    "\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    print(f\"Original feature shape: {X_train.shape}\")\n",
    "    print(f\"Selected feature shape: {X_train_selected.shape}\")\n",
    "\n",
    "    return X_train_selected, X_test_selected, selector\n",
    "\n",
    "def perform_rfe_selection(X_train, y_train, X_test, n_features_to_select, step=0.1, estimator=None):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "         print(\"RFE Selection skipped: train or test data is empty.\")\n",
    "         return X_train, X_test, None  \n",
    "\n",
    "    n_total_features = X_train.shape[1]\n",
    "    n_features_int = n_features_to_select  \n",
    "    percentage = None  \n",
    "    percentage_str = None  \n",
    "\n",
    "    if estimator is None:\n",
    "        estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    if isinstance(n_features_to_select, str) and n_features_to_select.endswith('%'):\n",
    "        try:\n",
    "            percentage_str = n_features_to_select  \n",
    "            percentage = float(n_features_to_select[:-1]) / 100.0\n",
    "            n_features_int = max(1, int(n_total_features * percentage))  \n",
    "            print(f\"Selecting top {n_features_int} features based on {percentage_str} percentage using RFE...\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid percentage string for n_features_to_select: {n_features_to_select}. Skipping RFE.\")\n",
    "            return X_train, X_test, None\n",
    "    elif isinstance(n_features_to_select, int) and n_features_to_select > 0:\n",
    "        n_features_int = min(n_features_to_select, n_total_features)  \n",
    "        print(f\"Selecting {n_features_int} features using RFE...\")\n",
    "    elif n_features_to_select == 'auto':\n",
    "        print(\"RFE with 'auto' feature selection requires RFECV, which is not implemented in this helper. Skipping selection.\")\n",
    "        return X_train, X_test, None  \n",
    "    else:\n",
    "        print(f\"Invalid n_features_to_select value: {n_features_to_select}. Skipping RFE.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "    if n_features_int <= 0 or n_features_int > n_total_features:\n",
    "        print(f\"Calculated number of features for RFE ({n_features_int}) is invalid. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "    if n_features_int == n_total_features:\n",
    "        print(\"Number of features to select is equal to total features. Skipping selection.\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "    try:         \n",
    "        if n_features_int >= n_total_features:\n",
    "            print(\"Number of features to select is >= total features. Skipping RFE selection.\")\n",
    "            return X_train, X_test, None  \n",
    "\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features_int, step=step)\n",
    "        rfe.fit(X_train, y_train)  \n",
    "\n",
    "        X_train_selected = rfe.transform(X_train)\n",
    "        X_test_selected = rfe.transform(X_test)\n",
    "\n",
    "        print(f\"Original feature shape: {X_train.shape}\")\n",
    "        print(f\"Selected feature shape: {X_train_selected.shape}\")\n",
    "\n",
    "        return X_train_selected, X_test_selected, rfe\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE fit/transform: {e}\")\n",
    "        return X_train, X_test, None  \n",
    "\n",
    "def tune_model_hyperparameters(model_estimator, X_train, y_train, param_grid, cv_strategy, scoring='f1', search_method='RandomSearch', n_iter=10):\n",
    "    if X_train is None or y_train is None or X_train.shape[0] == 0:\n",
    "        print(\"Hyperparameter tuning skipped: training data is empty.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nPerforming {search_method} tuning (scoring='{scoring}')... with {n_iter} iterations\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if search_method == 'RandomSearch':\n",
    "         search_cv = RandomizedSearchCV(\n",
    "            estimator=model_estimator,\n",
    "            param_distributions=param_grid,  \n",
    "            n_iter=n_iter,  \n",
    "            cv=cv_strategy,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,  \n",
    "            verbose=1,\n",
    "            random_state=42  \n",
    "         )\n",
    "    else:\n",
    "        print(f\"Unknown search_method: {search_method}. Use 'GridSearch' or 'RandomSearch'.\")\n",
    "        return None\n",
    "\n",
    "    search_cv.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"{search_method} duration: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(search_cv.best_params_)\n",
    "    print(\"\\nBest CV score:\")\n",
    "    print(search_cv.best_score_)\n",
    "\n",
    "    return search_cv\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\", feature_set_name=\"Unknown Feature Set\"):\n",
    "    if model is None or X_test is None or y_test is None or X_test.shape[0] == 0:\n",
    "        print(f\"{model_name} evaluation skipped on {feature_set_name}: model not trained or test data is empty.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name} on the test set using {feature_set_name}...\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction duration: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix ({model_name} on {feature_set_name}):\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': conf_matrix.tolist()  \n",
    "    }\n",
    "\n",
    "def perform_pca_dimension_reduction(X_train, X_test, n_components):\n",
    "    if X_train is None or X_test is None or X_train.shape[0] == 0:\n",
    "        print(\"PCA skipped bc the data is empty..\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        n_total_features = X_train.shape[1]\n",
    "        if isinstance(n_components, float) and 0 < n_components < 1:\n",
    "            print(f\"Applying PCA to retain {n_components:.0%} of variance...\")\n",
    "        elif isinstance(n_components, int) and 0 < n_components < n_total_features:\n",
    "            print(f\"Applying PCA to reduce to {n_components} components...\")\n",
    "        else: return X_train, X_test, None\n",
    "        pca = PCA(n_components=n_components, random_state=42) # didn't wanna ruin a convention that came from a pop cultural joke\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        print(f\"Original feature shape: {X_train.shape}\")\n",
    "        print(f\"PCA transformed feature shape: {X_train_pca.shape}\")\n",
    "        print(f\"Explained variance ratio with {pca.n_components_} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "        return X_train_pca, X_test_pca, pca\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA: {e}\")\n",
    "        return X_train, X_test, None\n",
    "\n",
    "print(\"Imports and helper functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_extract_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: kaggle\n",
      "Data root: ..\\data_subsets\\fire_dataset\n",
      "Target image size: (128, 128)\n",
      "Color spaces loaded: ['bgr', 'hsv', 'ycbcr']\n",
      "Normalize pixels: True\n",
      "\n",
      "Feature extraction parameters: {'hist_bins': 100, 'lbp_radius': 3, 'lbp_n_points': None, 'lbp_method': 'uniform', 'hog_orientations': 9, 'hog_pixels_per_cell': (8, 8), 'hog_cells_per_block': (2, 2), 'hog_block_norm': 'L2-Hys'}\n",
      "Determining Labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label determination complete.\n",
      "Loading images and extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete.\n",
      "Total images initially found: 999\n",
      "Images skipped (read error): 1\n",
      "Images skipped (feature error): 0\n",
      "Images successfully processed: 998\n",
      "Final features array shape: (998, 8526)\n",
      "Final labels array shape: (998,)\n",
      "\n",
      "--- Splitting Data ---\n",
      "\n",
      "Splitting data: training (80%) testing (20%)\n",
      "Training features shape: (798, 8526)\n",
      "Testing features shape: (200, 8526)\n",
      "Training labels shape: (798,)\n",
      "Testing labels shape: (200,)\n",
      "Train label distribution: {0: 194, 1: 604}\n",
      "Test label distribution: {0: 49, 1: 151}\n",
      "\n",
      "--- Scaling Features (Initial) ---\n",
      "\n",
      "Scaling features...\n",
      "Scaling complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset_choice = 'kaggle'\n",
    "\n",
    "try:\n",
    "    config = get_config(dataset_choice)\n",
    "    feature_params = get_feature_params()\n",
    "    features_array_orig, labels_array = load_and_extract_features_memory_safe(config, feature_params)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Configuration Error: {e}\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File Not Found Error: {e}. Please check data paths in get_config.\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during data loading and feature extraction: {e}\")\n",
    "    features_array_orig = np.array([])\n",
    "    labels_array = np.array([])\n",
    "    \n",
    "X_train_orig, X_test_orig, y_train, y_test = None, None, None, None\n",
    "X_train_scaled, X_test_scaled, scaler = None, None, None\n",
    "feature_sets = {}\n",
    "feature_transformers = {}\n",
    "\n",
    "if features_array_orig.shape[0] > 0:\n",
    "    print(\"\\n--- Splitting Data ---\")\n",
    "    X_train_orig, X_test_orig, y_train, y_test = split_data(features_array_orig, labels_array, test_size=0.2, random_state=42)\n",
    "    print(\"\\n--- Scaling Features (Initial) ---\")\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train_orig, X_test_orig)\n",
    "    feature_sets['Scaled_All'] = (X_train_scaled, X_test_scaled)\n",
    "    feature_transformers['Scaled_All'] = scaler\n",
    "\n",
    "else:\n",
    "    print(\"No features loaded. Cannot proceed with splitting, scaling, or modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feature_engineering_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Feature Engineering (Selection) ---\n",
      "Starting with 8526 features after scaling.\n",
      "\n",
      "Attempting Correlation Selection with 75%...\n",
      "Selecting top 6394 features based on 75% percentage using Correlation...\n",
      "Original feature shape: (798, 8526)\n",
      "Selected feature shape: (798, 6394)\n",
      "\n",
      "Attempting Correlation Selection with 50%...\n",
      "Selecting top 4263 features based on 50% percentage using Correlation...\n",
      "Original feature shape: (798, 8526)\n",
      "Selected feature shape: (798, 4263)\n",
      "\n",
      "Attempting RFE Selection with 75% (step=0.1)...\n",
      "Selecting top 6394 features based on 75% percentage using RFE...\n",
      "Original feature shape: (798, 8526)\n",
      "Selected feature shape: (798, 6394)\n",
      "\n",
      "Attempting RFE Selection with 50% (step=0.1)...\n",
      "Selecting top 4263 features based on 50% percentage using RFE...\n",
      "Original feature shape: (798, 8526)\n",
      "Selected feature shape: (798, 4263)\n",
      "\n",
      "--- Available Feature Sets for Tuning ---\n",
      "- Scaled_All: 8526 features, before PCA..\n",
      "- Scaled_Corr75%: 6394 features, before PCA..\n",
      "- Scaled_Corr50%: 4263 features, before PCA..\n",
      "- Scaled_RFE75%: 6394 features, before PCA..\n",
      "- Scaled_RFE50%: 4263 features, before PCA..\n",
      "\n",
      "PCA with n_components=0.95...\n",
      "Applying PCA to retain 95% of variance...\n",
      "Original feature shape: (798, 8526)\n",
      "PCA transformed feature shape: (798, 592)\n",
      "Explained variance ratio with 592 components: 0.9502\n",
      "\n",
      "PCA with n_components=500...\n",
      "Applying PCA to reduce to 500 components...\n",
      "Original feature shape: (798, 8526)\n",
      "PCA transformed feature shape: (798, 500)\n",
      "Explained variance ratio with 500 components: 0.9042\n",
      "\n",
      "--- AFTER PCA: ---\n",
      "- Scaled_All: 8526 features\n",
      "- Scaled_Corr75%: 6394 features\n",
      "- Scaled_Corr50%: 4263 features\n",
      "- Scaled_RFE75%: 6394 features\n",
      "- Scaled_RFE50%: 4263 features\n",
      "- Scaled_PCA_95%: 592 features\n",
      "- Scaled_PCA_500: 500 features\n"
     ]
    }
   ],
   "source": [
    "if X_train_scaled is not None and y_train is not None:\n",
    "    print(\"\\n--- Performing Feature Engineering (Selection) ---\")\n",
    "    original_feature_count = X_train_scaled.shape[1]\n",
    "    print(f\"Starting with {original_feature_count} features after scaling.\")\n",
    "\n",
    "     \n",
    "    corr_feature_percentages = ['75%', '50%']\n",
    "    for percentage_str in corr_feature_percentages:\n",
    "        print(f\"\\nAttempting Correlation Selection with {percentage_str}...\")\n",
    "        try:\n",
    "            X_train_corr, X_test_corr, corr_selector = perform_correlation_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, k_features=percentage_str\n",
    "            )\n",
    "            if X_train_corr is not None and X_train_corr.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_Corr{percentage_str}'] = (X_train_corr, X_test_corr)\n",
    "                feature_transformers[f'Scaled_Corr{percentage_str}'] = corr_selector\n",
    "            else:\n",
    "                 print(f\"Correlation Selection with {percentage_str} did not reduce features or failed. Skipping adding this set.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Correlation Selection ({percentage_str}): {e}\")\n",
    "     \n",
    "    rfe_feature_percentages = ['75%', '50%']     \n",
    "    rfe_step_val = 0.1  \n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', random_state=42, max_iter=2000)\n",
    "\n",
    "    for percentage_str in rfe_feature_percentages:\n",
    "         print(f\"\\nAttempting RFE Selection with {percentage_str} (step={rfe_step_val})...\")\n",
    "         try:\n",
    "            X_train_rfe, X_test_rfe, rfe_selector = perform_rfe_selection(\n",
    "                X_train_scaled, y_train, X_test_scaled, n_features_to_select=percentage_str, step=rfe_step_val, estimator=rfe_estimator\n",
    "            )\n",
    "             \n",
    "            if X_train_rfe is not None and X_train_rfe.shape[1] < original_feature_count:\n",
    "                feature_sets[f'Scaled_RFE{percentage_str}'] = (X_train_rfe, X_test_rfe)\n",
    "                feature_transformers[f'Scaled_RFE{percentage_str}'] = rfe_selector\n",
    "            else:\n",
    "                 print(f\"RFE Selection with {percentage_str} did not reduce features or failed. Skipping adding this set.\")\n",
    "         except Exception as e:\n",
    "            print(f\"Error during RFE Selection ({percentage_str}): {e}\")\n",
    "\n",
    "    print(\"\\n--- Available Feature Sets for Tuning ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features, before PCA..\")\n",
    "    \n",
    "    pca_components= [0.95, 500] # aiming for .95 variance retention & 500 feature for first trial\n",
    "    for n_comp in pca_components:\n",
    "        print(f\"\\nPCA with n_components={n_comp}...\")\n",
    "        try:\n",
    "            X_train_pca, X_test_pca, pca_transformer = perform_pca_dimension_reduction(X_train_scaled, X_test_scaled, n_components=n_comp)\n",
    "            if X_train_pca is not None and X_train_pca.shape[1] < original_feature_count:\n",
    "                fs_name_suffix = f\"{int(n_comp*100)}%\" if isinstance(n_comp, float) else str(n_comp)\n",
    "                fs_name = f'Scaled_PCA_{fs_name_suffix}'\n",
    "                feature_sets[fs_name] = (X_train_pca, X_test_pca)\n",
    "                feature_transformers[fs_name] = pca_transformer\n",
    "            else:\n",
    "                print(f\"n_components={n_comp} failed..\")\n",
    "        except Exception as e:\n",
    "            print(f\"error during n_components={n_comp}: {e}\")\n",
    "\n",
    "    print(\"\\n--- AFTER PCA: ---\")\n",
    "    for name, (X_train_fs, _) in feature_sets.items():\n",
    "        print(f\"- {name}: {X_train_fs.shape[1]} features\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping feature engineering as scaled data is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac410d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU, Input\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "def create_custom_mlp(hidden_layer_1_neurons=128, hidden_layer_2_neurons=64,\n",
    "                        dropout_rate=0.3, activation='leaky_relu', learning_rate=0.001,\n",
    "                        meta=None):\n",
    "    n_features_in = meta[\"n_features_in_\"]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_features_in,)))\n",
    "    model.add(Dense(hidden_layer_1_neurons))\n",
    "    model.add(BatchNormalization())\n",
    "    if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "    else: model.add(tf.keras.layers.ReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if hidden_layer_2_neurons is not None and hidden_layer_2_neurons > 0:\n",
    "        model.add(Dense(hidden_layer_2_neurons))\n",
    "        model.add(BatchNormalization())\n",
    "        if activation == 'leaky_relu': model.add(LeakyReLU(alpha=0.1))\n",
    "        else: model.add(tf.keras.layers.ReLU())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "model_training_tuning_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training and Hyperparameter Tuning (RandomizedSearchCV) ---\n",
      "\n",
      "\n",
      "=== Training and Tuning SVM ===\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 33.57 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9534649100646231\n",
      "Best CV f1 for SVM on Scaled_All: 0.9535\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_All...\n",
      "Prediction duration: 0.3407 seconds\n",
      "Accuracy: 0.9250\n",
      "Precision: 0.9533\n",
      "Recall (Sensitivity): 0.9470\n",
      "F1 Score: 0.9502\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_All):\n",
      "[[ 42   7]\n",
      " [  8 143]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 12.60 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9608196841988169\n",
      "Best CV f1 for SVM on Scaled_Corr50%: 0.9608\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_Corr50%...\n",
      "Prediction duration: 0.0859 seconds\n",
      "Accuracy: 0.9100\n",
      "Precision: 0.9290\n",
      "Recall (Sensitivity): 0.9536\n",
      "F1 Score: 0.9412\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr50%):\n",
      "[[ 38  11]\n",
      " [  7 144]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 18.38 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9569187572504877\n",
      "Best CV f1 for SVM on Scaled_Corr75%: 0.9569\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_Corr75%...\n",
      "Prediction duration: 0.2008 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9530\n",
      "Recall (Sensitivity): 0.9404\n",
      "F1 Score: 0.9467\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_Corr75%):\n",
      "[[ 42   7]\n",
      " [  9 142]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_PCA_500 (500 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 0.67 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.947332799179037\n",
      "Best CV f1 for SVM on Scaled_PCA_500: 0.9473\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_PCA_500...\n",
      "Prediction duration: 0.0270 seconds\n",
      "Accuracy: 0.8850\n",
      "Precision: 0.9706\n",
      "Recall (Sensitivity): 0.8742\n",
      "F1 Score: 0.9199\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_PCA_500):\n",
      "[[ 45   4]\n",
      " [ 19 132]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_PCA_95% (592 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 0.87 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9496901693061369\n",
      "Best CV f1 for SVM on Scaled_PCA_95%: 0.9497\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_PCA_95%...\n",
      "Prediction duration: 0.0329 seconds\n",
      "Accuracy: 0.8800\n",
      "Precision: 0.9774\n",
      "Recall (Sensitivity): 0.8609\n",
      "F1 Score: 0.9155\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_PCA_95%):\n",
      "[[ 46   3]\n",
      " [ 21 130]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 25.31 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.981162921832293\n",
      "Best CV f1 for SVM on Scaled_RFE50%: 0.9812\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_RFE50%...\n",
      "Prediction duration: 0.6594 seconds\n",
      "Accuracy: 0.9150\n",
      "Precision: 0.9527\n",
      "Recall (Sensitivity): 0.9338\n",
      "F1 Score: 0.9431\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE50%):\n",
      "[[ 42   7]\n",
      " [ 10 141]]\n",
      "\n",
      "--- Tuning SVM on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 19.70 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
      "\n",
      "Best CV score:\n",
      "0.9641785711077834\n",
      "Best CV f1 for SVM on Scaled_RFE75%: 0.9642\n",
      "\n",
      "Evaluating SVM on the test set using Scaled_RFE75%...\n",
      "Prediction duration: 0.1880 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9530\n",
      "Recall (Sensitivity): 0.9404\n",
      "F1 Score: 0.9467\n",
      "\n",
      "Confusion Matrix (SVM on Scaled_RFE75%):\n",
      "[[ 42   7]\n",
      " [  9 142]]\n",
      "\n",
      "\n",
      "=== Training and Tuning LightGBM ===\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 193.02 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.8, 'num_leaves': 50, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 20, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "\n",
      "Best CV score:\n",
      "0.9777052457973259\n",
      "Best CV f1 for LightGBM on Scaled_All: 0.9777\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_All...\n",
      "Prediction duration: 0.0177 seconds\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.9667\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9635\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_All):\n",
      "[[ 44   5]\n",
      " [  6 145]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 76.97 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "\n",
      "Best CV score:\n",
      "0.9760838486082353\n",
      "Best CV f1 for LightGBM on Scaled_Corr50%: 0.9761\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_Corr50%...\n",
      "Prediction duration: 0.0140 seconds\n",
      "Accuracy: 0.9500\n",
      "Precision: 0.9732\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9667\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr50%):\n",
      "[[ 45   4]\n",
      " [  6 145]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 118.78 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.8, 'num_leaves': 50, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 20, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "\n",
      "Best CV score:\n",
      "0.9760106872447514\n",
      "Best CV f1 for LightGBM on Scaled_Corr75%: 0.9760\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_Corr75%...\n",
      "Prediction duration: 0.0200 seconds\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.9667\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9635\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_Corr75%):\n",
      "[[ 44   5]\n",
      " [  6 145]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_PCA_500 (500 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 17.41 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "\n",
      "Best CV score:\n",
      "0.9265347262482617\n",
      "Best CV f1 for LightGBM on Scaled_PCA_500: 0.9265\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_PCA_500...\n",
      "Prediction duration: 0.0030 seconds\n",
      "Accuracy: 0.8300\n",
      "Precision: 0.9209\n",
      "Recall (Sensitivity): 0.8477\n",
      "F1 Score: 0.8828\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_PCA_500):\n",
      "[[ 38  11]\n",
      " [ 23 128]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_PCA_95% (592 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 20.22 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 50, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "\n",
      "Best CV score:\n",
      "0.9256142318464743\n",
      "Best CV f1 for LightGBM on Scaled_PCA_95%: 0.9256\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_PCA_95%...\n",
      "Prediction duration: 0.0020 seconds\n",
      "Accuracy: 0.8400\n",
      "Precision: 0.9281\n",
      "Recall (Sensitivity): 0.8543\n",
      "F1 Score: 0.8897\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_PCA_95%):\n",
      "[[ 39  10]\n",
      " [ 22 129]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 76.67 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "\n",
      "Best CV score:\n",
      "0.9768698249486011\n",
      "Best CV f1 for LightGBM on Scaled_RFE50%: 0.9769\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_RFE50%...\n",
      "Prediction duration: 0.0155 seconds\n",
      "Accuracy: 0.9500\n",
      "Precision: 0.9732\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9667\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE50%):\n",
      "[[ 45   4]\n",
      " [  6 145]]\n",
      "\n",
      "--- Tuning LightGBM on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "RandomSearch duration: 119.33 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
      "\n",
      "Best CV score:\n",
      "0.9744742803640349\n",
      "Best CV f1 for LightGBM on Scaled_RFE75%: 0.9745\n",
      "\n",
      "Evaluating LightGBM on the test set using Scaled_RFE75%...\n",
      "Prediction duration: 0.0180 seconds\n",
      "Accuracy: 0.9450\n",
      "Precision: 0.9667\n",
      "Recall (Sensitivity): 0.9603\n",
      "F1 Score: 0.9635\n",
      "\n",
      "Confusion Matrix (LightGBM on Scaled_RFE75%):\n",
      "[[ 44   5]\n",
      " [  6 145]]\n",
      "\n",
      "\n",
      "=== Training and Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP ===\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_All (8526 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6407 - loss: 0.7246 - val_accuracy: 0.8313 - val_loss: 0.4404\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8968 - loss: 0.2849 - val_accuracy: 0.9062 - val_loss: 0.2228\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9536 - loss: 0.1525 - val_accuracy: 0.9062 - val_loss: 0.1991\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9848 - loss: 0.0917 - val_accuracy: 0.9125 - val_loss: 0.1704\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9755 - loss: 0.0792 - val_accuracy: 0.9250 - val_loss: 0.1651\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - loss: 0.0561 - val_accuracy: 0.9312 - val_loss: 0.1594\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9977 - loss: 0.0363 - val_accuracy: 0.9312 - val_loss: 0.1551\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9961 - loss: 0.0330 - val_accuracy: 0.9187 - val_loss: 0.1508\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9943 - loss: 0.0288 - val_accuracy: 0.9250 - val_loss: 0.1530\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9973 - loss: 0.0251 - val_accuracy: 0.9312 - val_loss: 0.1548\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9988 - loss: 0.0216 - val_accuracy: 0.9250 - val_loss: 0.1562\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0174 - val_accuracy: 0.9375 - val_loss: 0.1493\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - loss: 0.0175 - val_accuracy: 0.9312 - val_loss: 0.1510\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0120 - val_accuracy: 0.9312 - val_loss: 0.1524\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9989 - loss: 0.0135 - val_accuracy: 0.9312 - val_loss: 0.1534\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0152 - val_accuracy: 0.9312 - val_loss: 0.1498\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0174 - val_accuracy: 0.9375 - val_loss: 0.1502\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0101 - val_accuracy: 0.9312 - val_loss: 0.1505\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 0.9375 - val_loss: 0.1506\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.9312 - val_loss: 0.1527\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9312 - val_loss: 0.1557\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9982 - loss: 0.0102 - val_accuracy: 0.9312 - val_loss: 0.1536\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 0.9312 - val_loss: 0.1550\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.9250 - val_loss: 0.1626\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.9312 - val_loss: 0.1638\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.9250 - val_loss: 0.1619\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9250 - val_loss: 0.1632\n",
      "RandomSearch duration: 243.62 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 128, 'model__dropout_rate': 0.6, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9445375023264471\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_All: 0.9445\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_All...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Prediction duration: 0.1564 seconds\n",
      "Accuracy: 0.9050\n",
      "Precision: 0.9583\n",
      "Recall (Sensitivity): 0.9139\n",
      "F1 Score: 0.9356\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_All):\n",
      "[[ 43   6]\n",
      " [ 13 138]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_Corr50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7035 - loss: 0.6397 - val_accuracy: 0.8813 - val_loss: 0.2627\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9038 - loss: 0.3014 - val_accuracy: 0.9000 - val_loss: 0.1738\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9551 - loss: 0.1636 - val_accuracy: 0.9438 - val_loss: 0.1424\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9682 - loss: 0.1260 - val_accuracy: 0.9500 - val_loss: 0.1317\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9815 - loss: 0.0989 - val_accuracy: 0.9625 - val_loss: 0.1226\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9891 - loss: 0.0804 - val_accuracy: 0.9688 - val_loss: 0.1201\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0717 - val_accuracy: 0.9625 - val_loss: 0.1191\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9891 - loss: 0.0703 - val_accuracy: 0.9688 - val_loss: 0.1225\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9988 - loss: 0.0455 - val_accuracy: 0.9688 - val_loss: 0.1222\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9927 - loss: 0.0439 - val_accuracy: 0.9625 - val_loss: 0.1212\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0454 - val_accuracy: 0.9625 - val_loss: 0.1208\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0411 - val_accuracy: 0.9625 - val_loss: 0.1248\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0351 - val_accuracy: 0.9750 - val_loss: 0.1173\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9977 - loss: 0.0314 - val_accuracy: 0.9688 - val_loss: 0.1179\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9963 - loss: 0.0293 - val_accuracy: 0.9625 - val_loss: 0.1211\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0239 - val_accuracy: 0.9625 - val_loss: 0.1208\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.0195 - val_accuracy: 0.9625 - val_loss: 0.1272\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0188 - val_accuracy: 0.9563 - val_loss: 0.1237\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0173 - val_accuracy: 0.9688 - val_loss: 0.1175\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0204 - val_accuracy: 0.9563 - val_loss: 0.1172\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0162 - val_accuracy: 0.9563 - val_loss: 0.1162\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0157 - val_accuracy: 0.9563 - val_loss: 0.1163\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.9563 - val_loss: 0.1206\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 0.9625 - val_loss: 0.1242\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0197 - val_accuracy: 0.9563 - val_loss: 0.1268\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.9625 - val_loss: 0.1292\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9625 - val_loss: 0.1285\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.0077 - val_accuracy: 0.9625 - val_loss: 0.1314\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.9563 - val_loss: 0.1322\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0103 - val_accuracy: 0.9500 - val_loss: 0.1320\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.9500 - val_loss: 0.1345\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.9438 - val_loss: 0.1400\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.9438 - val_loss: 0.1399\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9375 - val_loss: 0.1438\n",
      "Epoch 35/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0085 - val_accuracy: 0.9375 - val_loss: 0.1474\n",
      "Epoch 36/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0076 - val_accuracy: 0.9438 - val_loss: 0.1458\n",
      "RandomSearch duration: 115.29 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.6, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9525166840181202\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_Corr50%: 0.9525\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_Corr50%...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Prediction duration: 0.1395 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9530\n",
      "Recall (Sensitivity): 0.9404\n",
      "F1 Score: 0.9467\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_Corr50%):\n",
      "[[ 42   7]\n",
      " [  9 142]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_Corr75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.6191 - loss: 0.7659 - val_accuracy: 0.8062 - val_loss: 0.7216\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8221 - loss: 0.4130 - val_accuracy: 0.8062 - val_loss: 0.6292\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9021 - loss: 0.2749 - val_accuracy: 0.8188 - val_loss: 0.4409\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9163 - loss: 0.2182 - val_accuracy: 0.8375 - val_loss: 0.3677\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9548 - loss: 0.1721 - val_accuracy: 0.8375 - val_loss: 0.3119\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9663 - loss: 0.1313 - val_accuracy: 0.8625 - val_loss: 0.2756\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9681 - loss: 0.1061 - val_accuracy: 0.8813 - val_loss: 0.2450\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9733 - loss: 0.1156 - val_accuracy: 0.9062 - val_loss: 0.2085\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9952 - loss: 0.0653 - val_accuracy: 0.9062 - val_loss: 0.1954\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9827 - loss: 0.0814 - val_accuracy: 0.9125 - val_loss: 0.1811\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9751 - loss: 0.0779 - val_accuracy: 0.9000 - val_loss: 0.1818\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9874 - loss: 0.0547 - val_accuracy: 0.9250 - val_loss: 0.1783\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9931 - loss: 0.0475 - val_accuracy: 0.9187 - val_loss: 0.1768\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9993 - loss: 0.0318 - val_accuracy: 0.9250 - val_loss: 0.1657\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0243 - val_accuracy: 0.9375 - val_loss: 0.1699\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9970 - loss: 0.0305 - val_accuracy: 0.9375 - val_loss: 0.1595\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9988 - loss: 0.0272 - val_accuracy: 0.9438 - val_loss: 0.1501\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9938 - loss: 0.0287 - val_accuracy: 0.9375 - val_loss: 0.1359\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9998 - loss: 0.0246 - val_accuracy: 0.9438 - val_loss: 0.1322\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9949 - loss: 0.0286 - val_accuracy: 0.9375 - val_loss: 0.1334\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0191 - val_accuracy: 0.9438 - val_loss: 0.1389\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0158 - val_accuracy: 0.9438 - val_loss: 0.1342\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9988 - loss: 0.0161 - val_accuracy: 0.9375 - val_loss: 0.1335\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0221 - val_accuracy: 0.9438 - val_loss: 0.1373\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9996 - loss: 0.0144 - val_accuracy: 0.9438 - val_loss: 0.1384\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9961 - loss: 0.0149 - val_accuracy: 0.9563 - val_loss: 0.1412\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.9563 - val_loss: 0.1372\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 0.9500 - val_loss: 0.1348\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9941 - loss: 0.0228 - val_accuracy: 0.9500 - val_loss: 0.1371\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9982 - loss: 0.0159 - val_accuracy: 0.9500 - val_loss: 0.1309\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9946 - loss: 0.0168 - val_accuracy: 0.9625 - val_loss: 0.1362\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.9563 - val_loss: 0.1363\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.9438 - val_loss: 0.1392\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0121 - val_accuracy: 0.9563 - val_loss: 0.1340\n",
      "Epoch 35/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.9500 - val_loss: 0.1332\n",
      "Epoch 36/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9563 - val_loss: 0.1334\n",
      "Epoch 37/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.9375 - val_loss: 0.1383\n",
      "Epoch 38/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9438 - val_loss: 0.1407\n",
      "Epoch 39/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9500 - val_loss: 0.1419\n",
      "Epoch 40/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0121 - val_accuracy: 0.9438 - val_loss: 0.1432\n",
      "Epoch 41/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9992 - loss: 0.0076 - val_accuracy: 0.9563 - val_loss: 0.1377\n",
      "Epoch 42/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.9563 - val_loss: 0.1481\n",
      "Epoch 43/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9994 - loss: 0.0149 - val_accuracy: 0.9625 - val_loss: 0.1451\n",
      "Epoch 44/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9968 - loss: 0.0144 - val_accuracy: 0.9500 - val_loss: 0.1479\n",
      "Epoch 45/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9994 - loss: 0.0058 - val_accuracy: 0.9438 - val_loss: 0.1600\n",
      "RandomSearch duration: 194.66 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 64, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.6, 'model__activation': 'leaky_relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9512300801444354\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_Corr75%: 0.9512\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_Corr75%...\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B7FE50E660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Prediction duration: 0.1995 seconds\n",
      "Accuracy: 0.9100\n",
      "Precision: 0.9650\n",
      "Recall (Sensitivity): 0.9139\n",
      "F1 Score: 0.9388\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_Corr75%):\n",
      "[[ 44   5]\n",
      " [ 13 138]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_PCA_500 (500 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6407 - loss: 0.6199 - val_accuracy: 0.8500 - val_loss: 0.3914\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8512 - loss: 0.3923 - val_accuracy: 0.8750 - val_loss: 0.3145\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3415 - val_accuracy: 0.8938 - val_loss: 0.2812\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9339 - loss: 0.2675 - val_accuracy: 0.9000 - val_loss: 0.2606\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9619 - loss: 0.1954 - val_accuracy: 0.9125 - val_loss: 0.2425\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.1474 - val_accuracy: 0.9187 - val_loss: 0.2310\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9789 - loss: 0.1331 - val_accuracy: 0.9187 - val_loss: 0.2269\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9919 - loss: 0.1036 - val_accuracy: 0.9187 - val_loss: 0.2195\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9932 - loss: 0.0864 - val_accuracy: 0.9187 - val_loss: 0.2140\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9955 - loss: 0.0832 - val_accuracy: 0.9187 - val_loss: 0.2113\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9935 - loss: 0.0638 - val_accuracy: 0.9187 - val_loss: 0.2123\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9964 - loss: 0.0565 - val_accuracy: 0.9187 - val_loss: 0.2120\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0521 - val_accuracy: 0.9187 - val_loss: 0.2093\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0412 - val_accuracy: 0.9187 - val_loss: 0.2066\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0317 - val_accuracy: 0.9187 - val_loss: 0.2055\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0292 - val_accuracy: 0.9187 - val_loss: 0.2040\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 0.9187 - val_loss: 0.2034\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0310 - val_accuracy: 0.9187 - val_loss: 0.2033\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0244 - val_accuracy: 0.9250 - val_loss: 0.2026\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0220 - val_accuracy: 0.9187 - val_loss: 0.2037\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0199 - val_accuracy: 0.9250 - val_loss: 0.2039\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0211 - val_accuracy: 0.9250 - val_loss: 0.2062\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0162 - val_accuracy: 0.9187 - val_loss: 0.2073\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.9250 - val_loss: 0.2090\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 0.9250 - val_loss: 0.2093\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0173 - val_accuracy: 0.9250 - val_loss: 0.2121\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0126 - val_accuracy: 0.9187 - val_loss: 0.2141\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9187 - val_loss: 0.2168\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.9187 - val_loss: 0.2177\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9187 - val_loss: 0.2188\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9187 - val_loss: 0.2207\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 0.9187 - val_loss: 0.2243\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9250 - val_loss: 0.2321\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9250 - val_loss: 0.2335\n",
      "RandomSearch duration: 83.84 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.01, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9379116268277322\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_PCA_500: 0.9379\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_PCA_500...\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B7FEA2B880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "Prediction duration: 0.7848 seconds\n",
      "Accuracy: 0.8900\n",
      "Precision: 0.9448\n",
      "Recall (Sensitivity): 0.9073\n",
      "F1 Score: 0.9257\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_PCA_500):\n",
      "[[ 41   8]\n",
      " [ 14 137]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_PCA_95% (592 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5602 - loss: 0.7391 - val_accuracy: 0.7875 - val_loss: 0.4527\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7717 - loss: 0.4414 - val_accuracy: 0.8188 - val_loss: 0.4191\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9063 - loss: 0.2886 - val_accuracy: 0.8500 - val_loss: 0.3726\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9291 - loss: 0.2093 - val_accuracy: 0.8813 - val_loss: 0.3090\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9519 - loss: 0.1568 - val_accuracy: 0.8938 - val_loss: 0.2819\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9930 - loss: 0.0996 - val_accuracy: 0.9000 - val_loss: 0.2636\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.0827 - val_accuracy: 0.9125 - val_loss: 0.2470\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0617 - val_accuracy: 0.9125 - val_loss: 0.2370\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9993 - loss: 0.0448 - val_accuracy: 0.9187 - val_loss: 0.2296\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9973 - loss: 0.0381 - val_accuracy: 0.9250 - val_loss: 0.2236\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9930 - loss: 0.0393 - val_accuracy: 0.9312 - val_loss: 0.2232\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9974 - loss: 0.0282 - val_accuracy: 0.9187 - val_loss: 0.2226\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 0.9187 - val_loss: 0.2221\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 0.9187 - val_loss: 0.2222\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0230 - val_accuracy: 0.9187 - val_loss: 0.2232\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0154 - val_accuracy: 0.9125 - val_loss: 0.2230\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9980 - loss: 0.0220 - val_accuracy: 0.9125 - val_loss: 0.2250\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9961 - loss: 0.0204 - val_accuracy: 0.9062 - val_loss: 0.2268\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.9062 - val_loss: 0.2255\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0147 - val_accuracy: 0.9062 - val_loss: 0.2271\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.9062 - val_loss: 0.2322\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0103 - val_accuracy: 0.9062 - val_loss: 0.2349\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.9125 - val_loss: 0.2341\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0135 - val_accuracy: 0.9062 - val_loss: 0.2319\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0064 - val_accuracy: 0.9125 - val_loss: 0.2334\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 0.9125 - val_loss: 0.2352\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0072 - val_accuracy: 0.9187 - val_loss: 0.2360\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9125 - val_loss: 0.2379\n",
      "RandomSearch duration: 79.48 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.001, 'model__hidden_layer_2_neurons': 128, 'model__hidden_layer_1_neurons': 256, 'model__dropout_rate': 0.4, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9395710925012944\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_PCA_95%: 0.9396\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_PCA_95%...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Prediction duration: 0.2077 seconds\n",
      "Accuracy: 0.8800\n",
      "Precision: 0.9704\n",
      "Recall (Sensitivity): 0.8675\n",
      "F1 Score: 0.9161\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_PCA_95%):\n",
      "[[ 45   4]\n",
      " [ 20 131]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_RFE50% (4263 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6101 - loss: 0.7404 - val_accuracy: 0.8562 - val_loss: 0.3016\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8588 - loss: 0.3206 - val_accuracy: 0.9375 - val_loss: 0.1516\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9388 - loss: 0.2083 - val_accuracy: 0.9563 - val_loss: 0.1170\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.1232 - val_accuracy: 0.9500 - val_loss: 0.1076\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.1199 - val_accuracy: 0.9750 - val_loss: 0.0947\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9984 - loss: 0.0795 - val_accuracy: 0.9750 - val_loss: 0.0846\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0731 - val_accuracy: 0.9812 - val_loss: 0.0848\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0587 - val_accuracy: 0.9812 - val_loss: 0.0793\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0436 - val_accuracy: 0.9812 - val_loss: 0.0749\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0397 - val_accuracy: 0.9875 - val_loss: 0.0731\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0370 - val_accuracy: 0.9875 - val_loss: 0.0694\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0290 - val_accuracy: 0.9812 - val_loss: 0.0683\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9879 - loss: 0.0436 - val_accuracy: 0.9875 - val_loss: 0.0663\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9971 - loss: 0.0280 - val_accuracy: 0.9875 - val_loss: 0.0622\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0216 - val_accuracy: 0.9875 - val_loss: 0.0590\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0234 - val_accuracy: 0.9875 - val_loss: 0.0601\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0245 - val_accuracy: 0.9875 - val_loss: 0.0575\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 0.9875 - val_loss: 0.0576\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0217 - val_accuracy: 0.9812 - val_loss: 0.0583\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0171 - val_accuracy: 0.9812 - val_loss: 0.0573\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 0.9875 - val_loss: 0.0545\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9982 - loss: 0.0195 - val_accuracy: 0.9875 - val_loss: 0.0547\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 0.9875 - val_loss: 0.0541\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 0.9875 - val_loss: 0.0548\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 0.9875 - val_loss: 0.0544\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 0.9875 - val_loss: 0.0537\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 0.9875 - val_loss: 0.0529\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.9812 - val_loss: 0.0523\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0105 - val_accuracy: 0.9812 - val_loss: 0.0526\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0226 - val_accuracy: 0.9812 - val_loss: 0.0541\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.9875 - val_loss: 0.0523\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 0.9875 - val_loss: 0.0512\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9875 - val_loss: 0.0500\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9875 - val_loss: 0.0485\n",
      "Epoch 35/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0106 - val_accuracy: 0.9875 - val_loss: 0.0464\n",
      "Epoch 36/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9875 - val_loss: 0.0463\n",
      "Epoch 37/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9875 - val_loss: 0.0454\n",
      "Epoch 38/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9875 - val_loss: 0.0455\n",
      "Epoch 39/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9875 - val_loss: 0.0464\n",
      "Epoch 40/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9875 - val_loss: 0.0464\n",
      "Epoch 41/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9875 - val_loss: 0.0471\n",
      "Epoch 42/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.9875 - val_loss: 0.0482\n",
      "Epoch 43/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0044 - val_accuracy: 0.9875 - val_loss: 0.0503\n",
      "Epoch 44/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.9688 - val_loss: 0.0597\n",
      "Epoch 45/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.9875 - val_loss: 0.0506\n",
      "Epoch 46/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9875 - val_loss: 0.0500\n",
      "Epoch 47/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9875 - val_loss: 0.0506\n",
      "Epoch 48/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.9875 - val_loss: 0.0506\n",
      "Epoch 49/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.9875 - val_loss: 0.0500\n",
      "Epoch 50/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9875 - val_loss: 0.0490\n",
      "Epoch 51/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9875 - val_loss: 0.0468\n",
      "Epoch 52/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9875 - val_loss: 0.0459\n",
      "RandomSearch duration: 151.93 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.6, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9810087708522713\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_RFE50%: 0.9810\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_RFE50%...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Prediction duration: 0.1470 seconds\n",
      "Accuracy: 0.9200\n",
      "Precision: 0.9787\n",
      "Recall (Sensitivity): 0.9139\n",
      "F1 Score: 0.9452\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_RFE50%):\n",
      "[[ 46   3]\n",
      " [ 13 138]]\n",
      "\n",
      "--- Tuning 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Feature Set: Scaled_RFE75% (6394 features) ---\n",
      "\n",
      "Performing RandomSearch tuning (scoring='f1')... with 10 iterations\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6818 - loss: 0.6950 - val_accuracy: 0.8125 - val_loss: 0.3779\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8737 - loss: 0.3209 - val_accuracy: 0.9062 - val_loss: 0.2097\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9376 - loss: 0.1893 - val_accuracy: 0.9563 - val_loss: 0.1620\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.1333 - val_accuracy: 0.9563 - val_loss: 0.1400\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9803 - loss: 0.1160 - val_accuracy: 0.9500 - val_loss: 0.1395\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9787 - loss: 0.0876 - val_accuracy: 0.9563 - val_loss: 0.1333\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0675 - val_accuracy: 0.9500 - val_loss: 0.1274\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9981 - loss: 0.0525 - val_accuracy: 0.9500 - val_loss: 0.1218\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0478 - val_accuracy: 0.9688 - val_loss: 0.1175\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9971 - loss: 0.0493 - val_accuracy: 0.9688 - val_loss: 0.1188\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0382 - val_accuracy: 0.9625 - val_loss: 0.1139\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0357 - val_accuracy: 0.9688 - val_loss: 0.1113\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0298 - val_accuracy: 0.9688 - val_loss: 0.1113\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0343 - val_accuracy: 0.9625 - val_loss: 0.1109\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0256 - val_accuracy: 0.9625 - val_loss: 0.1073\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0257 - val_accuracy: 0.9625 - val_loss: 0.1047\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9947 - loss: 0.0290 - val_accuracy: 0.9563 - val_loss: 0.1056\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0191 - val_accuracy: 0.9625 - val_loss: 0.1046\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0200 - val_accuracy: 0.9625 - val_loss: 0.1055\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0178 - val_accuracy: 0.9750 - val_loss: 0.1042\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - loss: 0.0174 - val_accuracy: 0.9688 - val_loss: 0.1052\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9965 - loss: 0.0191 - val_accuracy: 0.9625 - val_loss: 0.1036\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0155 - val_accuracy: 0.9625 - val_loss: 0.1033\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0153 - val_accuracy: 0.9625 - val_loss: 0.1021\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.9625 - val_loss: 0.1060\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0104 - val_accuracy: 0.9625 - val_loss: 0.1027\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.9688 - val_loss: 0.1013\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0147 - val_accuracy: 0.9625 - val_loss: 0.1030\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.9625 - val_loss: 0.1031\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.9625 - val_loss: 0.1041\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 0.9625 - val_loss: 0.1041\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9563 - val_loss: 0.1037\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0088 - val_accuracy: 0.9688 - val_loss: 0.1002\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0102 - val_accuracy: 0.9688 - val_loss: 0.0993\n",
      "Epoch 35/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0106 - val_accuracy: 0.9688 - val_loss: 0.1009\n",
      "Epoch 36/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0106 - val_accuracy: 0.9688 - val_loss: 0.1017\n",
      "Epoch 37/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9982 - loss: 0.0102 - val_accuracy: 0.9625 - val_loss: 0.1041\n",
      "Epoch 38/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9625 - val_loss: 0.1043\n",
      "Epoch 39/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9563 - val_loss: 0.1073\n",
      "Epoch 40/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0053 - val_accuracy: 0.9625 - val_loss: 0.1071\n",
      "Epoch 41/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0065 - val_accuracy: 0.9563 - val_loss: 0.1047\n",
      "Epoch 42/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.9563 - val_loss: 0.1068\n",
      "Epoch 43/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.9625 - val_loss: 0.1072\n",
      "Epoch 44/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.9688 - val_loss: 0.1011\n",
      "Epoch 45/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9688 - val_loss: 0.1008\n",
      "Epoch 46/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9922 - loss: 0.0179 - val_accuracy: 0.9438 - val_loss: 0.1131\n",
      "Epoch 47/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9992 - loss: 0.0116 - val_accuracy: 0.9625 - val_loss: 0.0875\n",
      "Epoch 48/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9980 - loss: 0.0144 - val_accuracy: 0.9625 - val_loss: 0.0841\n",
      "Epoch 49/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.9375 - val_loss: 0.0971\n",
      "Epoch 50/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0075 - val_accuracy: 0.9563 - val_loss: 0.0886\n",
      "Epoch 51/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9688 - val_loss: 0.0873\n",
      "Epoch 52/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9688 - val_loss: 0.0861\n",
      "Epoch 53/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.9563 - val_loss: 0.0883\n",
      "Epoch 54/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9563 - val_loss: 0.0893\n",
      "Epoch 55/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0083 - val_accuracy: 0.9563 - val_loss: 0.0887\n",
      "Epoch 56/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9625 - val_loss: 0.0871\n",
      "Epoch 57/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0052 - val_accuracy: 0.9625 - val_loss: 0.0887\n",
      "Epoch 58/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9438 - val_loss: 0.1006\n",
      "Epoch 59/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.9563 - val_loss: 0.1054\n",
      "Epoch 60/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9563 - val_loss: 0.1029\n",
      "Epoch 61/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.9625 - val_loss: 0.0940\n",
      "Epoch 62/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.9688 - val_loss: 0.0905\n",
      "Epoch 63/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.9688 - val_loss: 0.0888\n",
      "RandomSearch duration: 197.81 seconds\n",
      "\n",
      "Best parameters found:\n",
      "{'optimizer__learning_rate': 0.005, 'model__hidden_layer_2_neurons': 0, 'model__hidden_layer_1_neurons': 64, 'model__dropout_rate': 0.6, 'model__activation': 'relu'}\n",
      "\n",
      "Best CV score:\n",
      "0.9583885830975111\n",
      "Best CV f1 for 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_RFE75%: 0.9584\n",
      "\n",
      "Evaluating 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on the test set using Scaled_RFE75%...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Prediction duration: 0.1430 seconds\n",
      "Accuracy: 0.9150\n",
      "Precision: 0.9527\n",
      "Recall (Sensitivity): 0.9338\n",
      "F1 Score: 0.9431\n",
      "\n",
      "Confusion Matrix ('MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP on Scaled_RFE75%):\n",
      "[[ 42   7]\n",
      " [ 10 141]]\n",
      "\n",
      "--- Model Training and Hyperparameter Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "#cvdeki iter'i 20'den 10'a cektim. cv fold'u 5 ten 3 yaptim. keras-tf icin gpu lib kurulabiliyormus\n",
    "if not feature_sets or y_train is None:\n",
    "    print(\"Skipping model training and tuning: No feature sets available or labels are missing.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Model Training and Hyperparameter Tuning (RandomizedSearchCV) ---\")     \n",
    "    models_to_tune = {\n",
    "        'SVM': {\n",
    "            'estimator': SVC(random_state=42),\n",
    "            'param_grid': {\n",
    "                'C': [0.1, 1, 10, 50], # 50, 100\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1], # 0.001\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'estimator': lgb.LGBMClassifier(random_state=42, objective='binary', metric='binary_logloss', verbosity=-1),  \n",
    "            'param_grid': {\n",
    "                'n_estimators': [50, 100, 150],      \n",
    "                'learning_rate': [0.01, 0.05, 0.1],   \n",
    "                'max_depth': [-1, 10, 20],           \n",
    "                'num_leaves': [31, 50, 70],          \n",
    "                'subsample': [0.8, 0.9],              \n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'min_split_gain': [0.1],    \n",
    "                'min_child_samples': [5]   \n",
    "            }\n",
    "        },\n",
    "        \"\"\"'MLP': {  \n",
    "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
    "            'param_grid': {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
    "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
    "                'solver': ['adam'],  \n",
    "                'alpha': [0.0001, 0.001],  \n",
    "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
    "            }\n",
    "        },\"\"\"\n",
    "        'Custom_MLP': {\n",
    "            'estimator': KerasClassifier(\n",
    "                model=create_custom_mlp,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                verbose=1, # kt ayar\n",
    "                validation_split=0.2, # early stopping\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, restore_best_weights=True)]\n",
    "            ),\n",
    "            'param_grid': {\n",
    "                'model__hidden_layer_1_neurons': [64, 128, 256],\n",
    "                'model__hidden_layer_2_neurons': [0, 64, 128],\n",
    "                'model__dropout_rate': [0.2, 0.4, 0.6],\n",
    "                'model__activation': ['relu', 'leaky_relu'],\n",
    "                'optimizer__learning_rate': [0.001, 0.005, 0.01]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # todo: degisiklikleri dfirea da uygula\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    #bunu daha onceki notebookta acklamstm. raporda/sunumda acklamam iyi olur\n",
    "    scoring_metric = 'f1'   \n",
    "    all_results = {}\n",
    "\n",
    "    best_overall_test_score = -np.inf\n",
    "    best_overall_combination = None\n",
    "    best_overall_trained_model = None\n",
    "    best_overall_X_test = None\n",
    "    best_overall_transformer = None\n",
    "\n",
    "    for model_name, model_config in models_to_tune.items():\n",
    "        all_results[model_name] = {}\n",
    "        estimator = model_config['estimator']\n",
    "        param_distributions = model_config['param_grid']\n",
    "        n_iter_search = model_config.get('n_iter', 10)  \n",
    "\n",
    "        print(f\"\\n\\n=== Training and Tuning {model_name} ===\")\n",
    "\n",
    "        for fs_name in sorted(feature_sets.keys()):\n",
    "            X_train_fs, X_test_fs = feature_sets[fs_name]\n",
    "            print(f\"\\n--- Tuning {model_name} on Feature Set: {fs_name} ({X_train_fs.shape[1]} features) ---\")\n",
    "\n",
    "            if X_train_fs is None or X_train_fs.shape[0] == 0:\n",
    "                print(f\"Skipping tuning for {model_name} on {fs_name}: Training data is empty.\")\n",
    "                continue\n",
    "\n",
    "            tuned_search = tune_model_hyperparameters(\n",
    "                estimator,\n",
    "                X_train_fs,\n",
    "                y_train,\n",
    "                param_grid=param_distributions,  \n",
    "                cv_strategy=cv_strategy,\n",
    "                scoring=scoring_metric,\n",
    "                search_method='RandomSearch',  \n",
    "                n_iter=n_iter_search\n",
    "            )\n",
    "\n",
    "            if tuned_search:\n",
    "                best_model_for_combination = tuned_search.best_estimator_\n",
    "                best_cv_score = tuned_search.best_score_\n",
    "                best_params = tuned_search.best_params_\n",
    "                print(f\"Best CV {scoring_metric} for {model_name} on {fs_name}: {best_cv_score:.4f}\")\n",
    "\n",
    "                test_metrics = evaluate_model(best_model_for_combination, X_test_fs, y_test, model_name, fs_name)\n",
    "\n",
    "                all_results[model_name][fs_name] = {\n",
    "                    'best_cv_score': best_cv_score,\n",
    "                    'best_params': best_params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'trained_model': best_model_for_combination,\n",
    "                    'transformer': feature_transformers.get(fs_name)   \n",
    "                }\n",
    "\n",
    "                if test_metrics and test_metrics.get('f1_score', -np.inf) > best_overall_test_score:\n",
    "                    best_overall_test_score = test_metrics['f1_score']\n",
    "                    best_overall_combination = (model_name, fs_name)\n",
    "                    best_overall_trained_model = best_model_for_combination\n",
    "                    best_overall_X_test = X_test_fs\n",
    "                    best_overall_transformer = feature_transformers.get(fs_name)\n",
    "    print(\"\\n--- Model Training and Hyperparameter Tuning Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "results_comparison_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Results Summary Across Models and Feature Sets ===\n",
      "\n",
      "Cross-Validation Results (Best CV F1 Score):\n",
      "-------------------------------------------------\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All: 0.9535\n",
      "  - Scaled_Corr50%: 0.9608\n",
      "  - Scaled_Corr75%: 0.9569\n",
      "  - Scaled_PCA_500: 0.9473\n",
      "  - Scaled_PCA_95%: 0.9497\n",
      "  - Scaled_RFE50%: 0.9812\n",
      "  - Scaled_RFE75%: 0.9642\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All: 0.9777\n",
      "  - Scaled_Corr50%: 0.9761\n",
      "  - Scaled_Corr75%: 0.9760\n",
      "  - Scaled_PCA_500: 0.9265\n",
      "  - Scaled_PCA_95%: 0.9256\n",
      "  - Scaled_RFE50%: 0.9769\n",
      "  - Scaled_RFE75%: 0.9745\n",
      "\n",
      "'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP:\n",
      "  - Scaled_All: 0.9445\n",
      "  - Scaled_Corr50%: 0.9525\n",
      "  - Scaled_Corr75%: 0.9512\n",
      "  - Scaled_PCA_500: 0.9379\n",
      "  - Scaled_PCA_95%: 0.9396\n",
      "  - Scaled_RFE50%: 0.9810\n",
      "  - Scaled_RFE75%: 0.9584\n",
      "\n",
      "Test Set Results (F1 Score):\n",
      "----------------------------\n",
      "\n",
      "SVM:\n",
      "  - Scaled_All: 0.9502\n",
      "  - Scaled_Corr50%: 0.9412\n",
      "  - Scaled_Corr75%: 0.9467\n",
      "  - Scaled_PCA_500: 0.9199\n",
      "  - Scaled_PCA_95%: 0.9155\n",
      "  - Scaled_RFE50%: 0.9431\n",
      "  - Scaled_RFE75%: 0.9467\n",
      "\n",
      "LightGBM:\n",
      "  - Scaled_All: 0.9635\n",
      "  - Scaled_Corr50%: 0.9667\n",
      "  - Scaled_Corr75%: 0.9635\n",
      "  - Scaled_PCA_500: 0.8828\n",
      "  - Scaled_PCA_95%: 0.8897\n",
      "  - Scaled_RFE50%: 0.9667\n",
      "  - Scaled_RFE75%: 0.9635\n",
      "\n",
      "'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP:\n",
      "  - Scaled_All: 0.9356\n",
      "  - Scaled_Corr50%: 0.9467\n",
      "  - Scaled_Corr75%: 0.9388\n",
      "  - Scaled_PCA_500: 0.9257\n",
      "  - Scaled_PCA_95%: 0.9161\n",
      "  - Scaled_RFE50%: 0.9452\n",
      "  - Scaled_RFE75%: 0.9431\n",
      "\n",
      "=== Overall Best Combination on Test Set (Based on F1 Score) ===\n",
      "Best Model: LightGBM\n",
      "Best Feature Set: Scaled_Corr50% (4263 features)\n",
      "Best CV F1 Score: 0.9761\n",
      "Test F1 Score: 0.9667\n",
      "Test Accuracy: 0.9500\n",
      "Test Precision: 0.9732\n",
      "Test Recall: 0.9603\n",
      "Best Parameters: {'subsample': 0.8, 'num_leaves': 70, 'n_estimators': 150, 'min_split_gain': 0.1, 'min_child_samples': 5, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Confusion Matrix:\n",
      "[[ 45   4]\n",
      " [  6 145]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n=== Results Summary Across Models and Feature Sets ===\")\n",
    "\n",
    "if 'all_results' not in locals() or not all_results:\n",
    "    print(\"No results available to summarize.\")\n",
    "else:\n",
    "    print(\"\\nCross-Validation Results (Best CV F1 Score):\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            # Sort feature sets for display\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                 result = fs_results[fs_name]\n",
    "                 cv_score = result.get('best_cv_score', float('nan'))\n",
    "                 print(f\"  - {fs_name}: {cv_score:.4f}\")\n",
    "        else:\n",
    "            print(\"  No results for this model.\")\n",
    "\n",
    "    print(\"\\nTest Set Results (F1 Score):\")\n",
    "    print(\"----------------------------\")\n",
    "    best_f1_per_model = {}\n",
    "\n",
    "    for model_name, fs_results in all_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if fs_results:\n",
    "            best_test_f1_for_model = -np.inf\n",
    "            best_fs_name_for_model = None\n",
    "\n",
    "            for fs_name in sorted(fs_results.keys()):\n",
    "                 result = fs_results[fs_name]\n",
    "                 test_f1 = result.get('test_metrics', {}).get('f1_score', float('nan'))\n",
    "                 print(f\"  - {fs_name}: {test_f1:.4f}\")\n",
    "\n",
    "                 if not np.isnan(test_f1) and test_f1 > best_test_f1_for_model:\n",
    "                     best_test_f1_for_model = test_f1\n",
    "                     best_fs_name_for_model = fs_name\n",
    "\n",
    "            if best_fs_name_for_model:\n",
    "                best_f1_per_model[model_name] = (best_fs_name_for_model, best_test_f1_for_model)\n",
    "        else:\n",
    "            print(\"  No test results for this model.\")\n",
    "\n",
    "    print(\"\\n=== Overall Best Combination on Test Set (Based on F1 Score) ===\")\n",
    "    if best_overall_combination:\n",
    "        model_name, fs_name = best_overall_combination\n",
    "        best_result = all_results[model_name][fs_name]\n",
    "        test_metrics = best_result['test_metrics']\n",
    "\n",
    "        print(f\"Best Model: {model_name}\")\n",
    "        actual_feature_count = feature_sets[fs_name][0].shape[1] if fs_name in feature_sets and feature_sets[fs_name][0] is not None else 'N/A'\n",
    "        print(f\"Best Feature Set: {fs_name} ({actual_feature_count} features)\")\n",
    "\n",
    "        print(f\"Best CV F1 Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_params']}\")\n",
    "        print(f\"Confusion Matrix:\\n{np.array(test_metrics['confusion_matrix'])}\")\n",
    "    else:\n",
    "        print(\"No successful model tuning and evaluation completed to determine the overall best combination.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save_model_code_kaggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Best Model Per Algorithm (Based on Test F1) ---\n",
      "no stored variable or alias best_f1_per_model\n",
      "Initial scaler already exists at ..\\models\\scaler_initial.pkl. Skipping save.\n",
      "\n",
      "Saving best model and transformer for each algorithm...\n",
      "\n",
      "Processing SVM...\n",
      "   Saved model: ..\\models\\kagglesvm_best_model_Scaled_All.pkl\n",
      "\n",
      "Processing LightGBM...\n",
      "   Saved model: ..\\models\\kagglelightgbm_best_model_Scaled_Corr50%.pkl\n",
      "   Saved feature selection transformer: ..\\models\\selector_Scaled_Corr50%.pkl\n",
      "\n",
      "Processing 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP...\n",
      "   Error saving 'MLP': {  \n",
      "            'estimator': MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.LeakyReLU() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },Custom_MLP model to ..\\models\\kaggle'mlp': {  \n",
      "            'estimator': mlpclassifier(random_state=42, max_iter=500, early_stopping=true, n_iter_no_change=10),  \n",
      "            'param_grid': {\n",
      "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \n",
      "                'activation': ['relu'], # tf.keras.layers.leakyrelu() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\n",
      "                'solver': ['adam'],  \n",
      "                'alpha': [0.0001, 0.001],  \n",
      "                'learning_rate_init': [0.001, 0.005, 0.01],  \n",
      "            }\n",
      "        },custom_mlp_best_model_Scaled_Corr50%.pkl: [Errno 22] Invalid argument: \"..\\\\models\\\\kaggle'mlp': {  \\n            'estimator': mlpclassifier(random_state=42, max_iter=500, early_stopping=true, n_iter_no_change=10),  \\n            'param_grid': {\\n                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  \\n                'activation': ['relu'], # tf.keras.layers.leakyrelu() dense katmannda eklenebilirmis. custom mlpye geciyorum customu unutmusum\\n                'solver': ['adam'],  \\n                'alpha': [0.0001, 0.001],  \\n                'learning_rate_init': [0.001, 0.005, 0.01],  \\n            }\\n        },custom_mlp_best_model_Scaled_Corr50%.pkl\"\n",
      "   Saved feature selection transformer: ..\\models\\selector_Scaled_Corr50%.pkl\n",
      "\n",
      "--- Saving Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_DIR = os.path.join('..', 'models')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n--- Saving Best Model Per Algorithm (Based on Test F1) ---\")\n",
    "\n",
    "%store -r best_f1_per_model\n",
    "\n",
    "if 'all_results' not in locals() or not all_results:\n",
    "     print(\"No results found from model training and tuning. Nothing to save.\")\n",
    "elif 'best_f1_per_model' not in locals() or not best_f1_per_model:\n",
    "     print(\"Could not determine best feature set per model. Skipping saving.\")\n",
    "else:\n",
    "     \n",
    "    initial_scaler_for_saving = feature_transformers.get('Scaled_All')\n",
    "    SCALER_FILENAME = 'scaler_initial.pkl'\n",
    "    SCALER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, SCALER_FILENAME)\n",
    "\n",
    "    if initial_scaler_for_saving:\n",
    "        if not os.path.exists(SCALER_SAVE_PATH):\n",
    "            try:\n",
    "                joblib.dump(initial_scaler_for_saving, SCALER_SAVE_PATH)\n",
    "                print(f\"Saved initial scaler: {SCALER_SAVE_PATH}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving initial scaler: {e}\")\n",
    "        else:\n",
    "            print(f\"Initial scaler already exists at {SCALER_SAVE_PATH}. Skipping save.\")\n",
    "    else:\n",
    "        print(\"Initial scaler ('Scaled_All') not found in feature_transformers. Cannot save initial scaler.\")\n",
    "\n",
    "    print(\"\\nSaving best model and transformer for each algorithm...\")\n",
    "    for model_name, (best_fs_name_for_model, best_test_f1_for_model) in best_f1_per_model.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        if best_fs_name_for_model and model_name in all_results and best_fs_name_for_model in all_results[model_name]:\n",
    "            best_combination_results = all_results[model_name][best_fs_name_for_model]\n",
    "            model_to_save = best_combination_results.get('trained_model')\n",
    "            transformer_to_save = best_combination_results.get('transformer')\n",
    "\n",
    "            if model_to_save:\n",
    "                model_filename = f'kaggle{model_name.lower()}_best_model_{best_fs_name_for_model}.pkl'\n",
    "                MODEL_SAVE_PATH_ALG = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "                try:\n",
    "                    joblib.dump(model_to_save, MODEL_SAVE_PATH_ALG)\n",
    "                    print(f\"   Saved model: {MODEL_SAVE_PATH_ALG}\")\n",
    "                except Exception as e:\n",
    "                     print(f\"   Error saving {model_name} model to {MODEL_SAVE_PATH_ALG}: {e}\")\n",
    "            else:\n",
    "                 print(f\"   No trained model found for {model_name} on {best_fs_name_for_model}.\")\n",
    "\n",
    "             \n",
    "            if transformer_to_save and best_fs_name_for_model != 'Scaled_All':\n",
    "                 transformer_filename = f'selector_{best_fs_name_for_model}.pkl'\n",
    "                 TRANSFORMER_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, transformer_filename)\n",
    "                 try:\n",
    "                     joblib.dump(transformer_to_save, TRANSFORMER_SAVE_PATH)\n",
    "                     print(f\"   Saved feature selection transformer: {TRANSFORMER_SAVE_PATH}\")\n",
    "                 except Exception as e:\n",
    "                    print(f\"   Error saving transformer for {best_fs_name_for_model} to {TRANSFORMER_SAVE_PATH}: {e}\")\n",
    "            elif best_fs_name_for_model != 'Scaled_All':\n",
    "                print(f\"   Warning: Feature selection transformer not found for {best_fs_name_for_model}. Cannot save it.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No valid results found for the best feature set '{best_fs_name_for_model}' for model {model_name}.\")\n",
    "\n",
    "print(\"\\n--- Saving Process Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
